{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376c149d",
   "metadata": {},
   "source": [
    "# Spatiotemporal Explainable AI for Power System Contingency Classification and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f87c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install pandapower\n",
    "%pip install pandapower -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6116728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency results saved to: ./n1_contingency_balanced.csv\n",
      "Load scenarios saved to: ./load_scenarios.xlsx\n",
      "✅ Contingency analysis complete.\n",
      "Final Severity Counts:\n",
      " Severity\n",
      "0    23395\n",
      "1    17605\n",
      "Name: count, dtype: int64\n",
      "Total load scenarios saved: 1000 scenarios, 20000 rows\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Step 2: Import required libraries\n",
    "import pandapower as pp\n",
    "import pandapower.networks as pn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os # Import the os module\n",
    "\n",
    "# Define the directory where you want to save the files\n",
    "# !!! IMPORTANT: CHANGE THIS PATH TO A DIRECTORY ON YOUR LOCAL MACHINE !!!\n",
    "save_directory = \"./\" # Example: create a 'generated_files' folder in the same directory as your notebook\n",
    "# Or use an absolute path: save_directory = \"/path/to/your/desired/directory\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# Step 3: Function to generate random load variation\n",
    "def vary_loads(net, scale_min=0.7, scale_max=1.0):\n",
    "    scaling_factors = np.random.uniform(scale_min, scale_max, size=len(net.load))\n",
    "    net.load['p_mw'] *= scaling_factors\n",
    "    net.load['q_mvar'] *= scaling_factors\n",
    "    return net\n",
    "\n",
    "# Step 4: Prepare to store results across all scenarios\n",
    "all_results = []\n",
    "load_scenarios = []\n",
    "\n",
    "# Step 5: Generate 1000 random load scenarios\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "for scenario_id in range(1000):\n",
    "    net = pn.case30()\n",
    "    net = vary_loads(net, 0.7, 1.0)  # 30% variation range in loads\n",
    "\n",
    "    # Store the load values for each scenario\n",
    "    for i, row in net.load.iterrows():\n",
    "        load_scenarios.append({\n",
    "            'Scenario': scenario_id,\n",
    "            'Load_Bus': row['bus'],\n",
    "            'Load_ID': i,\n",
    "            'P_mw': row['p_mw'],\n",
    "            'Q_mvar': row['q_mvar']\n",
    "        })\n",
    "\n",
    "    # Adjust line ratings (custom stress for line 8)\n",
    "    net.line['max_loading_percent'] = 115.0\n",
    "    net.line.at[8, 'max_loading_percent'] = 100.0\n",
    "\n",
    "    # N-1 Contingency simulation: Take one line out at a time\n",
    "    for i in net.line.index:\n",
    "        net_copy = copy.deepcopy(net)\n",
    "        net_copy.line.at[i, 'in_service'] = False\n",
    "\n",
    "        try:\n",
    "            pp.runpp(net_copy)\n",
    "            status = 'Stable'\n",
    "        except Exception as e:\n",
    "            status = 'Unstable'\n",
    "\n",
    "        result = {\n",
    "            'Scenario': scenario_id,\n",
    "            'Outaged_Line': i,\n",
    "            'Status': status\n",
    "        }\n",
    "\n",
    "        # Store bus voltages if stable\n",
    "        for bus in net_copy.bus.index:\n",
    "            result[f'V_bus_{bus}'] = net_copy.res_bus.vm_pu.at[bus] if status == 'Stable' else None\n",
    "\n",
    "        # Store line loadings if stable\n",
    "        for line in net_copy.line.index:\n",
    "            result[f'Loading_line_{line}'] = net_copy.res_line.loading_percent.at[line] if status == 'Stable' else None\n",
    "\n",
    "        all_results.append(result)\n",
    "\n",
    "# Step 6: Compile all results\n",
    "df_all = pd.DataFrame(all_results)\n",
    "\n",
    "# Step 7: Apply severity threshold (98% line loading triggers severity)\n",
    "loading_cols = [col for col in df_all.columns if col.startswith(\"Loading_line_\")]\n",
    "df_all['Severity'] = df_all[loading_cols].gt(98.0).any(axis=1).astype(int)\n",
    "\n",
    "# Step 8: Save contingency results to CSV\n",
    "# Construct the full path for saving\n",
    "csv_output_path = os.path.join(save_directory, \"n1_contingency_balanced.csv\")\n",
    "df_all.to_csv(csv_output_path, index=False)\n",
    "print(f\"Contingency results saved to: {csv_output_path}\")\n",
    "\n",
    "\n",
    "# Step 9: Save load scenarios to Excel\n",
    "df_loads = pd.DataFrame(load_scenarios)\n",
    "# Construct the full path for saving\n",
    "excel_output_path = os.path.join(save_directory, \"load_scenarios.xlsx\")\n",
    "df_loads.to_excel(excel_output_path, index=False)\n",
    "print(f\"Load scenarios saved to: {excel_output_path}\")\n",
    "\n",
    "\n",
    "# Step 10: Print summary\n",
    "print(\"✅ Contingency analysis complete.\")\n",
    "print(\"Final Severity Counts:\\n\", df_all['Severity'].value_counts())\n",
    "print(f\"Total load scenarios saved: {df_loads['Scenario'].nunique()} scenarios, {len(df_loads)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded66dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (41000, 75)\n",
      "Filled dataset shape: (41000, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eredd\\AppData\\Local\\Temp\\ipykernel_13440\\1283725754.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled = df_replaced.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Empty cells filled and saved to: ./n1_contingency_balanced_filled_complete.csv\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "#Task 1.2: Missing Value Detection and Cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # Import the os module\n",
    "\n",
    "# Define the directory where files are saved and will be loaded from\n",
    "# !!! IMPORTANT: Ensure this matches the save_directory in the previous cell !!!\n",
    "data_directory = \"./\" # Example: if you saved to 'generated_files'\n",
    "# Or use the absolute path: data_directory = \"/path/to/your/desired/directory\"\n",
    "\n",
    "# Load the CSV file treating empty strings as missing (NaN)\n",
    "# Construct the full path for loading\n",
    "file_path = os.path.join(data_directory, \"n1_contingency_balanced.csv\")\n",
    "df = pd.read_csv(file_path, keep_default_na=False)\n",
    "\n",
    "# Show original dimensions\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Replace only empty strings with NaN\n",
    "df_replaced = df.replace(\"\", np.nan)\n",
    "\n",
    "# Fill missing values: forward fill first, then backward fill\n",
    "df_filled = df_replaced.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Show filled dataset dimensions (should be the same)\n",
    "print(f\"Filled dataset shape: {df_filled.shape}\")\n",
    "\n",
    "# Save the result to a new CSV\n",
    "# Construct the full path for saving\n",
    "output_path = os.path.join(data_directory, \"n1_contingency_balanced_filled_complete.csv\")\n",
    "df_filled.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Empty cells filled and saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dcd076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\eredd\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\eredd\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio -q\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72edb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
      "Requirement already satisfied: torch_scatter in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2+pt20cpu)\n",
      "Requirement already satisfied: torch_sparse in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.18+pt20cpu)\n",
      "Requirement already satisfied: torch_cluster in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.3+pt20cpu)\n",
      "Requirement already satisfied: torch_spline_conv in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.2+pt20cpu)\n",
      "Requirement already satisfied: scipy in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_sparse) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scipy->torch_sparse) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch-geometric in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (3.12.13)\n",
      "Requirement already satisfied: fsspec in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (2025.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (2.3.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\eredd\\appdata\\roaming\\python\\python311\\site-packages (from torch-geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\eredd\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Install necessary libraries\n",
    "%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
    "%pip install torch-geometric\n",
    "%pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054b2a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\USERS\\EREDD\\APPDATA\\LOCAL\\PROGRAMS\\PYTHON\\PYTHON311\\Lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "c:\\USERS\\EREDD\\APPDATA\\LOCAL\\PROGRAMS\\PYTHON\\PYTHON311\\Lib\\site-packages\\torch_geometric\\typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "c:\\USERS\\EREDD\\APPDATA\\LOCAL\\PROGRAMS\\PYTHON\\PYTHON311\\Lib\\site-packages\\torch_geometric\\typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(\n",
      "c:\\USERS\\EREDD\\APPDATA\\LOCAL\\PROGRAMS\\PYTHON\\PYTHON311\\Lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "- Combined input: (41000, 111)\n",
      "- Target features: (41000, 71)\n",
      "- Severity labels: (41000,)\n",
      "- Ranking shape: (41000, 41)\n",
      "Train shape: (40590, 111) (40590, 71)\n",
      "Test shape: (410, 111) (410, 71)\n",
      "\n",
      "Training model: LSTM\n",
      "LSTM - Epoch 1 Loss: 0.2826\n",
      "LSTM - Epoch 2 Loss: 0.1298\n",
      "LSTM - Epoch 3 Loss: 0.1003\n",
      "LSTM - Epoch 4 Loss: 0.0967\n",
      "LSTM - Epoch 5 Loss: 0.0836\n",
      "LSTM - Epoch 6 Loss: 0.0818\n",
      "LSTM - Epoch 7 Loss: 0.0743\n",
      "LSTM - Epoch 8 Loss: 0.0675\n",
      "LSTM - Epoch 9 Loss: 0.0638\n",
      "LSTM - Epoch 10 Loss: 0.0688\n",
      "LSTM - Epoch 11 Loss: 0.0662\n",
      "LSTM - Epoch 12 Loss: 0.0626\n",
      "LSTM - Epoch 13 Loss: 0.0605\n",
      "LSTM - Epoch 14 Loss: 0.0592\n",
      "LSTM - Epoch 15 Loss: 0.0601\n",
      "LSTM - Epoch 16 Loss: 0.0591\n",
      "LSTM - Epoch 17 Loss: 0.0558\n",
      "LSTM - Epoch 18 Loss: 0.0591\n",
      "LSTM - Epoch 19 Loss: 0.0548\n",
      "LSTM - Epoch 20 Loss: 0.0564\n",
      "LSTM - Epoch 21 Loss: 0.0532\n",
      "LSTM - Epoch 22 Loss: 0.0576\n",
      "LSTM - Epoch 23 Loss: 0.0549\n",
      "LSTM - Epoch 24 Loss: 0.0530\n",
      "LSTM - Epoch 25 Loss: 0.0505\n",
      "LSTM - Epoch 26 Loss: 0.0529\n",
      "LSTM - Epoch 27 Loss: 0.0592\n",
      "LSTM - Epoch 28 Loss: 0.0528\n",
      "LSTM - Epoch 29 Loss: 0.0503\n",
      "LSTM - Epoch 30 Loss: 0.0541\n",
      "LSTM - Epoch 31 Loss: 0.0537\n",
      "LSTM - Epoch 32 Loss: 0.0524\n",
      "LSTM - Epoch 33 Loss: 0.0537\n",
      "LSTM - Epoch 34 Loss: 0.0532\n",
      "LSTM - Epoch 35 Loss: 0.0519\n",
      "LSTM - Epoch 36 Loss: 0.0503\n",
      "LSTM - Epoch 37 Loss: 0.0539\n",
      "LSTM - Epoch 38 Loss: 0.0518\n",
      "LSTM - Epoch 39 Loss: 0.0514\n",
      "LSTM - Epoch 40 Loss: 0.0519\n",
      "LSTM - Epoch 41 Loss: 0.0550\n",
      "LSTM - Epoch 42 Loss: 0.0518\n",
      "LSTM - Epoch 43 Loss: 0.0517\n",
      "LSTM - Epoch 44 Loss: 0.0537\n",
      "LSTM - Epoch 45 Loss: 0.0490\n",
      "LSTM - Epoch 46 Loss: 0.0505\n",
      "LSTM - Epoch 47 Loss: 0.0514\n",
      "LSTM - Epoch 48 Loss: 0.0489\n",
      "LSTM - Epoch 49 Loss: 0.0520\n",
      "LSTM - Epoch 50 Loss: 0.0477\n",
      "LSTM - Epoch 51 Loss: 0.0562\n",
      "LSTM - Epoch 52 Loss: 0.0492\n",
      "LSTM - Epoch 53 Loss: 0.0454\n",
      "LSTM - Epoch 54 Loss: 0.0486\n",
      "LSTM - Epoch 55 Loss: 0.0478\n",
      "LSTM - Epoch 56 Loss: 0.0473\n",
      "LSTM - Epoch 57 Loss: 0.0450\n",
      "LSTM - Epoch 58 Loss: 0.0484\n",
      "LSTM - Epoch 59 Loss: 0.0542\n",
      "LSTM - Epoch 60 Loss: 0.0501\n",
      "LSTM - Epoch 61 Loss: 0.0431\n",
      "LSTM - Epoch 62 Loss: 0.0431\n",
      "LSTM - Epoch 63 Loss: 0.0496\n",
      "LSTM - Epoch 64 Loss: 0.0450\n",
      "LSTM - Epoch 65 Loss: 0.0436\n",
      "LSTM - Epoch 66 Loss: 0.0460\n",
      "LSTM - Epoch 67 Loss: 0.0463\n",
      "LSTM - Epoch 68 Loss: 0.0460\n",
      "LSTM - Epoch 69 Loss: 0.0498\n",
      "LSTM - Epoch 70 Loss: 0.0487\n",
      "LSTM - Epoch 71 Loss: 0.0452\n",
      "LSTM - Epoch 72 Loss: 0.0478\n",
      "LSTM - Epoch 73 Loss: 0.0471\n",
      "LSTM - Epoch 74 Loss: 0.0485\n",
      "LSTM - Epoch 75 Loss: 0.0446\n",
      "LSTM - Epoch 76 Loss: 0.0431\n",
      "LSTM - Epoch 77 Loss: 0.0459\n",
      "LSTM - Epoch 78 Loss: 0.0439\n",
      "LSTM - Epoch 79 Loss: 0.0478\n",
      "LSTM - Epoch 80 Loss: 0.0437\n",
      "LSTM - Epoch 81 Loss: 0.0456\n",
      "LSTM - Epoch 82 Loss: 0.0459\n",
      "LSTM - Epoch 83 Loss: 0.0419\n",
      "LSTM - Epoch 84 Loss: 0.0457\n",
      "LSTM - Epoch 85 Loss: 0.0444\n",
      "LSTM - Epoch 86 Loss: 0.0405\n",
      "LSTM - Epoch 87 Loss: 0.0423\n",
      "LSTM - Epoch 88 Loss: 0.0422\n",
      "LSTM - Epoch 89 Loss: 0.0453\n",
      "LSTM - Epoch 90 Loss: 0.0443\n",
      "LSTM - Epoch 91 Loss: 0.0456\n",
      "LSTM - Epoch 92 Loss: 0.0415\n",
      "LSTM - Epoch 93 Loss: 0.0428\n",
      "LSTM - Epoch 94 Loss: 0.0447\n",
      "LSTM - Epoch 95 Loss: 0.0450\n",
      "LSTM - Epoch 96 Loss: 0.0475\n",
      "LSTM - Epoch 97 Loss: 0.0440\n",
      "LSTM - Epoch 98 Loss: 0.0426\n",
      "LSTM - Epoch 99 Loss: 0.0432\n",
      "LSTM - Epoch 100 Loss: 0.0425\n",
      "LSTM - Accuracy: 0.9878, Precision: 0.9843, Recall: 0.9766, F1: 0.9804\n",
      "\n",
      "Training model: GRU\n",
      "GRU - Epoch 1 Loss: 0.2526\n",
      "GRU - Epoch 2 Loss: 0.1218\n",
      "GRU - Epoch 3 Loss: 0.1017\n",
      "GRU - Epoch 4 Loss: 0.0869\n",
      "GRU - Epoch 5 Loss: 0.0906\n",
      "GRU - Epoch 6 Loss: 0.0901\n",
      "GRU - Epoch 7 Loss: 0.0749\n",
      "GRU - Epoch 8 Loss: 0.0784\n",
      "GRU - Epoch 9 Loss: 0.0763\n",
      "GRU - Epoch 10 Loss: 0.0736\n",
      "GRU - Epoch 11 Loss: 0.0772\n",
      "GRU - Epoch 12 Loss: 0.0701\n",
      "GRU - Epoch 13 Loss: 0.0654\n",
      "GRU - Epoch 14 Loss: 0.0649\n",
      "GRU - Epoch 15 Loss: 0.0640\n",
      "GRU - Epoch 16 Loss: 0.0609\n",
      "GRU - Epoch 17 Loss: 0.0698\n",
      "GRU - Epoch 18 Loss: 0.0652\n",
      "GRU - Epoch 19 Loss: 0.0630\n",
      "GRU - Epoch 20 Loss: 0.0621\n",
      "GRU - Epoch 21 Loss: 0.0609\n",
      "GRU - Epoch 22 Loss: 0.0571\n",
      "GRU - Epoch 23 Loss: 0.0606\n",
      "GRU - Epoch 24 Loss: 0.0595\n",
      "GRU - Epoch 25 Loss: 0.0580\n",
      "GRU - Epoch 26 Loss: 0.0638\n",
      "GRU - Epoch 27 Loss: 0.0631\n",
      "GRU - Epoch 28 Loss: 0.0673\n",
      "GRU - Epoch 29 Loss: 0.0540\n",
      "GRU - Epoch 30 Loss: 0.0622\n",
      "GRU - Epoch 31 Loss: 0.0656\n",
      "GRU - Epoch 32 Loss: 0.0557\n",
      "GRU - Epoch 33 Loss: 0.0647\n",
      "GRU - Epoch 34 Loss: 0.0548\n",
      "GRU - Epoch 35 Loss: 0.0559\n",
      "GRU - Epoch 36 Loss: 0.0576\n",
      "GRU - Epoch 37 Loss: 0.0634\n",
      "GRU - Epoch 38 Loss: 0.0682\n",
      "GRU - Epoch 39 Loss: 0.0647\n",
      "GRU - Epoch 40 Loss: 0.0589\n",
      "GRU - Epoch 41 Loss: 0.0594\n",
      "GRU - Epoch 42 Loss: 0.0583\n",
      "GRU - Epoch 43 Loss: 0.0614\n",
      "GRU - Epoch 44 Loss: 0.0575\n",
      "GRU - Epoch 45 Loss: 0.0566\n",
      "GRU - Epoch 46 Loss: 0.0551\n",
      "GRU - Epoch 47 Loss: 0.0581\n",
      "GRU - Epoch 48 Loss: 0.0501\n",
      "GRU - Epoch 49 Loss: 0.0512\n",
      "GRU - Epoch 50 Loss: 0.0531\n",
      "GRU - Epoch 51 Loss: 0.0505\n",
      "GRU - Epoch 52 Loss: 0.0529\n",
      "GRU - Epoch 53 Loss: 0.0516\n",
      "GRU - Epoch 54 Loss: 0.0575\n",
      "GRU - Epoch 55 Loss: 0.0486\n",
      "GRU - Epoch 56 Loss: 0.0595\n",
      "GRU - Epoch 57 Loss: 0.0490\n",
      "GRU - Epoch 58 Loss: 0.0504\n",
      "GRU - Epoch 59 Loss: 0.0459\n",
      "GRU - Epoch 60 Loss: 0.0483\n",
      "GRU - Epoch 61 Loss: 0.0547\n",
      "GRU - Epoch 62 Loss: 0.0497\n",
      "GRU - Epoch 63 Loss: 0.0533\n",
      "GRU - Epoch 64 Loss: 0.0533\n",
      "GRU - Epoch 65 Loss: 0.0521\n",
      "GRU - Epoch 66 Loss: 0.0444\n",
      "GRU - Epoch 67 Loss: 0.0499\n",
      "GRU - Epoch 68 Loss: 0.0450\n",
      "GRU - Epoch 69 Loss: 0.0528\n",
      "GRU - Epoch 70 Loss: 0.0477\n",
      "GRU - Epoch 71 Loss: 0.0468\n",
      "GRU - Epoch 72 Loss: 0.0463\n",
      "GRU - Epoch 73 Loss: 0.0459\n",
      "GRU - Epoch 74 Loss: 0.0508\n",
      "GRU - Epoch 75 Loss: 0.0495\n",
      "GRU - Epoch 76 Loss: 0.0444\n",
      "GRU - Epoch 77 Loss: 0.0433\n",
      "GRU - Epoch 78 Loss: 0.0431\n",
      "GRU - Epoch 79 Loss: 0.0546\n",
      "GRU - Epoch 80 Loss: 0.0494\n",
      "GRU - Epoch 81 Loss: 0.0492\n",
      "GRU - Epoch 82 Loss: 0.0469\n",
      "GRU - Epoch 83 Loss: 0.0458\n",
      "GRU - Epoch 84 Loss: 0.0468\n",
      "GRU - Epoch 85 Loss: 0.0435\n",
      "GRU - Epoch 86 Loss: 0.0544\n",
      "GRU - Epoch 87 Loss: 0.0441\n",
      "GRU - Epoch 88 Loss: 0.0453\n",
      "GRU - Epoch 89 Loss: 0.0483\n",
      "GRU - Epoch 90 Loss: 0.0473\n",
      "GRU - Epoch 91 Loss: 0.0521\n",
      "GRU - Epoch 92 Loss: 0.0492\n",
      "GRU - Epoch 93 Loss: 0.0543\n",
      "GRU - Epoch 94 Loss: 0.0537\n",
      "GRU - Epoch 95 Loss: 0.0482\n",
      "GRU - Epoch 96 Loss: 0.0516\n",
      "GRU - Epoch 97 Loss: 0.0449\n",
      "GRU - Epoch 98 Loss: 0.0476\n",
      "GRU - Epoch 99 Loss: 0.0495\n",
      "GRU - Epoch 100 Loss: 0.0449\n",
      "GRU - Accuracy: 0.9829, Precision: 1.0000, Recall: 0.9453, F1: 0.9719\n",
      "\n",
      "Training model: GCN\n",
      "GCN - Epoch 1 Loss: 0.2844\n",
      "GCN - Epoch 2 Loss: 0.1371\n",
      "GCN - Epoch 3 Loss: 0.1145\n",
      "GCN - Epoch 4 Loss: 0.1013\n",
      "GCN - Epoch 5 Loss: 0.0992\n",
      "GCN - Epoch 6 Loss: 0.0902\n",
      "GCN - Epoch 7 Loss: 0.0804\n",
      "GCN - Epoch 8 Loss: 0.0787\n",
      "GCN - Epoch 9 Loss: 0.0797\n",
      "GCN - Epoch 10 Loss: 0.0778\n",
      "GCN - Epoch 11 Loss: 0.0767\n",
      "GCN - Epoch 12 Loss: 0.0662\n",
      "GCN - Epoch 13 Loss: 0.0726\n",
      "GCN - Epoch 14 Loss: 0.0664\n",
      "GCN - Epoch 15 Loss: 0.0651\n",
      "GCN - Epoch 16 Loss: 0.0651\n",
      "GCN - Epoch 17 Loss: 0.0607\n",
      "GCN - Epoch 18 Loss: 0.0610\n",
      "GCN - Epoch 19 Loss: 0.0581\n",
      "GCN - Epoch 20 Loss: 0.0594\n",
      "GCN - Epoch 21 Loss: 0.0584\n",
      "GCN - Epoch 22 Loss: 0.0558\n",
      "GCN - Epoch 23 Loss: 0.0539\n",
      "GCN - Epoch 24 Loss: 0.0583\n",
      "GCN - Epoch 25 Loss: 0.0578\n",
      "GCN - Epoch 26 Loss: 0.0579\n",
      "GCN - Epoch 27 Loss: 0.0547\n",
      "GCN - Epoch 28 Loss: 0.0590\n",
      "GCN - Epoch 29 Loss: 0.0598\n",
      "GCN - Epoch 30 Loss: 0.0536\n",
      "GCN - Epoch 31 Loss: 0.0533\n",
      "GCN - Epoch 32 Loss: 0.0528\n",
      "GCN - Epoch 33 Loss: 0.0514\n",
      "GCN - Epoch 34 Loss: 0.0570\n",
      "GCN - Epoch 35 Loss: 0.0545\n",
      "GCN - Epoch 36 Loss: 0.0534\n",
      "GCN - Epoch 37 Loss: 0.0517\n",
      "GCN - Epoch 38 Loss: 0.0569\n",
      "GCN - Epoch 39 Loss: 0.0490\n",
      "GCN - Epoch 40 Loss: 0.0516\n",
      "GCN - Epoch 41 Loss: 0.0519\n",
      "GCN - Epoch 42 Loss: 0.0535\n",
      "GCN - Epoch 43 Loss: 0.0567\n",
      "GCN - Epoch 44 Loss: 0.0482\n",
      "GCN - Epoch 45 Loss: 0.0491\n",
      "GCN - Epoch 46 Loss: 0.0494\n",
      "GCN - Epoch 47 Loss: 0.0510\n",
      "GCN - Epoch 48 Loss: 0.0466\n",
      "GCN - Epoch 49 Loss: 0.0463\n",
      "GCN - Epoch 50 Loss: 0.0461\n",
      "GCN - Epoch 51 Loss: 0.0443\n",
      "GCN - Epoch 52 Loss: 0.0467\n",
      "GCN - Epoch 53 Loss: 0.0483\n",
      "GCN - Epoch 54 Loss: 0.0459\n",
      "GCN - Epoch 55 Loss: 0.0461\n",
      "GCN - Epoch 56 Loss: 0.0472\n",
      "GCN - Epoch 57 Loss: 0.0500\n",
      "GCN - Epoch 58 Loss: 0.0450\n",
      "GCN - Epoch 59 Loss: 0.0442\n",
      "GCN - Epoch 60 Loss: 0.0446\n",
      "GCN - Epoch 61 Loss: 0.0422\n",
      "GCN - Epoch 62 Loss: 0.0498\n",
      "GCN - Epoch 63 Loss: 0.0464\n",
      "GCN - Epoch 64 Loss: 0.0403\n",
      "GCN - Epoch 65 Loss: 0.0484\n",
      "GCN - Epoch 66 Loss: 0.0462\n",
      "GCN - Epoch 67 Loss: 0.0466\n",
      "GCN - Epoch 68 Loss: 0.0484\n",
      "GCN - Epoch 69 Loss: 0.0450\n",
      "GCN - Epoch 70 Loss: 0.0447\n",
      "GCN - Epoch 71 Loss: 0.0420\n",
      "GCN - Epoch 72 Loss: 0.0429\n",
      "GCN - Epoch 73 Loss: 0.0434\n",
      "GCN - Epoch 74 Loss: 0.0437\n",
      "GCN - Epoch 75 Loss: 0.0406\n",
      "GCN - Epoch 76 Loss: 0.0440\n",
      "GCN - Epoch 77 Loss: 0.0435\n",
      "GCN - Epoch 78 Loss: 0.0405\n",
      "GCN - Epoch 79 Loss: 0.0463\n",
      "GCN - Epoch 80 Loss: 0.0433\n",
      "GCN - Epoch 81 Loss: 0.0411\n",
      "GCN - Epoch 82 Loss: 0.0389\n",
      "GCN - Epoch 83 Loss: 0.0439\n",
      "GCN - Epoch 84 Loss: 0.0405\n",
      "GCN - Epoch 85 Loss: 0.0396\n",
      "GCN - Epoch 86 Loss: 0.0480\n",
      "GCN - Epoch 87 Loss: 0.0400\n",
      "GCN - Epoch 88 Loss: 0.0388\n",
      "GCN - Epoch 89 Loss: 0.0408\n",
      "GCN - Epoch 90 Loss: 0.0402\n",
      "GCN - Epoch 91 Loss: 0.0398\n",
      "GCN - Epoch 92 Loss: 0.0436\n",
      "GCN - Epoch 93 Loss: 0.0415\n",
      "GCN - Epoch 94 Loss: 0.0386\n",
      "GCN - Epoch 95 Loss: 0.0421\n",
      "GCN - Epoch 96 Loss: 0.0418\n",
      "GCN - Epoch 97 Loss: 0.0384\n",
      "GCN - Epoch 98 Loss: 0.0401\n",
      "GCN - Epoch 99 Loss: 0.0416\n",
      "GCN - Epoch 100 Loss: 0.0390\n",
      "GCN - Accuracy: 0.9927, Precision: 1.0000, Recall: 0.9766, F1: 0.9881\n",
      "\n",
      "Training model: GCN_LSTM\n",
      "GCN_LSTM - Epoch 1 Loss: 0.2885\n",
      "GCN_LSTM - Epoch 2 Loss: 0.1273\n",
      "GCN_LSTM - Epoch 3 Loss: 0.1129\n",
      "GCN_LSTM - Epoch 4 Loss: 0.1022\n",
      "GCN_LSTM - Epoch 5 Loss: 0.0996\n",
      "GCN_LSTM - Epoch 6 Loss: 0.0896\n",
      "GCN_LSTM - Epoch 7 Loss: 0.0818\n",
      "GCN_LSTM - Epoch 8 Loss: 0.0780\n",
      "GCN_LSTM - Epoch 9 Loss: 0.0785\n",
      "GCN_LSTM - Epoch 10 Loss: 0.0781\n",
      "GCN_LSTM - Epoch 11 Loss: 0.0791\n",
      "GCN_LSTM - Epoch 12 Loss: 0.0702\n",
      "GCN_LSTM - Epoch 13 Loss: 0.0745\n",
      "GCN_LSTM - Epoch 14 Loss: 0.0721\n",
      "GCN_LSTM - Epoch 15 Loss: 0.0771\n",
      "GCN_LSTM - Epoch 16 Loss: 0.0644\n",
      "GCN_LSTM - Epoch 17 Loss: 0.0642\n",
      "GCN_LSTM - Epoch 18 Loss: 0.0672\n",
      "GCN_LSTM - Epoch 19 Loss: 0.0677\n",
      "GCN_LSTM - Epoch 20 Loss: 0.0629\n",
      "GCN_LSTM - Epoch 21 Loss: 0.0605\n",
      "GCN_LSTM - Epoch 22 Loss: 0.0653\n",
      "GCN_LSTM - Epoch 23 Loss: 0.0619\n",
      "GCN_LSTM - Epoch 24 Loss: 0.0628\n",
      "GCN_LSTM - Epoch 25 Loss: 0.0667\n",
      "GCN_LSTM - Epoch 26 Loss: 0.0592\n",
      "GCN_LSTM - Epoch 27 Loss: 0.0626\n",
      "GCN_LSTM - Epoch 28 Loss: 0.0591\n",
      "GCN_LSTM - Epoch 29 Loss: 0.0573\n",
      "GCN_LSTM - Epoch 30 Loss: 0.0652\n",
      "GCN_LSTM - Epoch 31 Loss: 0.0538\n",
      "GCN_LSTM - Epoch 32 Loss: 0.0580\n",
      "GCN_LSTM - Epoch 33 Loss: 0.0604\n",
      "GCN_LSTM - Epoch 34 Loss: 0.0548\n",
      "GCN_LSTM - Epoch 35 Loss: 0.0577\n",
      "GCN_LSTM - Epoch 36 Loss: 0.0527\n",
      "GCN_LSTM - Epoch 37 Loss: 0.0506\n",
      "GCN_LSTM - Epoch 38 Loss: 0.0541\n",
      "GCN_LSTM - Epoch 39 Loss: 0.0527\n",
      "GCN_LSTM - Epoch 40 Loss: 0.0530\n",
      "GCN_LSTM - Epoch 41 Loss: 0.0537\n",
      "GCN_LSTM - Epoch 42 Loss: 0.0532\n",
      "GCN_LSTM - Epoch 43 Loss: 0.0484\n",
      "GCN_LSTM - Epoch 44 Loss: 0.0545\n",
      "GCN_LSTM - Epoch 45 Loss: 0.0523\n",
      "GCN_LSTM - Epoch 46 Loss: 0.0513\n",
      "GCN_LSTM - Epoch 47 Loss: 0.0495\n",
      "GCN_LSTM - Epoch 48 Loss: 0.0498\n",
      "GCN_LSTM - Epoch 49 Loss: 0.0504\n",
      "GCN_LSTM - Epoch 50 Loss: 0.0521\n",
      "GCN_LSTM - Epoch 51 Loss: 0.0438\n",
      "GCN_LSTM - Epoch 52 Loss: 0.0533\n",
      "GCN_LSTM - Epoch 53 Loss: 0.0485\n",
      "GCN_LSTM - Epoch 54 Loss: 0.0460\n",
      "GCN_LSTM - Epoch 55 Loss: 0.0496\n",
      "GCN_LSTM - Epoch 56 Loss: 0.0491\n",
      "GCN_LSTM - Epoch 57 Loss: 0.0461\n",
      "GCN_LSTM - Epoch 58 Loss: 0.0464\n",
      "GCN_LSTM - Epoch 59 Loss: 0.0480\n",
      "GCN_LSTM - Epoch 60 Loss: 0.0446\n",
      "GCN_LSTM - Epoch 61 Loss: 0.0427\n",
      "GCN_LSTM - Epoch 62 Loss: 0.0472\n",
      "GCN_LSTM - Epoch 63 Loss: 0.0506\n",
      "GCN_LSTM - Epoch 64 Loss: 0.0430\n",
      "GCN_LSTM - Epoch 65 Loss: 0.0413\n",
      "GCN_LSTM - Epoch 66 Loss: 0.0465\n",
      "GCN_LSTM - Epoch 67 Loss: 0.0446\n",
      "GCN_LSTM - Epoch 68 Loss: 0.0430\n",
      "GCN_LSTM - Epoch 69 Loss: 0.0482\n",
      "GCN_LSTM - Epoch 70 Loss: 0.0429\n",
      "GCN_LSTM - Epoch 71 Loss: 0.0414\n",
      "GCN_LSTM - Epoch 72 Loss: 0.0459\n",
      "GCN_LSTM - Epoch 73 Loss: 0.0452\n",
      "GCN_LSTM - Epoch 74 Loss: 0.0487\n",
      "GCN_LSTM - Epoch 75 Loss: 0.0420\n",
      "GCN_LSTM - Epoch 76 Loss: 0.0456\n",
      "GCN_LSTM - Epoch 77 Loss: 0.0426\n",
      "GCN_LSTM - Epoch 78 Loss: 0.0474\n",
      "GCN_LSTM - Epoch 79 Loss: 0.0401\n",
      "GCN_LSTM - Epoch 80 Loss: 0.0438\n",
      "GCN_LSTM - Epoch 81 Loss: 0.0394\n",
      "GCN_LSTM - Epoch 82 Loss: 0.0450\n",
      "GCN_LSTM - Epoch 83 Loss: 0.0438\n",
      "GCN_LSTM - Epoch 84 Loss: 0.0451\n",
      "GCN_LSTM - Epoch 85 Loss: 0.0401\n",
      "GCN_LSTM - Epoch 86 Loss: 0.0437\n",
      "GCN_LSTM - Epoch 87 Loss: 0.0425\n",
      "GCN_LSTM - Epoch 88 Loss: 0.0417\n",
      "GCN_LSTM - Epoch 89 Loss: 0.0424\n",
      "GCN_LSTM - Epoch 90 Loss: 0.0422\n",
      "GCN_LSTM - Epoch 91 Loss: 0.0456\n",
      "GCN_LSTM - Epoch 92 Loss: 0.0384\n",
      "GCN_LSTM - Epoch 93 Loss: 0.0431\n",
      "GCN_LSTM - Epoch 94 Loss: 0.0392\n",
      "GCN_LSTM - Epoch 95 Loss: 0.0401\n",
      "GCN_LSTM - Epoch 96 Loss: 0.0415\n",
      "GCN_LSTM - Epoch 97 Loss: 0.0373\n",
      "GCN_LSTM - Epoch 98 Loss: 0.0397\n",
      "GCN_LSTM - Epoch 99 Loss: 0.0502\n",
      "GCN_LSTM - Epoch 100 Loss: 0.0385\n",
      "GCN_LSTM - Accuracy: 0.9878, Precision: 0.9920, Recall: 0.9688, F1: 0.9802\n",
      "\n",
      "Training model: GCN_GRU\n",
      "GCN_GRU - Epoch 1 Loss: 0.2752\n",
      "GCN_GRU - Epoch 2 Loss: 0.1228\n",
      "GCN_GRU - Epoch 3 Loss: 0.1125\n",
      "GCN_GRU - Epoch 4 Loss: 0.0936\n",
      "GCN_GRU - Epoch 5 Loss: 0.0895\n",
      "GCN_GRU - Epoch 6 Loss: 0.0831\n",
      "GCN_GRU - Epoch 7 Loss: 0.0822\n",
      "GCN_GRU - Epoch 8 Loss: 0.0699\n",
      "GCN_GRU - Epoch 9 Loss: 0.0748\n",
      "GCN_GRU - Epoch 10 Loss: 0.0738\n",
      "GCN_GRU - Epoch 11 Loss: 0.0691\n",
      "GCN_GRU - Epoch 12 Loss: 0.0748\n",
      "GCN_GRU - Epoch 13 Loss: 0.0659\n",
      "GCN_GRU - Epoch 14 Loss: 0.0656\n",
      "GCN_GRU - Epoch 15 Loss: 0.0641\n",
      "GCN_GRU - Epoch 16 Loss: 0.0700\n",
      "GCN_GRU - Epoch 17 Loss: 0.0648\n",
      "GCN_GRU - Epoch 18 Loss: 0.0645\n",
      "GCN_GRU - Epoch 19 Loss: 0.0682\n",
      "GCN_GRU - Epoch 20 Loss: 0.0659\n",
      "GCN_GRU - Epoch 21 Loss: 0.0606\n",
      "GCN_GRU - Epoch 22 Loss: 0.0574\n",
      "GCN_GRU - Epoch 23 Loss: 0.0562\n",
      "GCN_GRU - Epoch 24 Loss: 0.0541\n",
      "GCN_GRU - Epoch 25 Loss: 0.0572\n",
      "GCN_GRU - Epoch 26 Loss: 0.0618\n",
      "GCN_GRU - Epoch 27 Loss: 0.0619\n",
      "GCN_GRU - Epoch 28 Loss: 0.0564\n",
      "GCN_GRU - Epoch 29 Loss: 0.0519\n",
      "GCN_GRU - Epoch 30 Loss: 0.0563\n",
      "GCN_GRU - Epoch 31 Loss: 0.0533\n",
      "GCN_GRU - Epoch 32 Loss: 0.0542\n",
      "GCN_GRU - Epoch 33 Loss: 0.0603\n",
      "GCN_GRU - Epoch 34 Loss: 0.0487\n",
      "GCN_GRU - Epoch 35 Loss: 0.0517\n",
      "GCN_GRU - Epoch 36 Loss: 0.0550\n",
      "GCN_GRU - Epoch 37 Loss: 0.0533\n",
      "GCN_GRU - Epoch 38 Loss: 0.0500\n",
      "GCN_GRU - Epoch 39 Loss: 0.0539\n",
      "GCN_GRU - Epoch 40 Loss: 0.0538\n",
      "GCN_GRU - Epoch 41 Loss: 0.0500\n",
      "GCN_GRU - Epoch 42 Loss: 0.0485\n",
      "GCN_GRU - Epoch 43 Loss: 0.0528\n",
      "GCN_GRU - Epoch 44 Loss: 0.0477\n",
      "GCN_GRU - Epoch 45 Loss: 0.0494\n",
      "GCN_GRU - Epoch 46 Loss: 0.0504\n",
      "GCN_GRU - Epoch 47 Loss: 0.0518\n",
      "GCN_GRU - Epoch 48 Loss: 0.0490\n",
      "GCN_GRU - Epoch 49 Loss: 0.0485\n",
      "GCN_GRU - Epoch 50 Loss: 0.0475\n",
      "GCN_GRU - Epoch 51 Loss: 0.0477\n",
      "GCN_GRU - Epoch 52 Loss: 0.0489\n",
      "GCN_GRU - Epoch 53 Loss: 0.0511\n",
      "GCN_GRU - Epoch 54 Loss: 0.0441\n",
      "GCN_GRU - Epoch 55 Loss: 0.0488\n",
      "GCN_GRU - Epoch 56 Loss: 0.0467\n",
      "GCN_GRU - Epoch 57 Loss: 0.0472\n",
      "GCN_GRU - Epoch 58 Loss: 0.0465\n",
      "GCN_GRU - Epoch 59 Loss: 0.0461\n",
      "GCN_GRU - Epoch 60 Loss: 0.0466\n",
      "GCN_GRU - Epoch 61 Loss: 0.0469\n",
      "GCN_GRU - Epoch 62 Loss: 0.0448\n",
      "GCN_GRU - Epoch 63 Loss: 0.0415\n",
      "GCN_GRU - Epoch 64 Loss: 0.0461\n",
      "GCN_GRU - Epoch 65 Loss: 0.0421\n",
      "GCN_GRU - Epoch 66 Loss: 0.0462\n",
      "GCN_GRU - Epoch 67 Loss: 0.0439\n",
      "GCN_GRU - Epoch 68 Loss: 0.0465\n",
      "GCN_GRU - Epoch 69 Loss: 0.0436\n",
      "GCN_GRU - Epoch 70 Loss: 0.0476\n",
      "GCN_GRU - Epoch 71 Loss: 0.0455\n",
      "GCN_GRU - Epoch 72 Loss: 0.0430\n",
      "GCN_GRU - Epoch 73 Loss: 0.0459\n",
      "GCN_GRU - Epoch 74 Loss: 0.0438\n",
      "GCN_GRU - Epoch 75 Loss: 0.0454\n",
      "GCN_GRU - Epoch 76 Loss: 0.0492\n",
      "GCN_GRU - Epoch 77 Loss: 0.0461\n",
      "GCN_GRU - Epoch 78 Loss: 0.0421\n",
      "GCN_GRU - Epoch 79 Loss: 0.0458\n",
      "GCN_GRU - Epoch 80 Loss: 0.0409\n",
      "GCN_GRU - Epoch 81 Loss: 0.0470\n",
      "GCN_GRU - Epoch 82 Loss: 0.0421\n",
      "GCN_GRU - Epoch 83 Loss: 0.0416\n",
      "GCN_GRU - Epoch 84 Loss: 0.0420\n",
      "GCN_GRU - Epoch 85 Loss: 0.0407\n",
      "GCN_GRU - Epoch 86 Loss: 0.0520\n",
      "GCN_GRU - Epoch 87 Loss: 0.0407\n",
      "GCN_GRU - Epoch 88 Loss: 0.0404\n",
      "GCN_GRU - Epoch 89 Loss: 0.0419\n",
      "GCN_GRU - Epoch 90 Loss: 0.0417\n",
      "GCN_GRU - Epoch 91 Loss: 0.0439\n",
      "GCN_GRU - Epoch 92 Loss: 0.0397\n",
      "GCN_GRU - Epoch 93 Loss: 0.0421\n",
      "GCN_GRU - Epoch 94 Loss: 0.0454\n",
      "GCN_GRU - Epoch 95 Loss: 0.0392\n",
      "GCN_GRU - Epoch 96 Loss: 0.0430\n",
      "GCN_GRU - Epoch 97 Loss: 0.0396\n",
      "GCN_GRU - Epoch 98 Loss: 0.0417\n",
      "GCN_GRU - Epoch 99 Loss: 0.0441\n",
      "GCN_GRU - Epoch 100 Loss: 0.0396\n",
      "GCN_GRU - Accuracy: 0.9927, Precision: 0.9921, Recall: 0.9844, F1: 0.9882\n",
      "\n",
      "Training model: GCN_GRU_LSTM\n",
      "GCN_GRU_LSTM - Epoch 1 Loss: 0.2830\n",
      "GCN_GRU_LSTM - Epoch 2 Loss: 0.1235\n",
      "GCN_GRU_LSTM - Epoch 3 Loss: 0.1122\n",
      "GCN_GRU_LSTM - Epoch 4 Loss: 0.0879\n",
      "GCN_GRU_LSTM - Epoch 5 Loss: 0.0872\n",
      "GCN_GRU_LSTM - Epoch 6 Loss: 0.0843\n",
      "GCN_GRU_LSTM - Epoch 7 Loss: 0.0827\n",
      "GCN_GRU_LSTM - Epoch 8 Loss: 0.0815\n",
      "GCN_GRU_LSTM - Epoch 9 Loss: 0.0747\n",
      "GCN_GRU_LSTM - Epoch 10 Loss: 0.0764\n",
      "GCN_GRU_LSTM - Epoch 11 Loss: 0.0674\n",
      "GCN_GRU_LSTM - Epoch 12 Loss: 0.0716\n",
      "GCN_GRU_LSTM - Epoch 13 Loss: 0.0674\n",
      "GCN_GRU_LSTM - Epoch 14 Loss: 0.0647\n",
      "GCN_GRU_LSTM - Epoch 15 Loss: 0.0682\n",
      "GCN_GRU_LSTM - Epoch 16 Loss: 0.0634\n",
      "GCN_GRU_LSTM - Epoch 17 Loss: 0.0699\n",
      "GCN_GRU_LSTM - Epoch 18 Loss: 0.0589\n",
      "GCN_GRU_LSTM - Epoch 19 Loss: 0.0637\n",
      "GCN_GRU_LSTM - Epoch 20 Loss: 0.0636\n",
      "GCN_GRU_LSTM - Epoch 21 Loss: 0.0580\n",
      "GCN_GRU_LSTM - Epoch 22 Loss: 0.0664\n",
      "GCN_GRU_LSTM - Epoch 23 Loss: 0.0572\n",
      "GCN_GRU_LSTM - Epoch 24 Loss: 0.0509\n",
      "GCN_GRU_LSTM - Epoch 25 Loss: 0.0613\n",
      "GCN_GRU_LSTM - Epoch 26 Loss: 0.0595\n",
      "GCN_GRU_LSTM - Epoch 27 Loss: 0.0535\n",
      "GCN_GRU_LSTM - Epoch 28 Loss: 0.0587\n",
      "GCN_GRU_LSTM - Epoch 29 Loss: 0.0577\n",
      "GCN_GRU_LSTM - Epoch 30 Loss: 0.0563\n",
      "GCN_GRU_LSTM - Epoch 31 Loss: 0.0552\n",
      "GCN_GRU_LSTM - Epoch 32 Loss: 0.0565\n",
      "GCN_GRU_LSTM - Epoch 33 Loss: 0.0598\n",
      "GCN_GRU_LSTM - Epoch 34 Loss: 0.0636\n",
      "GCN_GRU_LSTM - Epoch 35 Loss: 0.0521\n",
      "GCN_GRU_LSTM - Epoch 36 Loss: 0.0540\n",
      "GCN_GRU_LSTM - Epoch 37 Loss: 0.0529\n",
      "GCN_GRU_LSTM - Epoch 38 Loss: 0.0540\n",
      "GCN_GRU_LSTM - Epoch 39 Loss: 0.0517\n",
      "GCN_GRU_LSTM - Epoch 40 Loss: 0.0514\n",
      "GCN_GRU_LSTM - Epoch 41 Loss: 0.0503\n",
      "GCN_GRU_LSTM - Epoch 42 Loss: 0.0489\n",
      "GCN_GRU_LSTM - Epoch 43 Loss: 0.0539\n",
      "GCN_GRU_LSTM - Epoch 44 Loss: 0.0542\n",
      "GCN_GRU_LSTM - Epoch 45 Loss: 0.0519\n",
      "GCN_GRU_LSTM - Epoch 46 Loss: 0.0516\n",
      "GCN_GRU_LSTM - Epoch 47 Loss: 0.0511\n",
      "GCN_GRU_LSTM - Epoch 48 Loss: 0.0491\n",
      "GCN_GRU_LSTM - Epoch 49 Loss: 0.0490\n",
      "GCN_GRU_LSTM - Epoch 50 Loss: 0.0495\n",
      "GCN_GRU_LSTM - Epoch 51 Loss: 0.0476\n",
      "GCN_GRU_LSTM - Epoch 52 Loss: 0.0512\n",
      "GCN_GRU_LSTM - Epoch 53 Loss: 0.0477\n",
      "GCN_GRU_LSTM - Epoch 54 Loss: 0.0526\n",
      "GCN_GRU_LSTM - Epoch 55 Loss: 0.0515\n",
      "GCN_GRU_LSTM - Epoch 56 Loss: 0.0482\n",
      "GCN_GRU_LSTM - Epoch 57 Loss: 0.0437\n",
      "GCN_GRU_LSTM - Epoch 58 Loss: 0.0502\n",
      "GCN_GRU_LSTM - Epoch 59 Loss: 0.0463\n",
      "GCN_GRU_LSTM - Epoch 60 Loss: 0.0476\n",
      "GCN_GRU_LSTM - Epoch 61 Loss: 0.0470\n",
      "GCN_GRU_LSTM - Epoch 62 Loss: 0.0450\n",
      "GCN_GRU_LSTM - Epoch 63 Loss: 0.0462\n",
      "GCN_GRU_LSTM - Epoch 64 Loss: 0.0470\n",
      "GCN_GRU_LSTM - Epoch 65 Loss: 0.0461\n",
      "GCN_GRU_LSTM - Epoch 66 Loss: 0.0467\n",
      "GCN_GRU_LSTM - Epoch 67 Loss: 0.0446\n",
      "GCN_GRU_LSTM - Epoch 68 Loss: 0.0435\n",
      "GCN_GRU_LSTM - Epoch 69 Loss: 0.0484\n",
      "GCN_GRU_LSTM - Epoch 70 Loss: 0.0510\n",
      "GCN_GRU_LSTM - Epoch 71 Loss: 0.0448\n",
      "GCN_GRU_LSTM - Epoch 72 Loss: 0.0485\n",
      "GCN_GRU_LSTM - Epoch 73 Loss: 0.0419\n",
      "GCN_GRU_LSTM - Epoch 74 Loss: 0.0446\n",
      "GCN_GRU_LSTM - Epoch 75 Loss: 0.0439\n",
      "GCN_GRU_LSTM - Epoch 76 Loss: 0.0448\n",
      "GCN_GRU_LSTM - Epoch 77 Loss: 0.0441\n",
      "GCN_GRU_LSTM - Epoch 78 Loss: 0.0409\n",
      "GCN_GRU_LSTM - Epoch 79 Loss: 0.0459\n",
      "GCN_GRU_LSTM - Epoch 80 Loss: 0.0426\n",
      "GCN_GRU_LSTM - Epoch 81 Loss: 0.0469\n",
      "GCN_GRU_LSTM - Epoch 82 Loss: 0.0417\n",
      "GCN_GRU_LSTM - Epoch 83 Loss: 0.0410\n",
      "GCN_GRU_LSTM - Epoch 84 Loss: 0.0434\n",
      "GCN_GRU_LSTM - Epoch 85 Loss: 0.0432\n",
      "GCN_GRU_LSTM - Epoch 86 Loss: 0.0425\n",
      "GCN_GRU_LSTM - Epoch 87 Loss: 0.0403\n",
      "GCN_GRU_LSTM - Epoch 88 Loss: 0.0415\n",
      "GCN_GRU_LSTM - Epoch 89 Loss: 0.0412\n",
      "GCN_GRU_LSTM - Epoch 90 Loss: 0.0417\n",
      "GCN_GRU_LSTM - Epoch 91 Loss: 0.0440\n",
      "GCN_GRU_LSTM - Epoch 92 Loss: 0.0418\n",
      "GCN_GRU_LSTM - Epoch 93 Loss: 0.0384\n",
      "GCN_GRU_LSTM - Epoch 94 Loss: 0.0429\n",
      "GCN_GRU_LSTM - Epoch 95 Loss: 0.0443\n",
      "GCN_GRU_LSTM - Epoch 96 Loss: 0.0396\n",
      "GCN_GRU_LSTM - Epoch 97 Loss: 0.0392\n",
      "GCN_GRU_LSTM - Epoch 98 Loss: 0.0426\n",
      "GCN_GRU_LSTM - Epoch 99 Loss: 0.0397\n",
      "GCN_GRU_LSTM - Epoch 100 Loss: 0.0383\n",
      "GCN_GRU_LSTM - Accuracy: 0.9927, Precision: 1.0000, Recall: 0.9766, F1: 0.9881\n",
      "Excel file './phase2_model_results_100epochs.xlsx' updated with true rankings.\n",
      "Separate line flow comparison file './line_flow_comparison_100epochs.xlsx' created.\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# GCN-LSTM, LSTM, GRU, and GCN Multi-Task Learning Models for Phase 2 Contingency Prediction\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "import xlsxwriter\n",
    "import os # Import the os module\n",
    "\n",
    "# Define the directory where files are saved and will be loaded from\n",
    "# !!! IMPORTANT: Ensure this matches the save_directory in the first cell !!!\n",
    "data_directory = \"./\" # Example: if you saved to 'generated_files'\n",
    "# Or use the absolute path: data_directory = \"/path/to/your/desired/directory\"\n",
    "\n",
    "# Load input files\n",
    "# Construct the full paths for loading\n",
    "load_file_path = os.path.join(data_directory, \"load_scenarios.xlsx\")\n",
    "cont_file_path = os.path.join(data_directory, \"n1_contingency_balanced_filled_complete.csv\")\n",
    "\n",
    "load_df = pd.read_excel(load_file_path)\n",
    "cont_df = pd.read_csv(cont_file_path)\n",
    "cont_df = cont_df[cont_df['Scenario'] < 1000].reset_index(drop=True)\n",
    "\n",
    "# Extract and reshape load features\n",
    "load_features = load_df[[\"P_mw\", \"Q_mvar\"]].values\n",
    "load_features = load_features.reshape(1000, 40)\n",
    "repeat_factor = 41\n",
    "load_features_expanded = np.repeat(load_features, repeat_factor, axis=0)\n",
    "\n",
    "# Extract voltages and line flows\n",
    "bus_cols = [col for col in cont_df.columns if col.startswith(\"V_bus_\")]\n",
    "line_cols = [col for col in cont_df.columns if col.startswith(\"Loading_line_\")]\n",
    "voltages = cont_df[bus_cols].values.astype(np.float32)\n",
    "line_flows = cont_df[line_cols].values.astype(np.float32)\n",
    "combined_input = np.concatenate([load_features_expanded, voltages, line_flows], axis=1)\n",
    "\n",
    "# Targets\n",
    "features_out = cont_df[bus_cols + line_cols].values.astype(np.float32)\n",
    "labels_class = cont_df['Severity'].values.astype(np.int64)\n",
    "labels_rank = cont_df[line_cols].values.astype(np.float32) / 100\n",
    "\n",
    "# Sanity checks\n",
    "print(\"Input shapes:\")\n",
    "print(\"- Combined input:\", combined_input.shape)\n",
    "print(\"- Target features:\", features_out.shape)\n",
    "print(\"- Severity labels:\", labels_class.shape)\n",
    "print(\"- Ranking shape:\", labels_rank.shape)\n",
    "\n",
    "assert combined_input.shape[0] == features_out.shape[0] == labels_class.shape[0] == labels_rank.shape[0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = combined_input[:990*41], combined_input[990*41:]\n",
    "y_class_train, y_class_test = labels_class[:990*41], labels_class[990*41:]\n",
    "y_rank_train, y_rank_test = labels_rank[:990*41], labels_rank[990*41:]\n",
    "Y_train, Y_test = features_out[:990*41], features_out[990*41:]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, Y_test.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_class_train), torch.tensor(y_rank_train)), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_class_test), torch.tensor(y_rank_test)), batch_size=64)\n",
    "\n",
    "# Model Definitions (These are standard PyTorch modules, no changes needed for environment)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class LSTM_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size, batch_first=True)\n",
    "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
    "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h = h_n[-1]\n",
    "        return self.fc_cls(h), self.fc_rank(h)\n",
    "\n",
    "class GRU_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_size, batch_first=True)\n",
    "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
    "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, h_n = self.gru(x)\n",
    "        h = h_n[-1]\n",
    "        return self.fc_cls(h), self.fc_rank(h)\n",
    "\n",
    "class BaseMTL(nn.Module):\n",
    "    def __init__(self, base, hidden_size):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(hidden_size, 2)\n",
    "        self.regressor = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        return self.classifier(x), self.regressor(x)\n",
    "\n",
    "# Training and Evaluation\n",
    "all_results = []\n",
    "rank_matrix = {}\n",
    "class_matrix = {}\n",
    "class_pred_matrix = {}\n",
    "true_rank_matrix = np.argsort(-y_rank_test.reshape(-1, 41), axis=1) + 1\n",
    "\n",
    "input_dim = combined_input.shape[1]\n",
    "hidden_size = 64\n",
    "\n",
    "def train_and_evaluate(model_name, model, train_loader, test_loader):\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_rank = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(100):  # 100 epochs\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb_cls, yb_rank in train_loader:\n",
    "            out_cls, out_rank = model(xb)\n",
    "            loss_cls = criterion_class(out_cls, yb_cls)\n",
    "            loss_rank = criterion_rank(out_rank, yb_rank)\n",
    "            loss = loss_cls + 0.5 * loss_rank\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"{model_name} - Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_true, all_pred, pred_scores = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb_cls, yb_rank in test_loader:\n",
    "            out_cls, out_rank = model(xb)\n",
    "            preds = torch.argmax(out_cls, dim=1)\n",
    "            all_true.extend(yb_cls.cpu().numpy())\n",
    "            all_pred.extend(preds.cpu().numpy())\n",
    "            pred_scores.extend(out_rank.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_true, all_pred)\n",
    "    prec = precision_score(all_true, all_pred, zero_division=0)\n",
    "    rec = recall_score(all_true, all_pred, zero_division=0)\n",
    "    f1 = f1_score(all_true, all_pred, zero_division=0)\n",
    "\n",
    "    all_results.append({\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1})\n",
    "    class_matrix[model_name] = np.vstack(pred_scores)\n",
    "    class_pred_matrix[model_name] = np.array(all_pred).reshape(-1, 41)\n",
    "    rank_matrix[model_name] = np.argsort(-np.vstack(pred_scores), axis=1) + 1\n",
    "\n",
    "    print(f\"{model_name} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "train_and_evaluate(\"LSTM\", LSTM_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GRU\", GRU_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_GRU\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_GRU_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "\n",
    "\n",
    "# Saving results to Excel\n",
    "# Construct the full path for saving the main results file\n",
    "main_results_excel_path = os.path.join(data_directory, \"phase2_model_results_100epochs.xlsx\")\n",
    "with pd.ExcelWriter(main_results_excel_path, engine='xlsxwriter') as writer:\n",
    "    pd.DataFrame(all_results).to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    for model in rank_matrix:\n",
    "        df_rank = pd.DataFrame(rank_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(rank_matrix[model].shape[0])])\n",
    "        df_rank.to_excel(writer, sheet_name=f\"{model}_Ranking\")\n",
    "        df_cls = pd.DataFrame(class_pred_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(class_pred_matrix[model].shape[0])])\n",
    "        df_cls.to_excel(writer, sheet_name=f\"{model}_Classify\")\n",
    "    df_true_severity = pd.DataFrame(np.array(y_class_test).reshape(-1, 41).T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(y_class_test)//41)])\n",
    "    df_true_severity.to_excel(writer, sheet_name=\"True_Severity\")\n",
    "    df_true_rank = pd.DataFrame(true_rank_matrix.T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(true_rank_matrix))])\n",
    "    df_true_rank.to_excel(writer, sheet_name=\"True_Ranking\")\n",
    "\n",
    "print(f\"Excel file '{main_results_excel_path}' updated with true rankings.\")\n",
    "\n",
    "\n",
    "# Saving line flow comparison file\n",
    "flow_comparison_excel_path = os.path.join(data_directory, \"line_flow_comparison_100epochs.xlsx\")\n",
    "with pd.ExcelWriter(flow_comparison_excel_path, engine='xlsxwriter') as writer:\n",
    "    test_df = cont_df[cont_df['Scenario'] >= 990].reset_index(drop=True)\n",
    "\n",
    "    true_flow_matrix = []\n",
    "    true_columns = []\n",
    "    for scenario_id in range(990, 1000):\n",
    "        scenario_data = test_df[test_df['Scenario'] == scenario_id].reset_index(drop=True)\n",
    "        for outage_id in range(41):\n",
    "            outaged_line = scenario_data.loc[outage_id, 'Outaged_Line']\n",
    "            flow_row = []\n",
    "            for line_id in range(41):\n",
    "                flow = 0.0 if line_id == outage_id else scenario_data.loc[outage_id, f\"Loading_line_{line_id}\"] # Corrected line_id to outage_id for the flow == 0.0 condition\n",
    "                flow_row.append(flow)\n",
    "            true_flow_matrix.append(flow_row)\n",
    "            true_columns.append(f\"Scenario_{(scenario_id-990)*41 + outage_id}\")\n",
    "\n",
    "    df_true_flows = pd.DataFrame(np.array(true_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=true_columns)\n",
    "    df_true_flows.to_excel(writer, sheet_name=\"True_Line_Flows\")\n",
    "\n",
    "    for model_name, preds in class_matrix.items():\n",
    "        pred_flow_matrix = []\n",
    "        pred_columns = []\n",
    "        for idx in range(preds.shape[0]):\n",
    "            scenario_idx = idx // 41\n",
    "            outage_idx = idx % 41\n",
    "            flow_row = []\n",
    "            for line_id in range(41):\n",
    "                flow = 0.0 if line_id == outage_idx else preds[idx][line_id] * 100\n",
    "                flow_row.append(flow)\n",
    "            pred_flow_matrix.append(flow_row)\n",
    "            pred_columns.append(f\"Scenario_{idx}\")\n",
    "\n",
    "        df_pred_flows = pd.DataFrame(np.array(pred_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=pred_columns)\n",
    "        df_pred_flows.to_excel(writer, sheet_name=f\"Pred_{model_name}_Flows\")\n",
    "\n",
    "print(f\"Separate line flow comparison file '{flow_comparison_excel_path}' created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
