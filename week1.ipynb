{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376c149d",
   "metadata": {},
   "source": [
    "# Spatiotemporal Explainable AI for Power System Contingency Classification and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a2e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install pandapower\n",
    "%pip install pandapower -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import required libraries\n",
    "import pandapower as pp\n",
    "import pandapower.networks as pn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from google.colab import files\n",
    "\n",
    "# Step 3: Function to generate random load variation\n",
    "def vary_loads(net, scale_min=0.7, scale_max=1.0):\n",
    "    scaling_factors = np.random.uniform(scale_min, scale_max, size=len(net.load))\n",
    "    net.load['p_mw'] *= scaling_factors\n",
    "    net.load['q_mvar'] *= scaling_factors\n",
    "    return net\n",
    "\n",
    "# Step 4: Prepare to store results across all scenarios\n",
    "all_results = []\n",
    "load_scenarios = []\n",
    "\n",
    "# Step 5: Generate 1000 random load scenarios\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "for scenario_id in range(1000):\n",
    "    net = pn.case30()\n",
    "    net = vary_loads(net, 0.7, 1.0)  # 30% variation range in loads\n",
    "\n",
    "    # Store the load values for each scenario\n",
    "    for i, row in net.load.iterrows():\n",
    "        load_scenarios.append({\n",
    "            'Scenario': scenario_id,\n",
    "            'Load_Bus': row['bus'],\n",
    "            'Load_ID': i,\n",
    "            'P_mw': row['p_mw'],\n",
    "            'Q_mvar': row['q_mvar']\n",
    "        })\n",
    "\n",
    "    # Adjust line ratings (custom stress for line 8)\n",
    "    net.line['max_loading_percent'] = 115.0\n",
    "    net.line.at[8, 'max_loading_percent'] = 100.0\n",
    "\n",
    "    # N-1 Contingency simulation: Take one line out at a time\n",
    "    for i in net.line.index:\n",
    "        net_copy = copy.deepcopy(net)\n",
    "        net_copy.line.at[i, 'in_service'] = False\n",
    "\n",
    "        try:\n",
    "            pp.runpp(net_copy)\n",
    "            status = 'Stable'\n",
    "        except Exception as e:\n",
    "            status = 'Unstable'\n",
    "\n",
    "        result = {\n",
    "            'Scenario': scenario_id,\n",
    "            'Outaged_Line': i,\n",
    "            'Status': status\n",
    "        }\n",
    "\n",
    "        # Store bus voltages if stable\n",
    "        for bus in net_copy.bus.index:\n",
    "            result[f'V_bus_{bus}'] = net_copy.res_bus.vm_pu.at[bus] if status == 'Stable' else None\n",
    "\n",
    "        # Store line loadings if stable\n",
    "        for line in net_copy.line.index:\n",
    "            result[f'Loading_line_{line}'] = net_copy.res_line.loading_percent.at[line] if status == 'Stable' else None\n",
    "\n",
    "        all_results.append(result)\n",
    "\n",
    "# Step 6: Compile all results\n",
    "df_all = pd.DataFrame(all_results)\n",
    "\n",
    "# Step 7: Apply severity threshold (98% line loading triggers severity)\n",
    "loading_cols = [col for col in df_all.columns if col.startswith(\"Loading_line_\")]\n",
    "df_all['Severity'] = df_all[loading_cols].gt(98.0).any(axis=1).astype(int)\n",
    "\n",
    "# Step 8: Save contingency results to CSV\n",
    "df_all.to_csv(\"n1_contingency_balanced.csv\", index=False)\n",
    "files.download(\"n1_contingency_balanced.csv\")\n",
    "\n",
    "# Step 9: Save load scenarios to Excel\n",
    "df_loads = pd.DataFrame(load_scenarios)\n",
    "df_loads.to_excel(\"load_scenarios.xlsx\", index=False)\n",
    "files.download(\"load_scenarios.xlsx\")\n",
    "\n",
    "# Step 10: Print summary\n",
    "print(\"✅ Contingency analysis complete.\")\n",
    "print(\"Final Severity Counts:\\n\", df_all['Severity'].value_counts())\n",
    "print(f\"Total load scenarios saved: {df_loads['Scenario'].nunique()} scenarios, {len(df_loads)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1.2: Missing Value Detection and Cleaning: import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file treating empty strings as missing (NaN)\n",
    "file_path = \"/content/n1_contingency_balanced.csv\"  # Update path if needed\n",
    "df = pd.read_csv(file_path, keep_default_na=False)\n",
    "\n",
    "# Show original dimensions\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Replace only empty strings with NaN\n",
    "df_replaced = df.replace(\"\", np.nan)\n",
    "\n",
    "# Fill missing values: forward fill first, then backward fill\n",
    "df_filled = df_replaced.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Show filled dataset dimensions (should be the same)\n",
    "print(f\"Filled dataset shape: {df_filled.shape}\")\n",
    "\n",
    "# Save the result to a new CSV\n",
    "output_path = \"/content/n1_contingency_balanced_filled_complete.csv\"\n",
    "df_filled.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Empty cells filled and saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c733199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN-LSTM, LSTM, GRU, and GCN Multi-Task Learning Models for Phase 2 Contingency Prediction\n",
    "\n",
    "%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
    "%pip install torch-geometric\n",
    "%pip install xlsxwriter \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "import xlsxwriter\n",
    "\n",
    "# Load input files\n",
    "load_df = pd.read_excel(\"load_scenarios.xlsx\")\n",
    "cont_df = pd.read_csv(\"n1_contingency_balanced_filled_complete.csv\")\n",
    "cont_df = cont_df[cont_df['Scenario'] < 1000].reset_index(drop=True)\n",
    "\n",
    "# Extract and reshape load features\n",
    "load_features = load_df[[\"P_mw\", \"Q_mvar\"]].values\n",
    "assert load_features.shape[0] == 20000, \"Expected 20000 rows of load data\"\n",
    "load_features = load_features.reshape(1000, 40)\n",
    "repeat_factor = 41\n",
    "load_features_expanded = np.repeat(load_features, repeat_factor, axis=0)\n",
    "\n",
    "# Extract voltages and line flows\n",
    "bus_cols = [col for col in cont_df.columns if col.startswith(\"V_bus_\")]\n",
    "line_cols = [col for col in cont_df.columns if col.startswith(\"Loading_line_\")]\n",
    "voltages = cont_df[bus_cols].values.astype(np.float32)\n",
    "line_flows = cont_df[line_cols].values.astype(np.float32)\n",
    "combined_input = np.concatenate([load_features_expanded, voltages, line_flows], axis=1)\n",
    "\n",
    "# Targets\n",
    "features_out = cont_df[bus_cols + line_cols].values.astype(np.float32)\n",
    "labels_class = cont_df['Severity'].values.astype(np.int64)\n",
    "labels_rank = cont_df[line_cols].values.astype(np.float32) / 100\n",
    "\n",
    "# Sanity checks\n",
    "print(\"Input shapes:\")\n",
    "print(\"- Combined input:\", combined_input.shape)\n",
    "print(\"- Target features:\", features_out.shape)\n",
    "print(\"- Severity labels:\", labels_class.shape)\n",
    "print(\"- Ranking shape:\", labels_rank.shape)\n",
    "\n",
    "assert combined_input.shape[0] == features_out.shape[0] == labels_class.shape[0] == labels_rank.shape[0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = combined_input[:990*41], combined_input[990*41:]\n",
    "y_class_train, y_class_test = labels_class[:990*41], labels_class[990*41:]\n",
    "y_rank_train, y_rank_test = labels_rank[:990*41], labels_rank[990*41:]\n",
    "Y_train, Y_test = features_out[:990*41], features_out[990*41:]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, Y_test.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_class_train), torch.tensor(y_rank_train)), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_class_test), torch.tensor(y_rank_test)), batch_size=64)\n",
    "\n",
    "# Model Definitions\n",
    "class FeedForward(nn.Module):\n",
    "    def _init_(self, input_dim, hidden_size):\n",
    "        super()._init_()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class LSTM_MTL(nn.Module):\n",
    "    def _init_(self, input_dim, hidden_size):\n",
    "        super()._init_()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size, batch_first=True)\n",
    "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
    "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h = h_n[-1]\n",
    "        return self.fc_cls(h), self.fc_rank(h)\n",
    "\n",
    "class GRU_MTL(nn.Module):\n",
    "    def _init_(self, input_dim, hidden_size):\n",
    "        super()._init_()\n",
    "        self.gru = nn.GRU(input_dim, hidden_size, batch_first=True)\n",
    "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
    "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, h_n = self.gru(x)\n",
    "        h = h_n[-1]\n",
    "        return self.fc_cls(h), self.fc_rank(h)\n",
    "\n",
    "class BaseMTL(nn.Module):\n",
    "    def _init_(self, base, hidden_size):\n",
    "        super()._init_()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(hidden_size, 2)\n",
    "        self.regressor = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        return self.classifier(x), self.regressor(x)\n",
    "\n",
    "# Training and Evaluation\n",
    "all_results = []\n",
    "rank_matrix = {}\n",
    "class_matrix = {}\n",
    "class_pred_matrix = {}\n",
    "true_rank_matrix = np.argsort(-y_rank_test.reshape(-1, 41), axis=1) + 1\n",
    "\n",
    "input_dim = combined_input.shape[1]\n",
    "hidden_size = 64\n",
    "\n",
    "def train_and_evaluate(model_name, model, train_loader, test_loader):\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_rank = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb_cls, yb_rank in train_loader:\n",
    "            out_cls, out_rank = model(xb)\n",
    "            loss_cls = criterion_class(out_cls, yb_cls)\n",
    "            loss_rank = criterion_rank(out_rank, yb_rank)\n",
    "            loss = loss_cls + 0.5 * loss_rank\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"{model_name} - Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_true, all_pred, pred_scores = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb_cls, yb_rank in test_loader:\n",
    "            out_cls, out_rank = model(xb)\n",
    "            preds = torch.argmax(out_cls, dim=1)\n",
    "            all_true.extend(yb_cls.cpu().numpy())\n",
    "            all_pred.extend(preds.cpu().numpy())\n",
    "            pred_scores.extend(out_rank.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_true, all_pred)\n",
    "    prec = precision_score(all_true, all_pred, zero_division=0)\n",
    "    rec = recall_score(all_true, all_pred, zero_division=0)\n",
    "    f1 = f1_score(all_true, all_pred, zero_division=0)\n",
    "\n",
    "    all_results.append({\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1})\n",
    "    class_matrix[model_name] = np.vstack(pred_scores)\n",
    "    class_pred_matrix[model_name] = np.array(all_pred).reshape(-1, 41)\n",
    "    rank_matrix[model_name] = np.argsort(-np.vstack(pred_scores), axis=1) + 1\n",
    "\n",
    "    print(f\"{model_name} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "train_and_evaluate(\"LSTM\", LSTM_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GRU\", GRU_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_GRU\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_GRU_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "\n",
    "with pd.ExcelWriter(\"phase2_model_results.xlsx\", engine='xlsxwriter') as writer:\n",
    "    pd.DataFrame(all_results).to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    for model in rank_matrix:\n",
    "        df_rank = pd.DataFrame(rank_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(rank_matrix[model].shape[0])])\n",
    "        df_rank.to_excel(writer, sheet_name=f\"{model}_Ranking\")\n",
    "        df_cls = pd.DataFrame(class_pred_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(class_pred_matrix[model].shape[0])])\n",
    "        df_cls.to_excel(writer, sheet_name=f\"{model}_Classify\")\n",
    "    df_true_severity = pd.DataFrame(np.array(y_class_test).reshape(-1, 41).T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(y_class_test)//41)])\n",
    "    df_true_severity.to_excel(writer, sheet_name=\"True_Severity\")\n",
    "    df_true_rank = pd.DataFrame(true_rank_matrix.T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(true_rank_matrix))])\n",
    "    df_true_rank.to_excel(writer, sheet_name=\"True_Ranking\")\n",
    "\n",
    "print(\"Excel file 'phase2_model_results.xlsx' updated with true rankings.\")\n",
    "\n",
    "with pd.ExcelWriter(\"line_flow_comparison.xlsx\", engine='xlsxwriter') as writer:\n",
    "    test_df = cont_df[cont_df['Scenario'] >= 990].reset_index(drop=True)\n",
    "\n",
    "    true_flow_matrix = []\n",
    "    true_columns = []\n",
    "    for scenario_id in range(990, 1000):\n",
    "        scenario_data = test_df[test_df['Scenario'] == scenario_id].reset_index(drop=True)\n",
    "        for outage_id in range(41):\n",
    "            outaged_line = scenario_data.loc[outage_id, 'Outaged_Line']\n",
    "            flow_row = []\n",
    "            for line_id in range(41):\n",
    "                flow = 0.0 if line_id == outaged_line else scenario_data.loc[outage_id, f\"Loading_line_{line_id}\"]\n",
    "                flow_row.append(flow)\n",
    "            true_flow_matrix.append(flow_row)\n",
    "            true_columns.append(f\"Scenario_{scenario_id-990}Outage{outaged_line}\")\n",
    "    df_true_flows = pd.DataFrame(np.array(true_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=true_columns)\n",
    "    df_true_flows.to_excel(writer, sheet_name=\"True_Line_Flows\")\n",
    "\n",
    "    for model_name, preds in class_matrix.items():\n",
    "        pred_flow_matrix = []\n",
    "        pred_columns = []\n",
    "        for idx in range(preds.shape[0]):\n",
    "            scenario_idx = idx // 41\n",
    "            outage_idx = idx % 41\n",
    "            flow_row = []\n",
    "            for line_id in range(41):\n",
    "                flow = 0.0 if line_id == outage_idx else preds[idx][line_id] * 100\n",
    "                flow_row.append(flow)\n",
    "            pred_flow_matrix.append(flow_row)\n",
    "            pred_columns.append(f\"Scenario_{scenario_idx}Outage{outage_idx}\")\n",
    "        df_pred_flows = pd.DataFrame(np.array(pred_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=pred_columns)\n",
    "        df_pred_flows.to_excel(writer, sheet_name=f\"Pred_{model_name}_Flows\")\n",
    "\n",
    "print(\"Separate line flow comparison file 'line_flow_comparison.xlsx' created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
