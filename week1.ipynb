{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376c149d",
   "metadata": {},
   "source": [
    "# Spatiotemporal Explainable AI for Power System Contingency Classification and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f87c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install pandapower\n",
    "%pip install pandapower -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6116728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency results saved to: ./n1_contingency_balanced.csv\n",
      "Load scenarios saved to: ./load_scenarios.xlsx\n",
      "✅ Contingency analysis complete.\n",
      "Final Severity Counts:\n",
      " Severity\n",
      "0    23395\n",
      "1    17605\n",
      "Name: count, dtype: int64\n",
      "Total load scenarios saved: 1000 scenarios, 20000 rows\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Step 2: Import required libraries\n",
    "import pandapower as pp\n",
    "import pandapower.networks as pn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os # Import the os module\n",
    "\n",
    "# Define the directory where you want to save the files\n",
    "# !!! IMPORTANT: CHANGE THIS PATH TO A DIRECTORY ON YOUR LOCAL MACHINE !!!\n",
    "save_directory = \"./\" # Example: create a 'generated_files' folder in the same directory as your notebook\n",
    "# Or use an absolute path: save_directory = \"/path/to/your/desired/directory\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# Step 3: Function to generate random load variation\n",
    "def vary_loads(net, scale_min=0.7, scale_max=1.0):\n",
    "    scaling_factors = np.random.uniform(scale_min, scale_max, size=len(net.load))\n",
    "    net.load['p_mw'] *= scaling_factors\n",
    "    net.load['q_mvar'] *= scaling_factors\n",
    "    return net\n",
    "\n",
    "# Step 4: Prepare to store results across all scenarios\n",
    "all_results = []\n",
    "load_scenarios = []\n",
    "\n",
    "# Step 5: Generate 1000 random load scenarios\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "for scenario_id in range(1000):\n",
    "    net = pn.case30()\n",
    "    net = vary_loads(net, 0.7, 1.0)  # 30% variation range in loads\n",
    "\n",
    "    # Store the load values for each scenario\n",
    "    for i, row in net.load.iterrows():\n",
    "        load_scenarios.append({\n",
    "            'Scenario': scenario_id,\n",
    "            'Load_Bus': row['bus'],\n",
    "            'Load_ID': i,\n",
    "            'P_mw': row['p_mw'],\n",
    "            'Q_mvar': row['q_mvar']\n",
    "        })\n",
    "\n",
    "    # Adjust line ratings (custom stress for line 8)\n",
    "    net.line['max_loading_percent'] = 115.0\n",
    "    net.line.at[8, 'max_loading_percent'] = 100.0\n",
    "\n",
    "    # N-1 Contingency simulation: Take one line out at a time\n",
    "    for i in net.line.index:\n",
    "        net_copy = copy.deepcopy(net)\n",
    "        net_copy.line.at[i, 'in_service'] = False\n",
    "\n",
    "        try:\n",
    "            pp.runpp(net_copy)\n",
    "            status = 'Stable'\n",
    "        except Exception as e:\n",
    "            status = 'Unstable'\n",
    "\n",
    "        result = {\n",
    "            'Scenario': scenario_id,\n",
    "            'Outaged_Line': i,\n",
    "            'Status': status\n",
    "        }\n",
    "\n",
    "        # Store bus voltages if stable\n",
    "        for bus in net_copy.bus.index:\n",
    "            result[f'V_bus_{bus}'] = net_copy.res_bus.vm_pu.at[bus] if status == 'Stable' else None\n",
    "\n",
    "        # Store line loadings if stable\n",
    "        for line in net_copy.line.index:\n",
    "            result[f'Loading_line_{line}'] = net_copy.res_line.loading_percent.at[line] if status == 'Stable' else None\n",
    "\n",
    "        all_results.append(result)\n",
    "\n",
    "# Step 6: Compile all results\n",
    "df_all = pd.DataFrame(all_results)\n",
    "\n",
    "# Step 7: Apply severity threshold (98% line loading triggers severity)\n",
    "loading_cols = [col for col in df_all.columns if col.startswith(\"Loading_line_\")]\n",
    "df_all['Severity'] = df_all[loading_cols].gt(98.0).any(axis=1).astype(int)\n",
    "\n",
    "# Step 8: Save contingency results to CSV\n",
    "# Construct the full path for saving\n",
    "csv_output_path = os.path.join(save_directory, \"n1_contingency_balanced.csv\")\n",
    "df_all.to_csv(csv_output_path, index=False)\n",
    "print(f\"Contingency results saved to: {csv_output_path}\")\n",
    "\n",
    "\n",
    "# Step 9: Save load scenarios to Excel\n",
    "df_loads = pd.DataFrame(load_scenarios)\n",
    "# Construct the full path for saving\n",
    "excel_output_path = os.path.join(save_directory, \"load_scenarios.xlsx\")\n",
    "df_loads.to_excel(excel_output_path, index=False)\n",
    "print(f\"Load scenarios saved to: {excel_output_path}\")\n",
    "\n",
    "\n",
    "# Step 10: Print summary\n",
    "print(\"✅ Contingency analysis complete.\")\n",
    "print(\"Final Severity Counts:\\n\", df_all['Severity'].value_counts())\n",
    "print(f\"Total load scenarios saved: {df_loads['Scenario'].nunique()} scenarios, {len(df_loads)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded66dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (41000, 75)\n",
      "Filled dataset shape: (41000, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eredd\\AppData\\Local\\Temp\\ipykernel_13440\\1283725754.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled = df_replaced.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Empty cells filled and saved to: ./n1_contingency_balanced_filled_complete.csv\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "#Task 1.2: Missing Value Detection and Cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # Import the os module\n",
    "\n",
    "# Define the directory where files are saved and will be loaded from\n",
    "# !!! IMPORTANT: Ensure this matches the save_directory in the previous cell !!!\n",
    "data_directory = \"./\" # Example: if you saved to 'generated_files'\n",
    "# Or use the absolute path: data_directory = \"/path/to/your/desired/directory\"\n",
    "\n",
    "# Load the CSV file treating empty strings as missing (NaN)\n",
    "# Construct the full path for loading\n",
    "file_path = os.path.join(data_directory, \"n1_contingency_balanced.csv\")\n",
    "df = pd.read_csv(file_path, keep_default_na=False)\n",
    "\n",
    "# Show original dimensions\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Replace only empty strings with NaN\n",
    "df_replaced = df.replace(\"\", np.nan)\n",
    "\n",
    "# Fill missing values: forward fill first, then backward fill\n",
    "df_filled = df_replaced.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Show filled dataset dimensions (should be the same)\n",
    "print(f\"Filled dataset shape: {df_filled.shape}\")\n",
    "\n",
    "# Save the result to a new CSV\n",
    "# Construct the full path for saving\n",
    "output_path = os.path.join(data_directory, \"n1_contingency_balanced_filled_complete.csv\")\n",
    "df_filled.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Empty cells filled and saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dcd076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\eredd\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\eredd\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio -q\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72edb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
      "Requirement already satisfied: torch_scatter in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2+pt20cpu)\n",
      "Requirement already satisfied: torch_sparse in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.18+pt20cpu)\n",
      "Requirement already satisfied: torch_cluster in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.3+pt20cpu)\n",
      "Requirement already satisfied: torch_spline_conv in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.2+pt20cpu)\n",
      "Requirement already satisfied: scipy in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_sparse) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scipy->torch_sparse) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch-geometric in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (3.12.13)\n",
      "Requirement already satisfied: fsspec in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (2025.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (2.3.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\eredd\\appdata\\roaming\\python\\python311\\site-packages (from torch-geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\eredd\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\eredd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Install necessary libraries\n",
    "%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
    "%pip install torch-geometric\n",
    "%pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b2a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eredd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "c:\\Users\\eredd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "c:\\Users\\eredd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(\n",
      "c:\\Users\\eredd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "- Combined input: (41000, 111)\n",
      "- Target features: (41000, 71)\n",
      "- Severity labels: (41000,)\n",
      "- Ranking shape: (41000, 41)\n",
      "Train shape: (40590, 111) (40590, 71)\n",
      "Test shape: (410, 111) (410, 71)\n",
      "\n",
      "Training model: LSTM\n",
      "LSTM - Epoch 1 Loss: 0.2586\n",
      "LSTM - Epoch 2 Loss: 0.1315\n",
      "LSTM - Epoch 3 Loss: 0.1055\n",
      "LSTM - Epoch 4 Loss: 0.0932\n",
      "LSTM - Epoch 5 Loss: 0.0938\n",
      "LSTM - Accuracy: 0.9488, Precision: 0.9908, Recall: 0.8438, F1: 0.9114\n",
      "\n",
      "Training model: GRU\n",
      "GRU - Epoch 1 Loss: 0.2749\n",
      "GRU - Epoch 2 Loss: 0.1201\n",
      "GRU - Epoch 3 Loss: 0.0991\n",
      "GRU - Epoch 4 Loss: 0.0899\n",
      "GRU - Epoch 5 Loss: 0.0930\n",
      "GRU - Accuracy: 0.9683, Precision: 0.9323, Recall: 0.9688, F1: 0.9502\n",
      "\n",
      "Training model: GCN\n",
      "GCN - Epoch 1 Loss: 0.2687\n",
      "GCN - Epoch 2 Loss: 0.1268\n",
      "GCN - Epoch 3 Loss: 0.1104\n",
      "GCN - Epoch 4 Loss: 0.1032\n",
      "GCN - Epoch 5 Loss: 0.0997\n",
      "GCN - Accuracy: 0.9707, Precision: 0.9462, Recall: 0.9609, F1: 0.9535\n",
      "\n",
      "Training model: GCN_LSTM\n",
      "GCN_LSTM - Epoch 1 Loss: 0.2562\n",
      "GCN_LSTM - Epoch 2 Loss: 0.1192\n",
      "GCN_LSTM - Epoch 3 Loss: 0.1016\n",
      "GCN_LSTM - Epoch 4 Loss: 0.0988\n",
      "GCN_LSTM - Epoch 5 Loss: 0.0909\n",
      "GCN_LSTM - Accuracy: 0.9805, Precision: 0.9918, Recall: 0.9453, F1: 0.9680\n",
      "\n",
      "Training model: GCN_GRU\n",
      "GCN_GRU - Epoch 1 Loss: 0.2621\n",
      "GCN_GRU - Epoch 2 Loss: 0.1294\n",
      "GCN_GRU - Epoch 3 Loss: 0.1081\n",
      "GCN_GRU - Epoch 4 Loss: 0.1015\n",
      "GCN_GRU - Epoch 5 Loss: 0.0971\n",
      "GCN_GRU - Accuracy: 0.9610, Precision: 0.9179, Recall: 0.9609, F1: 0.9389\n",
      "\n",
      "Training model: GCN_GRU_LSTM\n",
      "GCN_GRU_LSTM - Epoch 1 Loss: 0.3091\n",
      "GCN_GRU_LSTM - Epoch 2 Loss: 0.1470\n",
      "GCN_GRU_LSTM - Epoch 3 Loss: 0.1290\n",
      "GCN_GRU_LSTM - Epoch 4 Loss: 0.1150\n",
      "GCN_GRU_LSTM - Epoch 5 Loss: 0.1078\n",
      "GCN_GRU_LSTM - Accuracy: 0.9780, Precision: 0.9612, Recall: 0.9688, F1: 0.9650\n",
      "Excel file './phase2_model_results.xlsx' updated with true rankings.\n",
      "Separate line flow comparison file './line_flow_comparison.xlsx' created.\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# GCN-LSTM, LSTM, GRU, and GCN Multi-Task Learning Models for Phase 2 Contingency Prediction\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "import xlsxwriter\n",
    "import os # Import the os module\n",
    "\n",
    "# Define the directory where files are saved and will be loaded from\n",
    "# !!! IMPORTANT: Ensure this matches the save_directory in the first cell !!!\n",
    "data_directory = \"./\" # Example: if you saved to 'generated_files'\n",
    "# Or use the absolute path: data_directory = \"/path/to/your/desired/directory\"\n",
    "\n",
    "# Load input files\n",
    "# Construct the full paths for loading\n",
    "load_file_path = os.path.join(data_directory, \"load_scenarios.xlsx\")\n",
    "cont_file_path = os.path.join(data_directory, \"n1_contingency_balanced_filled_complete.csv\")\n",
    "\n",
    "load_df = pd.read_excel(load_file_path)\n",
    "cont_df = pd.read_csv(cont_file_path)\n",
    "cont_df = cont_df[cont_df['Scenario'] < 1000].reset_index(drop=True)\n",
    "\n",
    "# Extract and reshape load features\n",
    "load_features = load_df[[\"P_mw\", \"Q_mvar\"]].values\n",
    "load_features = load_features.reshape(1000, 40)\n",
    "repeat_factor = 41\n",
    "load_features_expanded = np.repeat(load_features, repeat_factor, axis=0)\n",
    "\n",
    "# Extract voltages and line flows\n",
    "bus_cols = [col for col in cont_df.columns if col.startswith(\"V_bus_\")]\n",
    "line_cols = [col for col in cont_df.columns if col.startswith(\"Loading_line_\")]\n",
    "voltages = cont_df[bus_cols].values.astype(np.float32)\n",
    "line_flows = cont_df[line_cols].values.astype(np.float32)\n",
    "combined_input = np.concatenate([load_features_expanded, voltages, line_flows], axis=1)\n",
    "\n",
    "# Targets\n",
    "features_out = cont_df[bus_cols + line_cols].values.astype(np.float32)\n",
    "labels_class = cont_df['Severity'].values.astype(np.int64)\n",
    "labels_rank = cont_df[line_cols].values.astype(np.float32) / 100\n",
    "\n",
    "# Sanity checks\n",
    "print(\"Input shapes:\")\n",
    "print(\"- Combined input:\", combined_input.shape)\n",
    "print(\"- Target features:\", features_out.shape)\n",
    "print(\"- Severity labels:\", labels_class.shape)\n",
    "print(\"- Ranking shape:\", labels_rank.shape)\n",
    "\n",
    "assert combined_input.shape[0] == features_out.shape[0] == labels_class.shape[0] == labels_rank.shape[0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = combined_input[:990*41], combined_input[990*41:]\n",
    "y_class_train, y_class_test = labels_class[:990*41], labels_class[990*41:]\n",
    "y_rank_train, y_rank_test = labels_rank[:990*41], labels_rank[990*41:]\n",
    "Y_train, Y_test = features_out[:990*41], features_out[990*41:]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, Y_test.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_class_train), torch.tensor(y_rank_train)), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_class_test), torch.tensor(y_rank_test)), batch_size=64)\n",
    "\n",
    "# Model Definitions (These are standard PyTorch modules, no changes needed for environment)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class LSTM_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size, batch_first=True)\n",
    "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
    "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h = h_n[-1]\n",
    "        return self.fc_cls(h), self.fc_rank(h)\n",
    "\n",
    "class GRU_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_size, batch_first=True)\n",
    "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
    "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, h_n = self.gru(x)\n",
    "        h = h_n[-1]\n",
    "        return self.fc_cls(h), self.fc_rank(h)\n",
    "\n",
    "class BaseMTL(nn.Module):\n",
    "    def __init__(self, base, hidden_size):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(hidden_size, 2)\n",
    "        self.regressor = nn.Linear(hidden_size, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        return self.classifier(x), self.regressor(x)\n",
    "\n",
    "# Training and Evaluation\n",
    "all_results = []\n",
    "rank_matrix = {}\n",
    "class_matrix = {}\n",
    "class_pred_matrix = {}\n",
    "true_rank_matrix = np.argsort(-y_rank_test.reshape(-1, 41), axis=1) + 1\n",
    "\n",
    "input_dim = combined_input.shape[1]\n",
    "hidden_size = 64\n",
    "\n",
    "def train_and_evaluate(model_name, model, train_loader, test_loader):\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_rank = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb_cls, yb_rank in train_loader:\n",
    "            out_cls, out_rank = model(xb)\n",
    "            loss_cls = criterion_class(out_cls, yb_cls)\n",
    "            loss_rank = criterion_rank(out_rank, yb_rank)\n",
    "            loss = loss_cls + 0.5 * loss_rank\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"{model_name} - Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_true, all_pred, pred_scores = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb_cls, yb_rank in test_loader:\n",
    "            out_cls, out_rank = model(xb)\n",
    "            preds = torch.argmax(out_cls, dim=1)\n",
    "            all_true.extend(yb_cls.cpu().numpy())\n",
    "            all_pred.extend(preds.cpu().numpy())\n",
    "            pred_scores.extend(out_rank.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_true, all_pred)\n",
    "    prec = precision_score(all_true, all_pred, zero_division=0)\n",
    "    rec = recall_score(all_true, all_pred, zero_division=0)\n",
    "    f1 = f1_score(all_true, all_pred, zero_division=0)\n",
    "\n",
    "    all_results.append({\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1})\n",
    "    class_matrix[model_name] = np.vstack(pred_scores)\n",
    "    class_pred_matrix[model_name] = np.array(all_pred).reshape(-1, 41)\n",
    "    rank_matrix[model_name] = np.argsort(-np.vstack(pred_scores), axis=1) + 1\n",
    "\n",
    "    print(f\"{model_name} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "train_and_evaluate(\"LSTM\", LSTM_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GRU\", GRU_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_GRU\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "train_and_evaluate(\"GCN_GRU_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
    "\n",
    "\n",
    "# Saving results to Excel\n",
    "# Construct the full path for saving the main results file\n",
    "main_results_excel_path = os.path.join(data_directory, \"phase2_model_results.xlsx\")\n",
    "with pd.ExcelWriter(main_results_excel_path, engine='xlsxwriter') as writer:\n",
    "    pd.DataFrame(all_results).to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    for model in rank_matrix:\n",
    "        df_rank = pd.DataFrame(rank_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(rank_matrix[model].shape[0])])\n",
    "        df_rank.to_excel(writer, sheet_name=f\"{model}_Ranking\")\n",
    "        df_cls = pd.DataFrame(class_pred_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(class_pred_matrix[model].shape[0])])\n",
    "        df_cls.to_excel(writer, sheet_name=f\"{model}_Classify\")\n",
    "    df_true_severity = pd.DataFrame(np.array(y_class_test).reshape(-1, 41).T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(y_class_test)//41)])\n",
    "    df_true_severity.to_excel(writer, sheet_name=\"True_Severity\")\n",
    "    df_true_rank = pd.DataFrame(true_rank_matrix.T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(true_rank_matrix))])\n",
    "    df_true_rank.to_excel(writer, sheet_name=\"True_Ranking\")\n",
    "\n",
    "print(f\"Excel file '{main_results_excel_path}' updated with true rankings.\")\n",
    "\n",
    "\n",
    "# Saving line flow comparison file\n",
    "flow_comparison_excel_path = os.path.join(data_directory, \"line_flow_comparison.xlsx\")\n",
    "with pd.ExcelWriter(flow_comparison_excel_path, engine='xlsxwriter') as writer:\n",
    "    test_df = cont_df[cont_df['Scenario'] >= 990].reset_index(drop=True)\n",
    "\n",
    "    true_flow_matrix = []\n",
    "    true_columns = []\n",
    "    for scenario_id in range(990, 1000):\n",
    "        scenario_data = test_df[test_df['Scenario'] == scenario_id].reset_index(drop=True)\n",
    "        for outage_id in range(41):\n",
    "            outaged_line = scenario_data.loc[outage_id, 'Outaged_Line']\n",
    "            flow_row = []\n",
    "            for line_id in range(41):\n",
    "                flow = 0.0 if line_id == outage_id else scenario_data.loc[outage_id, f\"Loading_line_{line_id}\"] # Corrected line_id to outage_id for the flow == 0.0 condition\n",
    "                flow_row.append(flow)\n",
    "            true_flow_matrix.append(flow_row)\n",
    "            true_columns.append(f\"Scenario_{(scenario_id-990)*41 + outage_id}\")\n",
    "\n",
    "    df_true_flows = pd.DataFrame(np.array(true_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=true_columns)\n",
    "    df_true_flows.to_excel(writer, sheet_name=\"True_Line_Flows\")\n",
    "\n",
    "    for model_name, preds in class_matrix.items():\n",
    "        pred_flow_matrix = []\n",
    "        pred_columns = []\n",
    "        for idx in range(preds.shape[0]):\n",
    "            scenario_idx = idx // 41\n",
    "            outage_idx = idx % 41\n",
    "            flow_row = []\n",
    "            for line_id in range(41):\n",
    "                flow = 0.0 if line_id == outage_idx else preds[idx][line_id] * 100\n",
    "                flow_row.append(flow)\n",
    "            pred_flow_matrix.append(flow_row)\n",
    "            pred_columns.append(f\"Scenario_{idx}\")\n",
    "\n",
    "        df_pred_flows = pd.DataFrame(np.array(pred_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=pred_columns)\n",
    "        df_pred_flows.to_excel(writer, sheet_name=f\"Pred_{model_name}_Flows\")\n",
    "\n",
    "print(f\"Separate line flow comparison file '{flow_comparison_excel_path}' created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
