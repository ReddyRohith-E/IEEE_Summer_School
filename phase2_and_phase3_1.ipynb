{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-geometric\n",
        "!pip install xlsxwriter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGe4avZvk8a4",
        "outputId": "f6a7193d-9445-4190-d2ae-cbf1110d1d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes:\n",
            "- Combined input: (41000, 111)\n",
            "- Target features: (41000, 71)\n",
            "- Severity labels: (41000,)\n",
            "- Ranking shape: (41000, 41)\n",
            "Train shape: (40590, 111) (40590, 71)\n",
            "Test shape: (410, 111) (410, 71)\n",
            "\n",
            "Training model: LSTM\n",
            "LSTM - Epoch 1 Loss: 0.2856\n",
            "LSTM - Epoch 2 Loss: 0.1253\n",
            "LSTM - Epoch 3 Loss: 0.0974\n",
            "LSTM - Epoch 4 Loss: 0.0951\n",
            "LSTM - Epoch 5 Loss: 0.0860\n",
            "LSTM - Accuracy: 0.9366, Precision: 0.9636, Recall: 0.8281, F1: 0.8908\n",
            "\n",
            "Training model: GRU\n",
            "GRU - Epoch 1 Loss: 0.2669\n",
            "GRU - Epoch 2 Loss: 0.1222\n",
            "GRU - Epoch 3 Loss: 0.1027\n",
            "GRU - Epoch 4 Loss: 0.0932\n",
            "GRU - Epoch 5 Loss: 0.0914\n",
            "GRU - Accuracy: 0.9463, Precision: 0.9649, Recall: 0.8594, F1: 0.9091\n",
            "\n",
            "Training model: GCN\n",
            "GCN - Epoch 1 Loss: 0.2453\n",
            "GCN - Epoch 2 Loss: 0.1212\n",
            "GCN - Epoch 3 Loss: 0.1099\n",
            "GCN - Epoch 4 Loss: 0.1010\n",
            "GCN - Epoch 5 Loss: 0.0926\n",
            "GCN - Accuracy: 0.9634, Precision: 0.9748, Recall: 0.9062, F1: 0.9393\n",
            "\n",
            "Training model: GCN_LSTM\n",
            "GCN_LSTM - Epoch 1 Loss: 0.2591\n",
            "GCN_LSTM - Epoch 2 Loss: 0.1309\n",
            "GCN_LSTM - Epoch 3 Loss: 0.1081\n",
            "GCN_LSTM - Epoch 4 Loss: 0.1055\n",
            "GCN_LSTM - Epoch 5 Loss: 0.0883\n",
            "GCN_LSTM - Accuracy: 0.9878, Precision: 0.9920, Recall: 0.9688, F1: 0.9802\n",
            "\n",
            "Training model: GCN_GRU\n",
            "GCN_GRU - Epoch 1 Loss: 0.3001\n",
            "GCN_GRU - Epoch 2 Loss: 0.1369\n",
            "GCN_GRU - Epoch 3 Loss: 0.1102\n",
            "GCN_GRU - Epoch 4 Loss: 0.0981\n",
            "GCN_GRU - Epoch 5 Loss: 0.0968\n",
            "GCN_GRU - Accuracy: 0.9244, Precision: 0.8129, Recall: 0.9844, F1: 0.8905\n",
            "\n",
            "Training model: GCN_GRU_LSTM\n",
            "GCN_GRU_LSTM - Epoch 1 Loss: 0.3067\n",
            "GCN_GRU_LSTM - Epoch 2 Loss: 0.1321\n",
            "GCN_GRU_LSTM - Epoch 3 Loss: 0.1213\n",
            "GCN_GRU_LSTM - Epoch 4 Loss: 0.1014\n",
            "GCN_GRU_LSTM - Epoch 5 Loss: 0.1080\n",
            "GCN_GRU_LSTM - Accuracy: 0.8951, Precision: 0.9885, Recall: 0.6719, F1: 0.8000\n",
            "Excel file 'phase2_model_results.xlsx' updated with true rankings.\n",
            "Separate line flow comparison file 'line_flow_comparison.xlsx' created.\n"
          ]
        }
      ],
      "source": [
        "# GCN-LSTM, LSTM, GRU, and GCN Multi-Task Learning Models for Phase 2 Contingency Prediction\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch_geometric.nn import GCNConv\n",
        "import xlsxwriter\n",
        "\n",
        "# Load input files\n",
        "load_df = pd.read_excel(\"load_scenarios.xlsx\")\n",
        "cont_df = pd.read_csv(\"n1_contingency_balanced_filled_complete.csv\")\n",
        "cont_df = cont_df[cont_df['Scenario'] < 1000].reset_index(drop=True)\n",
        "\n",
        "# Extract and reshape load features\n",
        "load_features = load_df[[\"P_mw\", \"Q_mvar\"]].values\n",
        "assert load_features.shape[0] == 20000, \"Expected 20000 rows of load data\"\n",
        "load_features = load_features.reshape(1000, 40)\n",
        "repeat_factor = 41\n",
        "load_features_expanded = np.repeat(load_features, repeat_factor, axis=0)\n",
        "\n",
        "# Extract voltages and line flows\n",
        "bus_cols = [col for col in cont_df.columns if col.startswith(\"V_bus_\")]\n",
        "line_cols = [col for col in cont_df.columns if col.startswith(\"Loading_line_\")]\n",
        "voltages = cont_df[bus_cols].values.astype(np.float32)\n",
        "line_flows = cont_df[line_cols].values.astype(np.float32)\n",
        "combined_input = np.concatenate([load_features_expanded, voltages, line_flows], axis=1)\n",
        "\n",
        "# Targets\n",
        "features_out = cont_df[bus_cols + line_cols].values.astype(np.float32)\n",
        "labels_class = cont_df['Severity'].values.astype(np.int64)\n",
        "labels_rank = cont_df[line_cols].values.astype(np.float32) / 100\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Input shapes:\")\n",
        "print(\"- Combined input:\", combined_input.shape)\n",
        "print(\"- Target features:\", features_out.shape)\n",
        "print(\"- Severity labels:\", labels_class.shape)\n",
        "print(\"- Ranking shape:\", labels_rank.shape)\n",
        "\n",
        "assert combined_input.shape[0] == features_out.shape[0] == labels_class.shape[0] == labels_rank.shape[0]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test = combined_input[:990*41], combined_input[990*41:]\n",
        "y_class_train, y_class_test = labels_class[:990*41], labels_class[990*41:]\n",
        "y_rank_train, y_rank_test = labels_rank[:990*41], labels_rank[990*41:]\n",
        "Y_train, Y_test = features_out[:990*41], features_out[990*41:]\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, Y_train.shape)\n",
        "print(\"Test shape:\", X_test.shape, Y_test.shape)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_class_train), torch.tensor(y_rank_train)), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_class_test), torch.tensor(y_rank_test)), batch_size=64)\n",
        "\n",
        "# Model Definitions\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class LSTM_MTL(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_size, batch_first=True)\n",
        "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
        "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h = h_n[-1]\n",
        "        return self.fc_cls(h), self.fc_rank(h)\n",
        "\n",
        "class GRU_MTL(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_size, batch_first=True)\n",
        "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
        "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        _, h_n = self.gru(x)\n",
        "        h = h_n[-1]\n",
        "        return self.fc_cls(h), self.fc_rank(h)\n",
        "\n",
        "class BaseMTL(nn.Module):\n",
        "    def __init__(self, base, hidden_size):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.classifier = nn.Linear(hidden_size, 2)\n",
        "        self.regressor = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        return self.classifier(x), self.regressor(x)\n",
        "\n",
        "# Training and Evaluation\n",
        "all_results = []\n",
        "rank_matrix = {}\n",
        "class_matrix = {}\n",
        "class_pred_matrix = {}\n",
        "true_rank_matrix = np.argsort(-y_rank_test.reshape(-1, 41), axis=1) + 1\n",
        "\n",
        "input_dim = combined_input.shape[1]\n",
        "hidden_size = 64\n",
        "\n",
        "def train_and_evaluate(model_name, model, train_loader, test_loader):\n",
        "    print(f\"\\nTraining model: {model_name}\")\n",
        "    criterion_class = nn.CrossEntropyLoss()\n",
        "    criterion_rank = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb_cls, yb_rank in train_loader:\n",
        "            out_cls, out_rank = model(xb)\n",
        "            loss_cls = criterion_class(out_cls, yb_cls)\n",
        "            loss_rank = criterion_rank(out_rank, yb_rank)\n",
        "            loss = loss_cls + 0.5 * loss_rank\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"{model_name} - Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_true, all_pred, pred_scores = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb_cls, yb_rank in test_loader:\n",
        "            out_cls, out_rank = model(xb)\n",
        "            preds = torch.argmax(out_cls, dim=1)\n",
        "            all_true.extend(yb_cls.cpu().numpy())\n",
        "            all_pred.extend(preds.cpu().numpy())\n",
        "            pred_scores.extend(out_rank.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_true, all_pred)\n",
        "    prec = precision_score(all_true, all_pred, zero_division=0)\n",
        "    rec = recall_score(all_true, all_pred, zero_division=0)\n",
        "    f1 = f1_score(all_true, all_pred, zero_division=0)\n",
        "\n",
        "    all_results.append({\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1})\n",
        "    class_matrix[model_name] = np.vstack(pred_scores)\n",
        "    class_pred_matrix[model_name] = np.array(all_pred).reshape(-1, 41)\n",
        "    rank_matrix[model_name] = np.argsort(-np.vstack(pred_scores), axis=1) + 1\n",
        "\n",
        "    print(f\"{model_name} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "train_and_evaluate(\"LSTM\", LSTM_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GRU\", GRU_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_GRU\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_GRU_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "\n",
        "with pd.ExcelWriter(\"phase2_model_results_100epochs.xlsx\", engine='xlsxwriter') as writer:\n",
        "    pd.DataFrame(all_results).to_excel(writer, sheet_name=\"Summary\", index=False)\n",
        "    for model in rank_matrix:\n",
        "        headers = [f\"Scenario_{i//41}_Cont_{i%41}\" for i in range(rank_matrix[model].shape[0])]\n",
        "        df_rank = pd.DataFrame(rank_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=headers)\n",
        "        df_rank.to_excel(writer, sheet_name=f\"{model}_Ranking\")\n",
        "        df_cls = pd.DataFrame(class_pred_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{i}\" for i in range(10)])\n",
        "        df_cls.to_excel(writer, sheet_name=f\"{model}_Classify\")\n",
        "    df_true_severity = pd.DataFrame(np.array(y_class_test).reshape(-1, 41).T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(y_class_test)//41)])\n",
        "    df_true_severity.to_excel(writer, sheet_name=\"True_Severity\")\n",
        "    df_true_rank = pd.DataFrame(true_rank_matrix.T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(true_rank_matrix))])\n",
        "    df_true_rank.to_excel(writer, sheet_name=\"True_Ranking\")\n",
        "\n",
        "print(\"Excel file 'phase2_model_results_100epochs.xlsx' updated with true rankings.\")\n",
        "\n",
        "with pd.ExcelWriter(\"line_flow_comparison_100epochs.xlsx\", engine='xlsxwriter') as writer:\n",
        "    test_df = cont_df[cont_df['Scenario'] >= 990].reset_index(drop=True)\n",
        "\n",
        "    true_flow_matrix = []\n",
        "    true_columns = []\n",
        "    for scenario_id in range(990, 1000):\n",
        "        scenario_data = test_df[test_df['Scenario'] == scenario_id].reset_index(drop=True)\n",
        "        for outage_id in range(41):\n",
        "            outaged_line = scenario_data.loc[outage_id, 'Outaged_Line']\n",
        "            flow_row = []\n",
        "            for line_id in range(41):\n",
        "                flow = 0.0 if line_id == outaged_line else scenario_data.loc[outage_id, f\"Loading_line_{line_id}\"]\n",
        "                flow_row.append(flow)\n",
        "            true_flow_matrix.append(flow_row)\n",
        "            true_columns.append(f\"Scenario_{scenario_id - 990}_Outage_{outaged_line}\")\n",
        "    df_true_flows = pd.DataFrame(np.array(true_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=true_columns)\n",
        "    df_true_flows.to_excel(writer, sheet_name=\"True_Line_Flows\")\n",
        "\n",
        "    for model_name, preds in class_matrix.items():\n",
        "        pred_flow_matrix = []\n",
        "        pred_columns = []\n",
        "        for idx in range(preds.shape[0]):\n",
        "            scenario_idx = idx // 41\n",
        "            outage_idx = idx % 41\n",
        "            flow_row = []\n",
        "            for line_id in range(41):\n",
        "                flow = 0.0 if line_id == outage_idx else preds[idx][line_id] * 100\n",
        "                flow_row.append(flow)\n",
        "            pred_flow_matrix.append(flow_row)\n",
        "            pred_columns.append(f\"Scenario_{scenario_idx}_Outage_{outage_idx}\")\n",
        "        df_pred_flows = pd.DataFrame(np.array(pred_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=pred_columns)\n",
        "        df_pred_flows.to_excel(writer, sheet_name=f\"Pred_{model_name}_Flows\")\n",
        "\n",
        "print(\"Separate line flow comparison file 'line_flow_comparison_100epochs.xlsx' created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install shap torch numpy pandas matplotlib xlsxwriter scikit-learn tqdm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ6rSEfZ5pt_",
        "outputId": "4fbee718-4341-4ebe-e136-34285f2233b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== HIGH-RESOLUTION SHAP ANALYSIS ======\n",
            "\n",
            "üîç Loading data with contingency features...\n",
            "Input tensor shape: torch.Size([205, 152])\n",
            "Background shape: (5, 152)\n",
            "\n",
            "üìä Loading Phase 2 model results...\n",
            "\n",
            "üß† Running SHAP analysis...\n",
            "\n",
            "üìä Exporting SHAP summary tables...\n",
            "\n",
            "üé® Generating 1000 DPI plots...\n",
            "\n",
            "‚úÖ Analysis complete! High-resolution plots saved to shap_results\n",
            "‚úÖ SHAP summary tables saved to shap_summary.xlsx\n"
          ]
        }
      ],
      "source": [
        "# COMPLETE SHAP ANALYSIS FOR PHASE 2 MODELS (SCENARIOS 5 TO 9)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import shap\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from warnings import filterwarnings\n",
        "\n",
        "# ====================== CONFIGURATION ======================\n",
        "matplotlib.use('Agg')\n",
        "filterwarnings('ignore')\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams.update({\n",
        "    'figure.dpi': 1000,\n",
        "    'savefig.dpi': 1000,\n",
        "    'font.size': 8,\n",
        "    'axes.titlesize': 10,\n",
        "    'axes.labelsize': 9\n",
        "})\n",
        "\n",
        "# ====================== MODEL WRAPPER ======================\n",
        "class PrecomputedModel:\n",
        "    def __init__(self, classification_output, ranking_output):\n",
        "        self.classification_output = classification_output.astype(float).values.T\n",
        "        self.ranking_output = ranking_output.astype(float).values.T\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        if self.classification_output.shape[1] == 0:\n",
        "            class_output = np.zeros((batch_size, 2))\n",
        "        else:\n",
        "            repeats_cls = int(np.ceil(batch_size / self.classification_output.shape[0]))\n",
        "            class_output = np.tile(self.classification_output, (repeats_cls, 1))[:batch_size]\n",
        "\n",
        "        repeats_rank = int(np.ceil(batch_size / self.ranking_output.shape[0]))\n",
        "        rank_output = np.tile(self.ranking_output, (repeats_rank, 1))[:batch_size]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(class_output, dtype=torch.float32),\n",
        "            torch.tensor(rank_output, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "# ====================== LOAD DATA ======================\n",
        "def load_data_with_metadata():\n",
        "    load_df = pd.read_excel(\"./load_scenarios.xlsx\")\n",
        "    cont_df = pd.read_csv(\"./n1_contingency_balanced_filled_complete.csv\")\n",
        "    cont_df = cont_df[cont_df['Scenario'].between(995, 999)].reset_index(drop=True)\n",
        "    load_df = load_df[load_df['Scenario'].between(995, 999)]\n",
        "\n",
        "    load_df = load_df.pivot(index='Scenario', columns='Load_ID', values=['P_mw', 'Q_mvar'])\n",
        "    load_df.columns = [f\"{a}_{b}\" for a, b in load_df.columns]\n",
        "    load_df = load_df.sort_index(axis=1)\n",
        "    load_mat = load_df.values.astype(np.float32)\n",
        "    load_expanded = np.repeat(load_mat, 41, axis=0)\n",
        "\n",
        "    voltage_cols = [f\"V_bus_{i}\" for i in range(30)]\n",
        "    lineflow_cols = [f\"Loading_line_{i}\" for i in range(41)]\n",
        "    voltage_mat = cont_df[voltage_cols].values.astype(np.float32)\n",
        "    lineflow_mat = cont_df[lineflow_cols].values.astype(np.float32)\n",
        "\n",
        "    contingency_mat = np.zeros((cont_df.shape[0], 41))\n",
        "    contingency_mat[np.arange(cont_df.shape[0]), cont_df['Outaged_Line'].values] = 1\n",
        "\n",
        "    input_features = np.concatenate([\n",
        "        load_expanded, voltage_mat, lineflow_mat, contingency_mat\n",
        "    ], axis=1)\n",
        "\n",
        "    feature_names = []\n",
        "    for i in range(20):\n",
        "        feature_names += [f\"P_mw_load_{i}\", f\"Q_mvar_load_{i}\"]\n",
        "    feature_names += [f\"V_bus_{i}\" for i in range(30)]\n",
        "    feature_names += [f\"Flow_line_{i}\" for i in range(41)]\n",
        "    feature_names += [f\"Contingency_line_{i}\" for i in range(41)]\n",
        "\n",
        "    feature_metadata = {\n",
        "        'type_indices': {\n",
        "            'voltage': (40, 70),\n",
        "            'line_flow': (70, 111),\n",
        "            'contingency': (111, 152)\n",
        "        },\n",
        "        'group_cols': {\n",
        "            'voltage': feature_names[40:70],\n",
        "            'line_flow': feature_names[70:111],\n",
        "            'contingency': feature_names[111:152]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'tensor': torch.tensor(input_features).float(),\n",
        "        'feature_names': feature_names,\n",
        "        'scenario_ids': list(range(995, 1000)),\n",
        "        'feature_metadata': feature_metadata\n",
        "    }\n",
        "\n",
        "# ====================== RUN SHAP ======================\n",
        "def run_shap_analysis(models, background, metadata):\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        results[name] = {'cls': {}, 'rank': {}}\n",
        "        for task, output_index in zip(['cls', 'rank'], [0, 1]):\n",
        "            if task == 'cls' and model.classification_output.shape[1] == 0:\n",
        "                continue\n",
        "            explainer = shap.Explainer(lambda x: model(x)[output_index], background)\n",
        "            shap_values = explainer(background)\n",
        "            for group in metadata['type_indices']:\n",
        "                start, end = metadata['type_indices'][group]\n",
        "                values = shap_values.values[:, start:end]\n",
        "                if values.ndim > 2:\n",
        "                    values = np.mean(values, axis=1)\n",
        "                results[name][task][group] = {\n",
        "                    'values': values,\n",
        "                    'feature_names': metadata['group_cols'][group]\n",
        "                }\n",
        "    return results\n",
        "\n",
        "# ====================== SAVE PLOTS ======================\n",
        "def save_high_res_plots(results, output_dir='shap_results'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for model_name in results:\n",
        "        for task in results[model_name]:\n",
        "            for group in results[model_name][task]:\n",
        "                shap_vals = results[model_name][task][group]['values']\n",
        "                features = results[model_name][task][group]['feature_names']\n",
        "                if shap_vals.shape[1] != len(features):\n",
        "                    shap_vals = shap_vals[:, :len(features)]\n",
        "                shap.summary_plot(\n",
        "                    shap_vals, features=features,\n",
        "                    plot_type=\"bar\", show=False\n",
        "                )\n",
        "                plt.title(f\"{model_name} - {task.upper()} - {group.title()}\")\n",
        "                filename = f\"{model_name}_{task}_{group}.png\"\n",
        "                plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "# ====================== EXCEL EXPORT ======================\n",
        "def export_shap_summary_to_excel(results, output_file=\"shap_summary_100epochs.xlsx\"):\n",
        "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
        "    for model_name in results:\n",
        "        for task in ['cls', 'rank']:\n",
        "            for group in results[model_name][task]:\n",
        "                values = results[model_name][task][group]['values']\n",
        "                features = results[model_name][task][group]['feature_names']\n",
        "                summary_values = np.mean(np.abs(values), axis=0)\n",
        "                min_len = min(len(summary_values), len(features))\n",
        "                df = pd.DataFrame({\n",
        "                    \"Feature\": features[:min_len],\n",
        "                    \"SHAP Value\": summary_values[:min_len]\n",
        "                })\n",
        "                df = df.sort_values(by=\"SHAP Value\", ascending=False)\n",
        "                sheet_name = f\"{model_name}_{task}_{group}\"[:31]\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "    writer.close()\n",
        "\n",
        "# ====================== MAIN ======================\n",
        "def main():\n",
        "    print(\"====== HIGH-RESOLUTION SHAP ANALYSIS ======\")\n",
        "    print(\"\\nüîç Loading data with contingency features...\")\n",
        "    data = load_data_with_metadata()\n",
        "    background = data['tensor'][:5].numpy()\n",
        "    print(f\"Input tensor shape: {data['tensor'].shape}\")\n",
        "    print(f\"Background shape: {background.shape}\")\n",
        "\n",
        "    print(\"\\nüìä Loading Phase 2 model results...\")\n",
        "    result_file = \"./phase2_model_results_100epochs.xlsx\"\n",
        "    xls = pd.ExcelFile(result_file)\n",
        "\n",
        "    models = {}\n",
        "    model_names = [\n",
        "        (\"GCN_LSTM\", \"GCN_LSTM_Classify\", \"GCN_LSTM_Ranking\"),\n",
        "        (\"GCN_GRU_LSTM\", \"GCN_GRU_LSTM_Classify\", \"GCN_GRU_LSTM_Ranking\"),\n",
        "        (\"LSTM\", \"LSTM_Classify\", \"LSTM_Ranking\")\n",
        "    ]\n",
        "\n",
        "    cls_cols = [f\"Scenario_{i}\" for i in range(5, 10)]\n",
        "    rank_cols = [f\"Scenario_{i}_Cont_{j}\" for i in range(5, 10) for j in range(41)]\n",
        "\n",
        "    for tag, cls_sheet, rank_sheet in model_names:\n",
        "        classification_output = xls.parse(cls_sheet)\n",
        "        classification_output = classification_output.loc[:, classification_output.columns.isin(cls_cols)]\n",
        "        ranking_output = xls.parse(rank_sheet)\n",
        "        ranking_output = ranking_output.loc[:, ranking_output.columns.isin(rank_cols)]\n",
        "        models[tag] = PrecomputedModel(classification_output, ranking_output)\n",
        "\n",
        "    print(\"\\nüß† Running SHAP analysis...\")\n",
        "    shap_results = run_shap_analysis(models, background, data['feature_metadata'])\n",
        "\n",
        "    print(\"\\nüìä Exporting SHAP summary tables...\")\n",
        "    export_shap_summary_to_excel(shap_results)\n",
        "\n",
        "    print(\"\\nüé® Generating 1000 DPI plots...\")\n",
        "    save_high_res_plots(shap_results)\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis complete! High-resolution plots saved to shap_results\")\n",
        "    print(\"‚úÖ SHAP summary tables saved to shap_summary_100epochs.xlsx\")\n",
        "\n",
        "    for model_name in shap_results:\n",
        "        summary_df = []\n",
        "        for group in ['voltage', 'line_flow', 'contingency']:\n",
        "            for task in ['cls', 'rank']:\n",
        "                values = shap_results[model_name][task][group]['values']\n",
        "                avg = np.mean(np.abs(values))\n",
        "                summary_df.append({\"Feature Group\": group.title(), \"Task\": 'Classification' if task == 'cls' else 'Ranking', \"Average SHAP Value\": avg})\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_df)\n",
        "\n",
        "        cls_df = summary_df[summary_df['Task'] == 'Classification']\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(cls_df['Feature Group'], cls_df['Average SHAP Value'], color='skyblue')\n",
        "        plt.ylabel(\"Average SHAP Value\")\n",
        "        plt.title(f\"{model_name} - Classification Feature Importance\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"shap_results/{model_name}_Classification_Bar.png\", dpi=1000)\n",
        "        plt.show()\n",
        "\n",
        "        rank_df = summary_df[summary_df['Task'] == 'Ranking']\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(rank_df['Feature Group'], rank_df['Average SHAP Value'], color='salmon')\n",
        "        plt.ylabel(\"Average SHAP Value\")\n",
        "        plt.title(f\"{model_name} - Ranking Feature Importance\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"shap_results/{model_name}_Ranking_Bar.png\", dpi=1000)\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
