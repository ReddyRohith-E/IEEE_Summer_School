{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGe4avZvk8a4",
        "outputId": "f6a7193d-9445-4190-d2ae-cbf1110d1d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (494 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m494.0/494.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.3%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (750 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m750.9/750.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (208 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, torch_sparse, torch_cluster\n",
            "Successfully installed torch_cluster-1.6.3+pt20cpu torch_scatter-2.1.2+pt20cpu torch_sparse-0.6.18+pt20cpu torch_spline_conv-1.2.2+pt20cpu\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes:\n",
            "- Combined input: (41000, 111)\n",
            "- Target features: (41000, 71)\n",
            "- Severity labels: (41000,)\n",
            "- Ranking shape: (41000, 41)\n",
            "Train shape: (40590, 111) (40590, 71)\n",
            "Test shape: (410, 111) (410, 71)\n",
            "\n",
            "Training model: LSTM\n",
            "LSTM - Epoch 1 Loss: 0.2639\n",
            "LSTM - Epoch 2 Loss: 0.1284\n",
            "LSTM - Epoch 3 Loss: 0.1051\n",
            "LSTM - Epoch 4 Loss: 0.0894\n",
            "LSTM - Epoch 5 Loss: 0.0891\n",
            "LSTM - Accuracy: 0.9659, Precision: 0.9831, Recall: 0.9062, F1: 0.9431\n",
            "\n",
            "Training model: GRU\n",
            "GRU - Epoch 1 Loss: 0.2547\n",
            "GRU - Epoch 2 Loss: 0.1176\n",
            "GRU - Epoch 3 Loss: 0.1021\n",
            "GRU - Epoch 4 Loss: 0.0900\n",
            "GRU - Epoch 5 Loss: 0.0851\n",
            "GRU - Accuracy: 0.9683, Precision: 0.9915, Recall: 0.9062, F1: 0.9469\n",
            "\n",
            "Training model: GCN\n",
            "GCN - Epoch 1 Loss: 0.2944\n",
            "GCN - Epoch 2 Loss: 0.1295\n",
            "GCN - Epoch 3 Loss: 0.1182\n",
            "GCN - Epoch 4 Loss: 0.1135\n",
            "GCN - Epoch 5 Loss: 0.1074\n",
            "GCN - Accuracy: 0.9707, Precision: 0.9462, Recall: 0.9609, F1: 0.9535\n",
            "\n",
            "Training model: GCN_LSTM\n",
            "GCN_LSTM - Epoch 1 Loss: 0.3288\n",
            "GCN_LSTM - Epoch 2 Loss: 0.1364\n",
            "GCN_LSTM - Epoch 3 Loss: 0.1123\n",
            "GCN_LSTM - Epoch 4 Loss: 0.1010\n",
            "GCN_LSTM - Epoch 5 Loss: 0.0906\n",
            "GCN_LSTM - Accuracy: 0.9829, Precision: 0.9690, Recall: 0.9766, F1: 0.9728\n",
            "\n",
            "Training model: GCN_GRU\n",
            "GCN_GRU - Epoch 1 Loss: 0.2610\n",
            "GCN_GRU - Epoch 2 Loss: 0.1305\n",
            "GCN_GRU - Epoch 3 Loss: 0.1142\n",
            "GCN_GRU - Epoch 4 Loss: 0.1034\n",
            "GCN_GRU - Epoch 5 Loss: 0.0940\n",
            "GCN_GRU - Accuracy: 0.9220, Precision: 0.9706, Recall: 0.7734, F1: 0.8609\n",
            "\n",
            "Training model: GCN_GRU_LSTM\n",
            "GCN_GRU_LSTM - Epoch 1 Loss: 0.3093\n",
            "GCN_GRU_LSTM - Epoch 2 Loss: 0.1205\n",
            "GCN_GRU_LSTM - Epoch 3 Loss: 0.1016\n",
            "GCN_GRU_LSTM - Epoch 4 Loss: 0.0987\n",
            "GCN_GRU_LSTM - Epoch 5 Loss: 0.0902\n",
            "GCN_GRU_LSTM - Accuracy: 0.9707, Precision: 0.9754, Recall: 0.9297, F1: 0.9520\n",
            "Excel file 'phase2_model_results.xlsx' updated with true rankings.\n",
            "Separate line flow comparison file 'line_flow_comparison.xlsx' created.\n"
          ]
        }
      ],
      "source": [
        "# GCN-LSTM, LSTM, GRU, and GCN Multi-Task Learning Models for Phase 2 Contingency Prediction\n",
        "\n",
        "!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-geometric\n",
        "!pip install xlsxwriter\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch_geometric.nn import GCNConv\n",
        "import xlsxwriter\n",
        "\n",
        "# Load input files\n",
        "load_df = pd.read_excel(\"load_scenarios.xlsx\")\n",
        "cont_df = pd.read_csv(\"n1_contingency_balanced_filled_complete.csv\")\n",
        "cont_df = cont_df[cont_df['Scenario'] < 1000].reset_index(drop=True)\n",
        "\n",
        "# Extract and reshape load features\n",
        "load_features = load_df[[\"P_mw\", \"Q_mvar\"]].values\n",
        "assert load_features.shape[0] == 20000, \"Expected 20000 rows of load data\"\n",
        "load_features = load_features.reshape(1000, 40)\n",
        "repeat_factor = 41\n",
        "load_features_expanded = np.repeat(load_features, repeat_factor, axis=0)\n",
        "\n",
        "# Extract voltages and line flows\n",
        "bus_cols = [col for col in cont_df.columns if col.startswith(\"V_bus_\")]\n",
        "line_cols = [col for col in cont_df.columns if col.startswith(\"Loading_line_\")]\n",
        "voltages = cont_df[bus_cols].values.astype(np.float32)\n",
        "line_flows = cont_df[line_cols].values.astype(np.float32)\n",
        "combined_input = np.concatenate([load_features_expanded, voltages, line_flows], axis=1)\n",
        "\n",
        "# Targets\n",
        "features_out = cont_df[bus_cols + line_cols].values.astype(np.float32)\n",
        "labels_class = cont_df['Severity'].values.astype(np.int64)\n",
        "labels_rank = cont_df[line_cols].values.astype(np.float32) / 100\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Input shapes:\")\n",
        "print(\"- Combined input:\", combined_input.shape)\n",
        "print(\"- Target features:\", features_out.shape)\n",
        "print(\"- Severity labels:\", labels_class.shape)\n",
        "print(\"- Ranking shape:\", labels_rank.shape)\n",
        "\n",
        "assert combined_input.shape[0] == features_out.shape[0] == labels_class.shape[0] == labels_rank.shape[0]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test = combined_input[:990*41], combined_input[990*41:]\n",
        "y_class_train, y_class_test = labels_class[:990*41], labels_class[990*41:]\n",
        "y_rank_train, y_rank_test = labels_rank[:990*41], labels_rank[990*41:]\n",
        "Y_train, Y_test = features_out[:990*41], features_out[990*41:]\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, Y_train.shape)\n",
        "print(\"Test shape:\", X_test.shape, Y_test.shape)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_class_train), torch.tensor(y_rank_train)), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_class_test), torch.tensor(y_rank_test)), batch_size=64)\n",
        "\n",
        "# Model Definitions\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class LSTM_MTL(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_size, batch_first=True)\n",
        "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
        "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h = h_n[-1]\n",
        "        return self.fc_cls(h), self.fc_rank(h)\n",
        "\n",
        "class GRU_MTL(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_size, batch_first=True)\n",
        "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
        "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        _, h_n = self.gru(x)\n",
        "        h = h_n[-1]\n",
        "        return self.fc_cls(h), self.fc_rank(h)\n",
        "\n",
        "class BaseMTL(nn.Module):\n",
        "    def __init__(self, base, hidden_size):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.classifier = nn.Linear(hidden_size, 2)\n",
        "        self.regressor = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        return self.classifier(x), self.regressor(x)\n",
        "\n",
        "# Training and Evaluation\n",
        "all_results = []\n",
        "rank_matrix = {}\n",
        "class_matrix = {}\n",
        "class_pred_matrix = {}\n",
        "true_rank_matrix = np.argsort(-y_rank_test.reshape(-1, 41), axis=1) + 1\n",
        "\n",
        "input_dim = combined_input.shape[1]\n",
        "hidden_size = 64\n",
        "\n",
        "def train_and_evaluate(model_name, model, train_loader, test_loader):\n",
        "    print(f\"\\nTraining model: {model_name}\")\n",
        "    criterion_class = nn.CrossEntropyLoss()\n",
        "    criterion_rank = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb_cls, yb_rank in train_loader:\n",
        "            out_cls, out_rank = model(xb)\n",
        "            loss_cls = criterion_class(out_cls, yb_cls)\n",
        "            loss_rank = criterion_rank(out_rank, yb_rank)\n",
        "            loss = loss_cls + 0.5 * loss_rank\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"{model_name} - Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_true, all_pred, pred_scores = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb_cls, yb_rank in test_loader:\n",
        "            out_cls, out_rank = model(xb)\n",
        "            preds = torch.argmax(out_cls, dim=1)\n",
        "            all_true.extend(yb_cls.cpu().numpy())\n",
        "            all_pred.extend(preds.cpu().numpy())\n",
        "            pred_scores.extend(out_rank.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_true, all_pred)\n",
        "    prec = precision_score(all_true, all_pred, zero_division=0)\n",
        "    rec = recall_score(all_true, all_pred, zero_division=0)\n",
        "    f1 = f1_score(all_true, all_pred, zero_division=0)\n",
        "\n",
        "    all_results.append({\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1})\n",
        "    class_matrix[model_name] = np.vstack(pred_scores)\n",
        "    class_pred_matrix[model_name] = np.array(all_pred).reshape(-1, 41)\n",
        "    rank_matrix[model_name] = np.argsort(-np.vstack(pred_scores), axis=1) + 1\n",
        "\n",
        "    print(f\"{model_name} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "train_and_evaluate(\"LSTM\", LSTM_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GRU\", GRU_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_GRU\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_GRU_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "\n",
        "with pd.ExcelWriter(\"phase2_model_results.xlsx\", engine='xlsxwriter') as writer:\n",
        "    pd.DataFrame(all_results).to_excel(writer, sheet_name=\"Summary\", index=False)\n",
        "    for model in rank_matrix:\n",
        "        headers = [f\"Scenario_{i//41}_Cont_{i%41}\" for i in range(rank_matrix[model].shape[0])]\n",
        "        df_rank = pd.DataFrame(rank_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=headers)\n",
        "        df_rank.to_excel(writer, sheet_name=f\"{model}_Ranking\")\n",
        "        df_cls = pd.DataFrame(class_pred_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{i}\" for i in range(10)])\n",
        "        df_cls.to_excel(writer, sheet_name=f\"{model}_Classify\")\n",
        "    df_true_severity = pd.DataFrame(np.array(y_class_test).reshape(-1, 41).T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(y_class_test)//41)])\n",
        "    df_true_severity.to_excel(writer, sheet_name=\"True_Severity\")\n",
        "    df_true_rank = pd.DataFrame(true_rank_matrix.T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(true_rank_matrix))])\n",
        "    df_true_rank.to_excel(writer, sheet_name=\"True_Ranking\")\n",
        "\n",
        "print(\"Excel file 'phase2_model_results.xlsx' updated with true rankings.\")\n",
        "\n",
        "with pd.ExcelWriter(\"line_flow_comparison.xlsx\", engine='xlsxwriter') as writer:\n",
        "    test_df = cont_df[cont_df['Scenario'] >= 990].reset_index(drop=True)\n",
        "\n",
        "    true_flow_matrix = []\n",
        "    true_columns = []\n",
        "    for scenario_id in range(990, 1000):\n",
        "        scenario_data = test_df[test_df['Scenario'] == scenario_id].reset_index(drop=True)\n",
        "        for outage_id in range(41):\n",
        "            outaged_line = scenario_data.loc[outage_id, 'Outaged_Line']\n",
        "            flow_row = []\n",
        "            for line_id in range(41):\n",
        "                flow = 0.0 if line_id == outaged_line else scenario_data.loc[outage_id, f\"Loading_line_{line_id}\"]\n",
        "                flow_row.append(flow)\n",
        "            true_flow_matrix.append(flow_row)\n",
        "            true_columns.append(f\"Scenario_{scenario_id - 990}_Outage_{outaged_line}\")\n",
        "    df_true_flows = pd.DataFrame(np.array(true_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=true_columns)\n",
        "    df_true_flows.to_excel(writer, sheet_name=\"True_Line_Flows\")\n",
        "\n",
        "    for model_name, preds in class_matrix.items():\n",
        "        pred_flow_matrix = []\n",
        "        pred_columns = []\n",
        "        for idx in range(preds.shape[0]):\n",
        "            scenario_idx = idx // 41\n",
        "            outage_idx = idx % 41\n",
        "            flow_row = []\n",
        "            for line_id in range(41):\n",
        "                flow = 0.0 if line_id == outage_idx else preds[idx][line_id] * 100\n",
        "                flow_row.append(flow)\n",
        "            pred_flow_matrix.append(flow_row)\n",
        "            pred_columns.append(f\"Scenario_{scenario_idx}_Outage_{outage_idx}\")\n",
        "        df_pred_flows = pd.DataFrame(np.array(pred_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=pred_columns)\n",
        "        df_pred_flows.to_excel(writer, sheet_name=f\"Pred_{model_name}_Flows\")\n",
        "\n",
        "print(\"Separate line flow comparison file 'line_flow_comparison.xlsx' created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ6rSEfZ5pt_",
        "outputId": "4fbee718-4341-4ebe-e136-34285f2233b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h====== HIGH-RESOLUTION SHAP ANALYSIS ======\n",
            "\n",
            "ğŸ” Loading data with contingency features...\n",
            "Input tensor shape: torch.Size([205, 152])\n",
            "Background shape: (5, 152)\n",
            "\n",
            "ğŸ“Š Loading Phase 2 model results...\n",
            "\n",
            "ğŸ§  Running SHAP analysis...\n",
            "\n",
            "ğŸ“Š Exporting SHAP summary tables...\n",
            "\n",
            "ğŸ¨ Generating 1000 DPI plots...\n",
            "\n",
            "âœ… Analysis complete! High-resolution plots saved to shap_results\n",
            "âœ… SHAP summary tables saved to shap_summary.xlsx\n"
          ]
        }
      ],
      "source": [
        "# COMPLETE SHAP ANALYSIS FOR PHASE 2 MODELS (SCENARIOS 5 TO 9)\n",
        "!pip install shap torch numpy pandas matplotlib xlsxwriter scikit-learn tqdm --quiet\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import shap\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from warnings import filterwarnings\n",
        "\n",
        "# ====================== CONFIGURATION ======================\n",
        "matplotlib.use('Agg')\n",
        "filterwarnings('ignore')\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams.update({\n",
        "    'figure.dpi': 1000,\n",
        "    'savefig.dpi': 1000,\n",
        "    'font.size': 8,\n",
        "    'axes.titlesize': 10,\n",
        "    'axes.labelsize': 9\n",
        "})\n",
        "\n",
        "# ====================== MODEL WRAPPER ======================\n",
        "class PrecomputedModel:\n",
        "    def __init__(self, classification_output, ranking_output):\n",
        "        self.classification_output = classification_output.astype(float).values.T\n",
        "        self.ranking_output = ranking_output.astype(float).values.T\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        if self.classification_output.shape[1] == 0:\n",
        "            class_output = np.zeros((batch_size, 2))\n",
        "        else:\n",
        "            repeats_cls = int(np.ceil(batch_size / self.classification_output.shape[0]))\n",
        "            class_output = np.tile(self.classification_output, (repeats_cls, 1))[:batch_size]\n",
        "\n",
        "        repeats_rank = int(np.ceil(batch_size / self.ranking_output.shape[0]))\n",
        "        rank_output = np.tile(self.ranking_output, (repeats_rank, 1))[:batch_size]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(class_output, dtype=torch.float32),\n",
        "            torch.tensor(rank_output, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "# ====================== LOAD DATA ======================\n",
        "def load_data_with_metadata():\n",
        "    load_df = pd.read_excel(\"/content/load_scenarios.xlsx\")\n",
        "    cont_df = pd.read_csv(\"/content/n1_contingency_balanced_filled_complete.csv\")\n",
        "    cont_df = cont_df[cont_df['Scenario'].between(995, 999)].reset_index(drop=True)\n",
        "    load_df = load_df[load_df['Scenario'].between(995, 999)]\n",
        "\n",
        "    load_df = load_df.pivot(index='Scenario', columns='Load_ID', values=['P_mw', 'Q_mvar'])\n",
        "    load_df.columns = [f\"{a}_{b}\" for a, b in load_df.columns]\n",
        "    load_df = load_df.sort_index(axis=1)\n",
        "    load_mat = load_df.values.astype(np.float32)\n",
        "    load_expanded = np.repeat(load_mat, 41, axis=0)\n",
        "\n",
        "    voltage_cols = [f\"V_bus_{i}\" for i in range(30)]\n",
        "    lineflow_cols = [f\"Loading_line_{i}\" for i in range(41)]\n",
        "    voltage_mat = cont_df[voltage_cols].values.astype(np.float32)\n",
        "    lineflow_mat = cont_df[lineflow_cols].values.astype(np.float32)\n",
        "\n",
        "    contingency_mat = np.zeros((cont_df.shape[0], 41))\n",
        "    contingency_mat[np.arange(cont_df.shape[0]), cont_df['Outaged_Line'].values] = 1\n",
        "\n",
        "    input_features = np.concatenate([\n",
        "        load_expanded, voltage_mat, lineflow_mat, contingency_mat\n",
        "    ], axis=1)\n",
        "\n",
        "    feature_names = []\n",
        "    for i in range(20):\n",
        "        feature_names += [f\"P_mw_load_{i}\", f\"Q_mvar_load_{i}\"]\n",
        "    feature_names += [f\"V_bus_{i}\" for i in range(30)]\n",
        "    feature_names += [f\"Flow_line_{i}\" for i in range(41)]\n",
        "    feature_names += [f\"Contingency_line_{i}\" for i in range(41)]\n",
        "\n",
        "    feature_metadata = {\n",
        "        'type_indices': {\n",
        "            'voltage': (40, 70),\n",
        "            'line_flow': (70, 111),\n",
        "            'contingency': (111, 152)\n",
        "        },\n",
        "        'group_cols': {\n",
        "            'voltage': feature_names[40:70],\n",
        "            'line_flow': feature_names[70:111],\n",
        "            'contingency': feature_names[111:152]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'tensor': torch.tensor(input_features).float(),\n",
        "        'feature_names': feature_names,\n",
        "        'scenario_ids': list(range(995, 1000)),\n",
        "        'feature_metadata': feature_metadata\n",
        "    }\n",
        "\n",
        "# ====================== RUN SHAP ======================\n",
        "def run_shap_analysis(models, background, metadata):\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        results[name] = {'cls': {}, 'rank': {}}\n",
        "        for task, output_index in zip(['cls', 'rank'], [0, 1]):\n",
        "            if task == 'cls' and model.classification_output.shape[1] == 0:\n",
        "                continue\n",
        "            explainer = shap.Explainer(lambda x: model(x)[output_index], background)\n",
        "            shap_values = explainer(background)\n",
        "            for group in metadata['type_indices']:\n",
        "                start, end = metadata['type_indices'][group]\n",
        "                values = shap_values.values[:, start:end]\n",
        "                if values.ndim > 2:\n",
        "                    values = np.mean(values, axis=1)\n",
        "                results[name][task][group] = {\n",
        "                    'values': values,\n",
        "                    'feature_names': metadata['group_cols'][group]\n",
        "                }\n",
        "    return results\n",
        "\n",
        "# ====================== SAVE PLOTS ======================\n",
        "def save_high_res_plots(results, output_dir='shap_results'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for model_name in results:\n",
        "        for task in results[model_name]:\n",
        "            for group in results[model_name][task]:\n",
        "                shap_vals = results[model_name][task][group]['values']\n",
        "                features = results[model_name][task][group]['feature_names']\n",
        "                if shap_vals.shape[1] != len(features):\n",
        "                    shap_vals = shap_vals[:, :len(features)]\n",
        "                shap.summary_plot(\n",
        "                    shap_vals, features=features,\n",
        "                    plot_type=\"bar\", show=False\n",
        "                )\n",
        "                plt.title(f\"{model_name} - {task.upper()} - {group.title()}\")\n",
        "                filename = f\"{model_name}_{task}_{group}.png\"\n",
        "                plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "# ====================== EXCEL EXPORT ======================\n",
        "def export_shap_summary_to_excel(results, output_file=\"shap_summary.xlsx\"):\n",
        "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
        "    for model_name in results:\n",
        "        for task in ['cls', 'rank']:\n",
        "            for group in results[model_name][task]:\n",
        "                values = results[model_name][task][group]['values']\n",
        "                features = results[model_name][task][group]['feature_names']\n",
        "                summary_values = np.mean(np.abs(values), axis=0)\n",
        "                min_len = min(len(summary_values), len(features))\n",
        "                df = pd.DataFrame({\n",
        "                    \"Feature\": features[:min_len],\n",
        "                    \"SHAP Value\": summary_values[:min_len]\n",
        "                })\n",
        "                df = df.sort_values(by=\"SHAP Value\", ascending=False)\n",
        "                sheet_name = f\"{model_name}_{task}_{group}\"[:31]\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "    writer.close()\n",
        "\n",
        "# ====================== MAIN ======================\n",
        "def main():\n",
        "    print(\"====== HIGH-RESOLUTION SHAP ANALYSIS ======\")\n",
        "    print(\"\\nğŸ” Loading data with contingency features...\")\n",
        "    data = load_data_with_metadata()\n",
        "    background = data['tensor'][:5].numpy()\n",
        "    print(f\"Input tensor shape: {data['tensor'].shape}\")\n",
        "    print(f\"Background shape: {background.shape}\")\n",
        "\n",
        "    print(\"\\nğŸ“Š Loading Phase 2 model results...\")\n",
        "    result_file = \"/content/phase2_model_results.xlsx\"\n",
        "    xls = pd.ExcelFile(result_file)\n",
        "\n",
        "    models = {}\n",
        "    model_names = [\n",
        "        (\"GCN_LSTM\", \"GCN_LSTM_Classify\", \"GCN_LSTM_Ranking\"),\n",
        "        (\"GCN_GRU_LSTM\", \"GCN_GRU_LSTM_Classify\", \"GCN_GRU_LSTM_Ranking\"),\n",
        "        (\"LSTM\", \"LSTM_Classify\", \"LSTM_Ranking\")\n",
        "    ]\n",
        "\n",
        "    cls_cols = [f\"Scenario_{i}\" for i in range(5, 10)]\n",
        "    rank_cols = [f\"Scenario_{i}_Cont_{j}\" for i in range(5, 10) for j in range(41)]\n",
        "\n",
        "    for tag, cls_sheet, rank_sheet in model_names:\n",
        "        classification_output = xls.parse(cls_sheet)\n",
        "        classification_output = classification_output.loc[:, classification_output.columns.isin(cls_cols)]\n",
        "        ranking_output = xls.parse(rank_sheet)\n",
        "        ranking_output = ranking_output.loc[:, ranking_output.columns.isin(rank_cols)]\n",
        "        models[tag] = PrecomputedModel(classification_output, ranking_output)\n",
        "\n",
        "    print(\"\\nğŸ§  Running SHAP analysis...\")\n",
        "    shap_results = run_shap_analysis(models, background, data['feature_metadata'])\n",
        "\n",
        "    print(\"\\nğŸ“Š Exporting SHAP summary tables...\")\n",
        "    export_shap_summary_to_excel(shap_results)\n",
        "\n",
        "    print(\"\\nğŸ¨ Generating 1000 DPI plots...\")\n",
        "    save_high_res_plots(shap_results)\n",
        "\n",
        "    print(\"\\nâœ… Analysis complete! High-resolution plots saved to shap_results\")\n",
        "    print(\"âœ… SHAP summary tables saved to shap_summary.xlsx\")\n",
        "\n",
        "    for model_name in shap_results:\n",
        "        summary_df = []\n",
        "        for group in ['voltage', 'line_flow', 'contingency']:\n",
        "            for task in ['cls', 'rank']:\n",
        "                values = shap_results[model_name][task][group]['values']\n",
        "                avg = np.mean(np.abs(values))\n",
        "                summary_df.append({\"Feature Group\": group.title(), \"Task\": 'Classification' if task == 'cls' else 'Ranking', \"Average SHAP Value\": avg})\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_df)\n",
        "\n",
        "        cls_df = summary_df[summary_df['Task'] == 'Classification']\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(cls_df['Feature Group'], cls_df['Average SHAP Value'], color='skyblue')\n",
        "        plt.ylabel(\"Average SHAP Value\")\n",
        "        plt.title(f\"{model_name} - Classification Feature Importance\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"shap_results/{model_name}_Classification_Bar.png\", dpi=1000)\n",
        "        plt.show()\n",
        "\n",
        "        rank_df = summary_df[summary_df['Task'] == 'Ranking']\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(rank_df['Feature Group'], rank_df['Average SHAP Value'], color='salmon')\n",
        "        plt.ylabel(\"Average SHAP Value\")\n",
        "        plt.title(f\"{model_name} - Ranking Feature Importance\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"shap_results/{model_name}_Ranking_Bar.png\", dpi=1000)\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
