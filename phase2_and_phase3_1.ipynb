{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "%pip install torch-geometric\n",
        "%pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install shap torch numpy pandas matplotlib xlsxwriter scikit-learn tqdm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGe4avZvk8a4",
        "outputId": "f6a7193d-9445-4190-d2ae-cbf1110d1d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes:\n",
            "- Combined input: (41000, 111)\n",
            "- Target features: (41000, 71)\n",
            "- Severity labels: (41000,)\n",
            "- Ranking shape: (41000, 41)\n",
            "Train shape: (40590, 111) (40590, 71)\n",
            "Test shape: (410, 111) (410, 71)\n",
            "\n",
            "Training model: LSTM\n",
            "LSTM - Epoch 1 Loss: 0.2773\n",
            "LSTM - Epoch 2 Loss: 0.1183\n",
            "LSTM - Epoch 3 Loss: 0.0977\n",
            "LSTM - Epoch 4 Loss: 0.0946\n",
            "LSTM - Epoch 5 Loss: 0.0910\n",
            "LSTM - Epoch 6 Loss: 0.0761\n",
            "LSTM - Epoch 7 Loss: 0.0699\n",
            "LSTM - Epoch 8 Loss: 0.0721\n",
            "LSTM - Epoch 9 Loss: 0.0720\n",
            "LSTM - Epoch 10 Loss: 0.0680\n",
            "LSTM - Epoch 11 Loss: 0.0665\n",
            "LSTM - Epoch 12 Loss: 0.0739\n",
            "LSTM - Epoch 13 Loss: 0.0602\n",
            "LSTM - Epoch 14 Loss: 0.0612\n",
            "LSTM - Epoch 15 Loss: 0.0622\n",
            "LSTM - Epoch 16 Loss: 0.0589\n",
            "LSTM - Epoch 17 Loss: 0.0613\n",
            "LSTM - Epoch 18 Loss: 0.0668\n",
            "LSTM - Epoch 19 Loss: 0.0560\n",
            "LSTM - Epoch 20 Loss: 0.0597\n",
            "LSTM - Epoch 21 Loss: 0.0578\n",
            "LSTM - Epoch 22 Loss: 0.0570\n",
            "LSTM - Epoch 23 Loss: 0.0579\n",
            "LSTM - Epoch 24 Loss: 0.0548\n",
            "LSTM - Epoch 25 Loss: 0.0559\n",
            "LSTM - Epoch 26 Loss: 0.0534\n",
            "LSTM - Epoch 27 Loss: 0.0571\n",
            "LSTM - Epoch 28 Loss: 0.0539\n",
            "LSTM - Epoch 29 Loss: 0.0541\n",
            "LSTM - Epoch 30 Loss: 0.0515\n",
            "LSTM - Epoch 31 Loss: 0.0493\n",
            "LSTM - Epoch 32 Loss: 0.0550\n",
            "LSTM - Epoch 33 Loss: 0.0525\n",
            "LSTM - Epoch 34 Loss: 0.0487\n",
            "LSTM - Epoch 35 Loss: 0.0564\n",
            "LSTM - Epoch 36 Loss: 0.0526\n",
            "LSTM - Epoch 37 Loss: 0.0488\n",
            "LSTM - Epoch 38 Loss: 0.0520\n",
            "LSTM - Epoch 39 Loss: 0.0512\n",
            "LSTM - Epoch 40 Loss: 0.0505\n",
            "LSTM - Epoch 41 Loss: 0.0531\n",
            "LSTM - Epoch 42 Loss: 0.0494\n",
            "LSTM - Epoch 43 Loss: 0.0512\n",
            "LSTM - Epoch 44 Loss: 0.0516\n",
            "LSTM - Epoch 45 Loss: 0.0465\n",
            "LSTM - Epoch 46 Loss: 0.0480\n",
            "LSTM - Epoch 47 Loss: 0.0489\n",
            "LSTM - Epoch 48 Loss: 0.0486\n",
            "LSTM - Epoch 49 Loss: 0.0479\n",
            "LSTM - Epoch 50 Loss: 0.0520\n",
            "LSTM - Accuracy: 0.9829, Precision: 0.9840, Recall: 0.9609, F1: 0.9723\n",
            "\n",
            "Training model: GRU\n",
            "GRU - Epoch 1 Loss: 0.2681\n",
            "GRU - Epoch 2 Loss: 0.1158\n",
            "GRU - Epoch 3 Loss: 0.0920\n",
            "GRU - Epoch 4 Loss: 0.0849\n",
            "GRU - Epoch 5 Loss: 0.0841\n",
            "GRU - Epoch 6 Loss: 0.0764\n",
            "GRU - Epoch 7 Loss: 0.0757\n",
            "GRU - Epoch 8 Loss: 0.0695\n",
            "GRU - Epoch 9 Loss: 0.0727\n",
            "GRU - Epoch 10 Loss: 0.0660\n",
            "GRU - Epoch 11 Loss: 0.0684\n",
            "GRU - Epoch 12 Loss: 0.0707\n",
            "GRU - Epoch 13 Loss: 0.0697\n",
            "GRU - Epoch 14 Loss: 0.0634\n",
            "GRU - Epoch 15 Loss: 0.0661\n",
            "GRU - Epoch 16 Loss: 0.0654\n",
            "GRU - Epoch 17 Loss: 0.0591\n",
            "GRU - Epoch 18 Loss: 0.0664\n",
            "GRU - Epoch 19 Loss: 0.0606\n",
            "GRU - Epoch 20 Loss: 0.0614\n",
            "GRU - Epoch 21 Loss: 0.0681\n",
            "GRU - Epoch 22 Loss: 0.0666\n",
            "GRU - Epoch 23 Loss: 0.0609\n",
            "GRU - Epoch 24 Loss: 0.0596\n",
            "GRU - Epoch 25 Loss: 0.0583\n",
            "GRU - Epoch 26 Loss: 0.0556\n",
            "GRU - Epoch 27 Loss: 0.0602\n",
            "GRU - Epoch 28 Loss: 0.0565\n",
            "GRU - Epoch 29 Loss: 0.0542\n",
            "GRU - Epoch 30 Loss: 0.0544\n",
            "GRU - Epoch 31 Loss: 0.0556\n",
            "GRU - Epoch 32 Loss: 0.0534\n",
            "GRU - Epoch 33 Loss: 0.0519\n",
            "GRU - Epoch 34 Loss: 0.0525\n",
            "GRU - Epoch 35 Loss: 0.0560\n",
            "GRU - Epoch 36 Loss: 0.0564\n",
            "GRU - Epoch 37 Loss: 0.0535\n",
            "GRU - Epoch 38 Loss: 0.0556\n",
            "GRU - Epoch 39 Loss: 0.0514\n",
            "GRU - Epoch 40 Loss: 0.0496\n",
            "GRU - Epoch 41 Loss: 0.0546\n",
            "GRU - Epoch 42 Loss: 0.0517\n",
            "GRU - Epoch 43 Loss: 0.0510\n",
            "GRU - Epoch 44 Loss: 0.0517\n",
            "GRU - Epoch 45 Loss: 0.0507\n",
            "GRU - Epoch 46 Loss: 0.0522\n",
            "GRU - Epoch 47 Loss: 0.0546\n",
            "GRU - Epoch 48 Loss: 0.0551\n",
            "GRU - Epoch 49 Loss: 0.0533\n",
            "GRU - Epoch 50 Loss: 0.0543\n",
            "GRU - Accuracy: 0.9805, Precision: 0.9545, Recall: 0.9844, F1: 0.9692\n",
            "\n",
            "Training model: GCN\n",
            "GCN - Epoch 1 Loss: 0.2747\n",
            "GCN - Epoch 2 Loss: 0.1373\n",
            "GCN - Epoch 3 Loss: 0.1236\n",
            "GCN - Epoch 4 Loss: 0.1091\n",
            "GCN - Epoch 5 Loss: 0.1069\n",
            "GCN - Epoch 6 Loss: 0.0996\n",
            "GCN - Epoch 7 Loss: 0.0939\n",
            "GCN - Epoch 8 Loss: 0.0863\n",
            "GCN - Epoch 9 Loss: 0.0860\n",
            "GCN - Epoch 10 Loss: 0.0934\n",
            "GCN - Epoch 11 Loss: 0.0766\n",
            "GCN - Epoch 12 Loss: 0.0829\n",
            "GCN - Epoch 13 Loss: 0.0707\n",
            "GCN - Epoch 14 Loss: 0.0692\n",
            "GCN - Epoch 15 Loss: 0.0746\n",
            "GCN - Epoch 16 Loss: 0.0711\n",
            "GCN - Epoch 17 Loss: 0.0672\n",
            "GCN - Epoch 18 Loss: 0.0628\n",
            "GCN - Epoch 19 Loss: 0.0678\n",
            "GCN - Epoch 20 Loss: 0.0655\n",
            "GCN - Epoch 21 Loss: 0.0722\n",
            "GCN - Epoch 22 Loss: 0.0655\n",
            "GCN - Epoch 23 Loss: 0.0618\n",
            "GCN - Epoch 24 Loss: 0.0616\n",
            "GCN - Epoch 25 Loss: 0.0619\n",
            "GCN - Epoch 26 Loss: 0.0594\n",
            "GCN - Epoch 27 Loss: 0.0638\n",
            "GCN - Epoch 28 Loss: 0.0621\n",
            "GCN - Epoch 29 Loss: 0.0631\n",
            "GCN - Epoch 30 Loss: 0.0631\n",
            "GCN - Epoch 31 Loss: 0.0639\n",
            "GCN - Epoch 32 Loss: 0.0591\n",
            "GCN - Epoch 33 Loss: 0.0589\n",
            "GCN - Epoch 34 Loss: 0.0556\n",
            "GCN - Epoch 35 Loss: 0.0552\n",
            "GCN - Epoch 36 Loss: 0.0591\n",
            "GCN - Epoch 37 Loss: 0.0516\n",
            "GCN - Epoch 38 Loss: 0.0579\n",
            "GCN - Epoch 39 Loss: 0.0526\n",
            "GCN - Epoch 40 Loss: 0.0498\n",
            "GCN - Epoch 41 Loss: 0.0534\n",
            "GCN - Epoch 42 Loss: 0.0565\n",
            "GCN - Epoch 43 Loss: 0.0520\n",
            "GCN - Epoch 44 Loss: 0.0530\n",
            "GCN - Epoch 45 Loss: 0.0545\n",
            "GCN - Epoch 46 Loss: 0.0503\n",
            "GCN - Epoch 47 Loss: 0.0512\n",
            "GCN - Epoch 48 Loss: 0.0519\n",
            "GCN - Epoch 49 Loss: 0.0539\n",
            "GCN - Epoch 50 Loss: 0.0497\n",
            "GCN - Accuracy: 0.9854, Precision: 0.9692, Recall: 0.9844, F1: 0.9767\n",
            "\n",
            "Training model: GCN_LSTM\n",
            "GCN_LSTM - Epoch 1 Loss: 0.2595\n",
            "GCN_LSTM - Epoch 2 Loss: 0.1299\n",
            "GCN_LSTM - Epoch 3 Loss: 0.1162\n",
            "GCN_LSTM - Epoch 4 Loss: 0.1033\n",
            "GCN_LSTM - Epoch 5 Loss: 0.0982\n",
            "GCN_LSTM - Epoch 6 Loss: 0.0881\n",
            "GCN_LSTM - Epoch 7 Loss: 0.0896\n",
            "GCN_LSTM - Epoch 8 Loss: 0.0788\n",
            "GCN_LSTM - Epoch 9 Loss: 0.0761\n",
            "GCN_LSTM - Epoch 10 Loss: 0.0713\n",
            "GCN_LSTM - Epoch 11 Loss: 0.0789\n",
            "GCN_LSTM - Epoch 12 Loss: 0.0705\n",
            "GCN_LSTM - Epoch 13 Loss: 0.0688\n",
            "GCN_LSTM - Epoch 14 Loss: 0.0689\n",
            "GCN_LSTM - Epoch 15 Loss: 0.0625\n",
            "GCN_LSTM - Epoch 16 Loss: 0.0662\n",
            "GCN_LSTM - Epoch 17 Loss: 0.0677\n",
            "GCN_LSTM - Epoch 18 Loss: 0.0656\n",
            "GCN_LSTM - Epoch 19 Loss: 0.0621\n",
            "GCN_LSTM - Epoch 20 Loss: 0.0611\n",
            "GCN_LSTM - Epoch 21 Loss: 0.0561\n",
            "GCN_LSTM - Epoch 22 Loss: 0.0551\n",
            "GCN_LSTM - Epoch 23 Loss: 0.0579\n",
            "GCN_LSTM - Epoch 24 Loss: 0.0562\n",
            "GCN_LSTM - Epoch 25 Loss: 0.0561\n",
            "GCN_LSTM - Epoch 26 Loss: 0.0613\n",
            "GCN_LSTM - Epoch 27 Loss: 0.0532\n",
            "GCN_LSTM - Epoch 28 Loss: 0.0582\n",
            "GCN_LSTM - Epoch 29 Loss: 0.0553\n",
            "GCN_LSTM - Epoch 30 Loss: 0.0570\n",
            "GCN_LSTM - Epoch 31 Loss: 0.0563\n",
            "GCN_LSTM - Epoch 32 Loss: 0.0542\n",
            "GCN_LSTM - Epoch 33 Loss: 0.0543\n",
            "GCN_LSTM - Epoch 34 Loss: 0.0506\n",
            "GCN_LSTM - Epoch 35 Loss: 0.0563\n",
            "GCN_LSTM - Epoch 36 Loss: 0.0504\n",
            "GCN_LSTM - Epoch 37 Loss: 0.0512\n",
            "GCN_LSTM - Epoch 38 Loss: 0.0568\n",
            "GCN_LSTM - Epoch 39 Loss: 0.0487\n",
            "GCN_LSTM - Epoch 40 Loss: 0.0552\n",
            "GCN_LSTM - Epoch 41 Loss: 0.0526\n",
            "GCN_LSTM - Epoch 42 Loss: 0.0505\n",
            "GCN_LSTM - Epoch 43 Loss: 0.0517\n",
            "GCN_LSTM - Epoch 44 Loss: 0.0482\n",
            "GCN_LSTM - Epoch 45 Loss: 0.0525\n",
            "GCN_LSTM - Epoch 46 Loss: 0.0462\n",
            "GCN_LSTM - Epoch 47 Loss: 0.0476\n",
            "GCN_LSTM - Epoch 48 Loss: 0.0448\n",
            "GCN_LSTM - Epoch 49 Loss: 0.0532\n",
            "GCN_LSTM - Epoch 50 Loss: 0.0465\n",
            "GCN_LSTM - Accuracy: 0.9902, Precision: 1.0000, Recall: 0.9688, F1: 0.9841\n",
            "\n",
            "Training model: GCN_GRU\n",
            "GCN_GRU - Epoch 1 Loss: 0.2934\n",
            "GCN_GRU - Epoch 2 Loss: 0.1321\n",
            "GCN_GRU - Epoch 3 Loss: 0.1197\n",
            "GCN_GRU - Epoch 4 Loss: 0.1048\n",
            "GCN_GRU - Epoch 5 Loss: 0.0971\n",
            "GCN_GRU - Epoch 6 Loss: 0.0928\n",
            "GCN_GRU - Epoch 7 Loss: 0.0840\n",
            "GCN_GRU - Epoch 8 Loss: 0.0881\n",
            "GCN_GRU - Epoch 9 Loss: 0.0786\n",
            "GCN_GRU - Epoch 10 Loss: 0.0842\n",
            "GCN_GRU - Epoch 11 Loss: 0.0756\n",
            "GCN_GRU - Epoch 12 Loss: 0.0781\n",
            "GCN_GRU - Epoch 13 Loss: 0.0772\n",
            "GCN_GRU - Epoch 14 Loss: 0.0754\n",
            "GCN_GRU - Epoch 15 Loss: 0.0651\n",
            "GCN_GRU - Epoch 16 Loss: 0.0694\n",
            "GCN_GRU - Epoch 17 Loss: 0.0735\n",
            "GCN_GRU - Epoch 18 Loss: 0.0623\n",
            "GCN_GRU - Epoch 19 Loss: 0.0651\n",
            "GCN_GRU - Epoch 20 Loss: 0.0629\n",
            "GCN_GRU - Epoch 21 Loss: 0.0692\n",
            "GCN_GRU - Epoch 22 Loss: 0.0679\n",
            "GCN_GRU - Epoch 23 Loss: 0.0632\n",
            "GCN_GRU - Epoch 24 Loss: 0.0603\n",
            "GCN_GRU - Epoch 25 Loss: 0.0573\n",
            "GCN_GRU - Epoch 26 Loss: 0.0582\n",
            "GCN_GRU - Epoch 27 Loss: 0.0566\n",
            "GCN_GRU - Epoch 28 Loss: 0.0568\n",
            "GCN_GRU - Epoch 29 Loss: 0.0567\n",
            "GCN_GRU - Epoch 30 Loss: 0.0583\n",
            "GCN_GRU - Epoch 31 Loss: 0.0609\n",
            "GCN_GRU - Epoch 32 Loss: 0.0619\n",
            "GCN_GRU - Epoch 33 Loss: 0.0528\n",
            "GCN_GRU - Epoch 34 Loss: 0.0576\n",
            "GCN_GRU - Epoch 35 Loss: 0.0565\n",
            "GCN_GRU - Epoch 36 Loss: 0.0529\n",
            "GCN_GRU - Epoch 37 Loss: 0.0520\n",
            "GCN_GRU - Epoch 38 Loss: 0.0512\n",
            "GCN_GRU - Epoch 39 Loss: 0.0511\n",
            "GCN_GRU - Epoch 40 Loss: 0.0510\n",
            "GCN_GRU - Epoch 41 Loss: 0.0523\n",
            "GCN_GRU - Epoch 42 Loss: 0.0511\n",
            "GCN_GRU - Epoch 43 Loss: 0.0515\n",
            "GCN_GRU - Epoch 44 Loss: 0.0514\n",
            "GCN_GRU - Epoch 45 Loss: 0.0546\n",
            "GCN_GRU - Epoch 46 Loss: 0.0494\n",
            "GCN_GRU - Epoch 47 Loss: 0.0500\n",
            "GCN_GRU - Epoch 48 Loss: 0.0486\n",
            "GCN_GRU - Epoch 49 Loss: 0.0495\n",
            "GCN_GRU - Epoch 50 Loss: 0.0451\n",
            "GCN_GRU - Accuracy: 0.9805, Precision: 0.9545, Recall: 0.9844, F1: 0.9692\n",
            "\n",
            "Training model: GCN_GRU_LSTM\n",
            "GCN_GRU_LSTM - Epoch 1 Loss: 0.2851\n",
            "GCN_GRU_LSTM - Epoch 2 Loss: 0.1262\n",
            "GCN_GRU_LSTM - Epoch 3 Loss: 0.1103\n",
            "GCN_GRU_LSTM - Epoch 4 Loss: 0.0976\n",
            "GCN_GRU_LSTM - Epoch 5 Loss: 0.0913\n",
            "GCN_GRU_LSTM - Epoch 6 Loss: 0.0876\n",
            "GCN_GRU_LSTM - Epoch 7 Loss: 0.0848\n",
            "GCN_GRU_LSTM - Epoch 8 Loss: 0.0769\n",
            "GCN_GRU_LSTM - Epoch 9 Loss: 0.0870\n",
            "GCN_GRU_LSTM - Epoch 10 Loss: 0.0773\n",
            "GCN_GRU_LSTM - Epoch 11 Loss: 0.0802\n",
            "GCN_GRU_LSTM - Epoch 12 Loss: 0.0695\n",
            "GCN_GRU_LSTM - Epoch 13 Loss: 0.0759\n",
            "GCN_GRU_LSTM - Epoch 14 Loss: 0.0688\n",
            "GCN_GRU_LSTM - Epoch 15 Loss: 0.0654\n",
            "GCN_GRU_LSTM - Epoch 16 Loss: 0.0700\n",
            "GCN_GRU_LSTM - Epoch 17 Loss: 0.0610\n",
            "GCN_GRU_LSTM - Epoch 18 Loss: 0.0628\n",
            "GCN_GRU_LSTM - Epoch 19 Loss: 0.0669\n",
            "GCN_GRU_LSTM - Epoch 20 Loss: 0.0689\n",
            "GCN_GRU_LSTM - Epoch 21 Loss: 0.0628\n",
            "GCN_GRU_LSTM - Epoch 22 Loss: 0.0579\n",
            "GCN_GRU_LSTM - Epoch 23 Loss: 0.0627\n",
            "GCN_GRU_LSTM - Epoch 24 Loss: 0.0608\n",
            "GCN_GRU_LSTM - Epoch 25 Loss: 0.0582\n",
            "GCN_GRU_LSTM - Epoch 26 Loss: 0.0581\n",
            "GCN_GRU_LSTM - Epoch 27 Loss: 0.0578\n",
            "GCN_GRU_LSTM - Epoch 28 Loss: 0.0620\n",
            "GCN_GRU_LSTM - Epoch 29 Loss: 0.0580\n",
            "GCN_GRU_LSTM - Epoch 30 Loss: 0.0619\n",
            "GCN_GRU_LSTM - Epoch 31 Loss: 0.0515\n",
            "GCN_GRU_LSTM - Epoch 32 Loss: 0.0535\n",
            "GCN_GRU_LSTM - Epoch 33 Loss: 0.0595\n",
            "GCN_GRU_LSTM - Epoch 34 Loss: 0.0531\n",
            "GCN_GRU_LSTM - Epoch 35 Loss: 0.0558\n",
            "GCN_GRU_LSTM - Epoch 36 Loss: 0.0520\n",
            "GCN_GRU_LSTM - Epoch 37 Loss: 0.0517\n",
            "GCN_GRU_LSTM - Epoch 38 Loss: 0.0600\n",
            "GCN_GRU_LSTM - Epoch 39 Loss: 0.0558\n",
            "GCN_GRU_LSTM - Epoch 40 Loss: 0.0506\n",
            "GCN_GRU_LSTM - Epoch 41 Loss: 0.0517\n",
            "GCN_GRU_LSTM - Epoch 42 Loss: 0.0479\n",
            "GCN_GRU_LSTM - Epoch 43 Loss: 0.0533\n",
            "GCN_GRU_LSTM - Epoch 44 Loss: 0.0545\n",
            "GCN_GRU_LSTM - Epoch 45 Loss: 0.0528\n",
            "GCN_GRU_LSTM - Epoch 46 Loss: 0.0546\n",
            "GCN_GRU_LSTM - Epoch 47 Loss: 0.0518\n",
            "GCN_GRU_LSTM - Epoch 48 Loss: 0.0457\n",
            "GCN_GRU_LSTM - Epoch 49 Loss: 0.0549\n",
            "GCN_GRU_LSTM - Epoch 50 Loss: 0.0503\n",
            "GCN_GRU_LSTM - Accuracy: 0.9902, Precision: 0.9844, Recall: 0.9844, F1: 0.9844\n",
            "Excel file 'phase2_model_results_50epochs.xlsx' updated with true rankings.\n",
            "Separate line flow comparison file 'line_flow_comparison_50epochs.xlsx' created.\n"
          ]
        }
      ],
      "source": [
        "# GCN-LSTM, LSTM, GRU, and GCN Multi-Task Learning Models for Phase 2 Contingency Prediction\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch_geometric.nn import GCNConv\n",
        "import xlsxwriter\n",
        "\n",
        "# Load input files\n",
        "load_df = pd.read_excel(\"load_scenarios.xlsx\")\n",
        "cont_df = pd.read_csv(\"n1_contingency_balanced_filled_complete.csv\")\n",
        "cont_df = cont_df[cont_df['Scenario'] < 1000].reset_index(drop=True)\n",
        "\n",
        "# Extract and reshape load features\n",
        "load_features = load_df[[\"P_mw\", \"Q_mvar\"]].values\n",
        "assert load_features.shape[0] == 20000, \"Expected 20000 rows of load data\"\n",
        "load_features = load_features.reshape(1000, 40)\n",
        "repeat_factor = 41\n",
        "load_features_expanded = np.repeat(load_features, repeat_factor, axis=0)\n",
        "\n",
        "# Extract voltages and line flows\n",
        "bus_cols = [col for col in cont_df.columns if col.startswith(\"V_bus_\")]\n",
        "line_cols = [col for col in cont_df.columns if col.startswith(\"Loading_line_\")]\n",
        "voltages = cont_df[bus_cols].values.astype(np.float32)\n",
        "line_flows = cont_df[line_cols].values.astype(np.float32)\n",
        "combined_input = np.concatenate([load_features_expanded, voltages, line_flows], axis=1)\n",
        "\n",
        "# Targets\n",
        "features_out = cont_df[bus_cols + line_cols].values.astype(np.float32)\n",
        "labels_class = cont_df['Severity'].values.astype(np.int64)\n",
        "labels_rank = cont_df[line_cols].values.astype(np.float32) / 100\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Input shapes:\")\n",
        "print(\"- Combined input:\", combined_input.shape)\n",
        "print(\"- Target features:\", features_out.shape)\n",
        "print(\"- Severity labels:\", labels_class.shape)\n",
        "print(\"- Ranking shape:\", labels_rank.shape)\n",
        "\n",
        "assert combined_input.shape[0] == features_out.shape[0] == labels_class.shape[0] == labels_rank.shape[0]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test = combined_input[:990*41], combined_input[990*41:]\n",
        "y_class_train, y_class_test = labels_class[:990*41], labels_class[990*41:]\n",
        "y_rank_train, y_rank_test = labels_rank[:990*41], labels_rank[990*41:]\n",
        "Y_train, Y_test = features_out[:990*41], features_out[990*41:]\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, Y_train.shape)\n",
        "print(\"Test shape:\", X_test.shape, Y_test.shape)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_class_train), torch.tensor(y_rank_train)), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_class_test), torch.tensor(y_rank_test)), batch_size=64)\n",
        "\n",
        "# Model Definitions\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class LSTM_MTL(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_size, batch_first=True)\n",
        "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
        "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h = h_n[-1]\n",
        "        return self.fc_cls(h), self.fc_rank(h)\n",
        "\n",
        "class GRU_MTL(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_size, batch_first=True)\n",
        "        self.fc_cls = nn.Linear(hidden_size, 2)\n",
        "        self.fc_rank = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        _, h_n = self.gru(x)\n",
        "        h = h_n[-1]\n",
        "        return self.fc_cls(h), self.fc_rank(h)\n",
        "\n",
        "class BaseMTL(nn.Module):\n",
        "    def __init__(self, base, hidden_size):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.classifier = nn.Linear(hidden_size, 2)\n",
        "        self.regressor = nn.Linear(hidden_size, 41)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        return self.classifier(x), self.regressor(x)\n",
        "\n",
        "# Training and Evaluation\n",
        "all_results = []\n",
        "rank_matrix = {}\n",
        "class_matrix = {}\n",
        "class_pred_matrix = {}\n",
        "true_rank_matrix = np.argsort(-y_rank_test.reshape(-1, 41), axis=1) + 1\n",
        "\n",
        "input_dim = combined_input.shape[1]\n",
        "hidden_size = 64\n",
        "\n",
        "def train_and_evaluate(model_name, model, train_loader, test_loader):\n",
        "    print(f\"\\nTraining model: {model_name}\")\n",
        "    criterion_class = nn.CrossEntropyLoss()\n",
        "    criterion_rank = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb_cls, yb_rank in train_loader:\n",
        "            out_cls, out_rank = model(xb)\n",
        "            loss_cls = criterion_class(out_cls, yb_cls)\n",
        "            loss_rank = criterion_rank(out_rank, yb_rank)\n",
        "            loss = loss_cls + 0.5 * loss_rank\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"{model_name} - Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_true, all_pred, pred_scores = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb_cls, yb_rank in test_loader:\n",
        "            out_cls, out_rank = model(xb)\n",
        "            preds = torch.argmax(out_cls, dim=1)\n",
        "            all_true.extend(yb_cls.cpu().numpy())\n",
        "            all_pred.extend(preds.cpu().numpy())\n",
        "            pred_scores.extend(out_rank.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_true, all_pred)\n",
        "    prec = precision_score(all_true, all_pred, zero_division=0)\n",
        "    rec = recall_score(all_true, all_pred, zero_division=0)\n",
        "    f1 = f1_score(all_true, all_pred, zero_division=0)\n",
        "\n",
        "    all_results.append({\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1})\n",
        "    class_matrix[model_name] = np.vstack(pred_scores)\n",
        "    class_pred_matrix[model_name] = np.array(all_pred).reshape(-1, 41)\n",
        "    rank_matrix[model_name] = np.argsort(-np.vstack(pred_scores), axis=1) + 1\n",
        "\n",
        "    print(f\"{model_name} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "train_and_evaluate(\"LSTM\", LSTM_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GRU\", GRU_MTL(input_dim, hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_GRU\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "train_and_evaluate(\"GCN_GRU_LSTM\", BaseMTL(FeedForward(input_dim, hidden_size), hidden_size), train_loader, test_loader)\n",
        "\n",
        "with pd.ExcelWriter(\"phase2_model_results_50epochs.xlsx\", engine='xlsxwriter') as writer:\n",
        "    pd.DataFrame(all_results).to_excel(writer, sheet_name=\"Summary\", index=False)\n",
        "    for model in rank_matrix:\n",
        "        headers = [f\"Scenario_{i//41}_Cont_{i%41}\" for i in range(rank_matrix[model].shape[0])]\n",
        "        df_rank = pd.DataFrame(rank_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=headers)\n",
        "        df_rank.to_excel(writer, sheet_name=f\"{model}_Ranking\")\n",
        "        df_cls = pd.DataFrame(class_pred_matrix[model].T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{i}\" for i in range(10)])\n",
        "        df_cls.to_excel(writer, sheet_name=f\"{model}_Classify\")\n",
        "    df_true_severity = pd.DataFrame(np.array(y_class_test).reshape(-1, 41).T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(y_class_test)//41)])\n",
        "    df_true_severity.to_excel(writer, sheet_name=\"True_Severity\")\n",
        "    df_true_rank = pd.DataFrame(true_rank_matrix.T, index=[f\"Line_{i}\" for i in range(41)], columns=[f\"Scenario_{j}\" for j in range(len(true_rank_matrix))])\n",
        "    df_true_rank.to_excel(writer, sheet_name=\"True_Ranking\")\n",
        "\n",
        "print(\"Excel file 'phase2_model_results_50epochs.xlsx' updated with true rankings.\")\n",
        "\n",
        "with pd.ExcelWriter(\"line_flow_comparison_50epochs.xlsx\", engine='xlsxwriter') as writer:\n",
        "    test_df = cont_df[cont_df['Scenario'] >= 990].reset_index(drop=True)\n",
        "\n",
        "    true_flow_matrix = []\n",
        "    true_columns = []\n",
        "    for scenario_id in range(990, 1000):\n",
        "        scenario_data = test_df[test_df['Scenario'] == scenario_id].reset_index(drop=True)\n",
        "        for outage_id in range(41):\n",
        "            outaged_line = scenario_data.loc[outage_id, 'Outaged_Line']\n",
        "            flow_row = []\n",
        "            for line_id in range(41):\n",
        "                flow = 0.0 if line_id == outaged_line else scenario_data.loc[outage_id, f\"Loading_line_{line_id}\"]\n",
        "                flow_row.append(flow)\n",
        "            true_flow_matrix.append(flow_row)\n",
        "            true_columns.append(f\"Scenario_{scenario_id - 990}_Outage_{outaged_line}\")\n",
        "    df_true_flows = pd.DataFrame(np.array(true_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=true_columns)\n",
        "    df_true_flows.to_excel(writer, sheet_name=\"True_Line_Flows\")\n",
        "\n",
        "    for model_name, preds in class_matrix.items():\n",
        "        pred_flow_matrix = []\n",
        "        pred_columns = []\n",
        "        for idx in range(preds.shape[0]):\n",
        "            scenario_idx = idx // 41\n",
        "            outage_idx = idx % 41\n",
        "            flow_row = []\n",
        "            for line_id in range(41):\n",
        "                flow = 0.0 if line_id == outage_idx else preds[idx][line_id] * 100\n",
        "                flow_row.append(flow)\n",
        "            pred_flow_matrix.append(flow_row)\n",
        "            pred_columns.append(f\"Scenario_{scenario_idx}_Outage_{outage_idx}\")\n",
        "        df_pred_flows = pd.DataFrame(np.array(pred_flow_matrix).T, index=[f\"Line_{i}\" for i in range(41)], columns=pred_columns)\n",
        "        df_pred_flows.to_excel(writer, sheet_name=f\"Pred_{model_name}_Flows\")\n",
        "\n",
        "print(\"Separate line flow comparison file 'line_flow_comparison_50epochs.xlsx' created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ6rSEfZ5pt_",
        "outputId": "4fbee718-4341-4ebe-e136-34285f2233b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== HIGH-RESOLUTION SHAP ANALYSIS ======\n",
            "\n",
            "üîç Loading data with contingency features...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tensor shape: torch.Size([205, 152])\n",
            "Background shape: (5, 152)\n",
            "\n",
            "üìä Loading Phase 2 model results...\n",
            "\n",
            "üß† Running SHAP analysis...\n",
            "\n",
            "üìä Exporting SHAP summary tables...\n",
            "\n",
            "üé® Generating 1000 DPI plots...\n",
            "\n",
            "‚úÖ Analysis complete! High-resolution plots saved to shap_results\n",
            "‚úÖ SHAP summary tables saved to shap_summary_50epochs.xlsx\n"
          ]
        }
      ],
      "source": [
        "# COMPLETE SHAP ANALYSIS FOR PHASE 2 MODELS (SCENARIOS 5 TO 9)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import shap\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from warnings import filterwarnings\n",
        "\n",
        "# ====================== CONFIGURATION ======================\n",
        "matplotlib.use('Agg')\n",
        "filterwarnings('ignore')\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams.update({\n",
        "    'figure.dpi': 1000,\n",
        "    'savefig.dpi': 1000,\n",
        "    'font.size': 8,\n",
        "    'axes.titlesize': 10,\n",
        "    'axes.labelsize': 9\n",
        "})\n",
        "\n",
        "# ====================== MODEL WRAPPER ======================\n",
        "class PrecomputedModel:\n",
        "    def __init__(self, classification_output, ranking_output):\n",
        "        self.classification_output = classification_output.astype(float).values.T\n",
        "        self.ranking_output = ranking_output.astype(float).values.T\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        if self.classification_output.shape[1] == 0:\n",
        "            class_output = np.zeros((batch_size, 2))\n",
        "        else:\n",
        "            repeats_cls = int(np.ceil(batch_size / self.classification_output.shape[0]))\n",
        "            class_output = np.tile(self.classification_output, (repeats_cls, 1))[:batch_size]\n",
        "\n",
        "        repeats_rank = int(np.ceil(batch_size / self.ranking_output.shape[0]))\n",
        "        rank_output = np.tile(self.ranking_output, (repeats_rank, 1))[:batch_size]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(class_output, dtype=torch.float32),\n",
        "            torch.tensor(rank_output, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "# ====================== LOAD DATA ======================\n",
        "def load_data_with_metadata():\n",
        "    load_df = pd.read_excel(\"./load_scenarios.xlsx\")\n",
        "    cont_df = pd.read_csv(\"./n1_contingency_balanced_filled_complete.csv\")\n",
        "    cont_df = cont_df[cont_df['Scenario'].between(995, 999)].reset_index(drop=True)\n",
        "    load_df = load_df[load_df['Scenario'].between(995, 999)]\n",
        "\n",
        "    load_df = load_df.pivot(index='Scenario', columns='Load_ID', values=['P_mw', 'Q_mvar'])\n",
        "    load_df.columns = [f\"{a}_{b}\" for a, b in load_df.columns]\n",
        "    load_df = load_df.sort_index(axis=1)\n",
        "    load_mat = load_df.values.astype(np.float32)\n",
        "    load_expanded = np.repeat(load_mat, 41, axis=0)\n",
        "\n",
        "    voltage_cols = [f\"V_bus_{i}\" for i in range(30)]\n",
        "    lineflow_cols = [f\"Loading_line_{i}\" for i in range(41)]\n",
        "    voltage_mat = cont_df[voltage_cols].values.astype(np.float32)\n",
        "    lineflow_mat = cont_df[lineflow_cols].values.astype(np.float32)\n",
        "\n",
        "    contingency_mat = np.zeros((cont_df.shape[0], 41))\n",
        "    contingency_mat[np.arange(cont_df.shape[0]), cont_df['Outaged_Line'].values] = 1\n",
        "\n",
        "    input_features = np.concatenate([\n",
        "        load_expanded, voltage_mat, lineflow_mat, contingency_mat\n",
        "    ], axis=1)\n",
        "\n",
        "    feature_names = []\n",
        "    for i in range(20):\n",
        "        feature_names += [f\"P_mw_load_{i}\", f\"Q_mvar_load_{i}\"]\n",
        "    feature_names += [f\"V_bus_{i}\" for i in range(30)]\n",
        "    feature_names += [f\"Flow_line_{i}\" for i in range(41)]\n",
        "    feature_names += [f\"Contingency_line_{i}\" for i in range(41)]\n",
        "\n",
        "    feature_metadata = {\n",
        "        'type_indices': {\n",
        "            'voltage': (40, 70),\n",
        "            'line_flow': (70, 111),\n",
        "            'contingency': (111, 152)\n",
        "        },\n",
        "        'group_cols': {\n",
        "            'voltage': feature_names[40:70],\n",
        "            'line_flow': feature_names[70:111],\n",
        "            'contingency': feature_names[111:152]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'tensor': torch.tensor(input_features).float(),\n",
        "        'feature_names': feature_names,\n",
        "        'scenario_ids': list(range(995, 1000)),\n",
        "        'feature_metadata': feature_metadata\n",
        "    }\n",
        "\n",
        "# ====================== RUN SHAP ======================\n",
        "def run_shap_analysis(models, background, metadata):\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        results[name] = {'cls': {}, 'rank': {}}\n",
        "        for task, output_index in zip(['cls', 'rank'], [0, 1]):\n",
        "            if task == 'cls' and model.classification_output.shape[1] == 0:\n",
        "                continue\n",
        "            explainer = shap.Explainer(lambda x: model(x)[output_index], background)\n",
        "            shap_values = explainer(background)\n",
        "            for group in metadata['type_indices']:\n",
        "                start, end = metadata['type_indices'][group]\n",
        "                values = shap_values.values[:, start:end]\n",
        "                if values.ndim > 2:\n",
        "                    values = np.mean(values, axis=1)\n",
        "                results[name][task][group] = {\n",
        "                    'values': values,\n",
        "                    'feature_names': metadata['group_cols'][group]\n",
        "                }\n",
        "    return results\n",
        "\n",
        "# ====================== SAVE PLOTS ======================\n",
        "def save_high_res_plots(results, output_dir='shap_results'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for model_name in results:\n",
        "        for task in results[model_name]:\n",
        "            for group in results[model_name][task]:\n",
        "                shap_vals = results[model_name][task][group]['values']\n",
        "                features = results[model_name][task][group]['feature_names']\n",
        "                if shap_vals.shape[1] != len(features):\n",
        "                    shap_vals = shap_vals[:, :len(features)]\n",
        "                shap.summary_plot(\n",
        "                    shap_vals, features=features,\n",
        "                    plot_type=\"bar\", show=False\n",
        "                )\n",
        "                plt.title(f\"{model_name} - {task.upper()} - {group.title()}\")\n",
        "                filename = f\"{model_name}_{task}_{group}.png\"\n",
        "                plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "# ====================== EXCEL EXPORT ======================\n",
        "def export_shap_summary_to_excel(results, output_file=\"shap_summary_50epochs.xlsx\"):\n",
        "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
        "    for model_name in results:\n",
        "        for task in ['cls', 'rank']:\n",
        "            for group in results[model_name][task]:\n",
        "                values = results[model_name][task][group]['values']\n",
        "                features = results[model_name][task][group]['feature_names']\n",
        "                summary_values = np.mean(np.abs(values), axis=0)\n",
        "                min_len = min(len(summary_values), len(features))\n",
        "                df = pd.DataFrame({\n",
        "                    \"Feature\": features[:min_len],\n",
        "                    \"SHAP Value\": summary_values[:min_len]\n",
        "                })\n",
        "                df = df.sort_values(by=\"SHAP Value\", ascending=False)\n",
        "                sheet_name = f\"{model_name}_{task}_{group}\"[:31]\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "    writer.close()\n",
        "\n",
        "# ====================== MAIN ======================\n",
        "def main():\n",
        "    print(\"====== HIGH-RESOLUTION SHAP ANALYSIS ======\")\n",
        "    print(\"\\nüîç Loading data with contingency features...\")\n",
        "    data = load_data_with_metadata()\n",
        "    background = data['tensor'][:5].numpy()\n",
        "    print(f\"Input tensor shape: {data['tensor'].shape}\")\n",
        "    print(f\"Background shape: {background.shape}\")\n",
        "\n",
        "    print(\"\\nüìä Loading Phase 2 model results...\")\n",
        "    result_file = \"./phase2_model_results_50epochs.xlsx\"\n",
        "    xls = pd.ExcelFile(result_file)\n",
        "\n",
        "    models = {}\n",
        "    model_names = [\n",
        "        (\"GCN_LSTM\", \"GCN_LSTM_Classify\", \"GCN_LSTM_Ranking\"),\n",
        "        (\"GCN_GRU_LSTM\", \"GCN_GRU_LSTM_Classify\", \"GCN_GRU_LSTM_Ranking\"),\n",
        "        (\"LSTM\", \"LSTM_Classify\", \"LSTM_Ranking\")\n",
        "    ]\n",
        "\n",
        "    cls_cols = [f\"Scenario_{i}\" for i in range(5, 10)]\n",
        "    rank_cols = [f\"Scenario_{i}_Cont_{j}\" for i in range(5, 10) for j in range(41)]\n",
        "\n",
        "    for tag, cls_sheet, rank_sheet in model_names:\n",
        "        classification_output = xls.parse(cls_sheet)\n",
        "        classification_output = classification_output.loc[:, classification_output.columns.isin(cls_cols)]\n",
        "        ranking_output = xls.parse(rank_sheet)\n",
        "        ranking_output = ranking_output.loc[:, ranking_output.columns.isin(rank_cols)]\n",
        "        models[tag] = PrecomputedModel(classification_output, ranking_output)\n",
        "\n",
        "    print(\"\\nüß† Running SHAP analysis...\")\n",
        "    shap_results = run_shap_analysis(models, background, data['feature_metadata'])\n",
        "\n",
        "    print(\"\\nüìä Exporting SHAP summary tables...\")\n",
        "    export_shap_summary_to_excel(shap_results)\n",
        "\n",
        "    print(\"\\nüé® Generating 1000 DPI plots...\")\n",
        "    save_high_res_plots(shap_results)\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis complete! High-resolution plots saved to shap_results\")\n",
        "    print(\"‚úÖ SHAP summary tables saved to shap_summary_50epochs.xlsx\")\n",
        "\n",
        "    for model_name in shap_results:\n",
        "        summary_df = []\n",
        "        for group in ['voltage', 'line_flow', 'contingency']:\n",
        "            for task in ['cls', 'rank']:\n",
        "                values = shap_results[model_name][task][group]['values']\n",
        "                avg = np.mean(np.abs(values))\n",
        "                summary_df.append({\"Feature Group\": group.title(), \"Task\": 'Classification' if task == 'cls' else 'Ranking', \"Average SHAP Value\": avg})\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_df)\n",
        "\n",
        "        cls_df = summary_df[summary_df['Task'] == 'Classification']\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(cls_df['Feature Group'], cls_df['Average SHAP Value'], color='skyblue')\n",
        "        plt.ylabel(\"Average SHAP Value\")\n",
        "        plt.title(f\"{model_name} - Classification Feature Importance\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"shap_results/{model_name}_Classification_Bar.png\", dpi=1000)\n",
        "        plt.show()\n",
        "\n",
        "        rank_df = summary_df[summary_df['Task'] == 'Ranking']\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(rank_df['Feature Group'], rank_df['Average SHAP Value'], color='salmon')\n",
        "        plt.ylabel(\"Average SHAP Value\")\n",
        "        plt.title(f\"{model_name} - Ranking Feature Importance\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"shap_results/{model_name}_Ranking_Bar.png\", dpi=1000)\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
