{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48625b7",
   "metadata": {},
   "source": [
    "# Phase 4: XAI Benchmarking\n",
    "## Comparison of XAI Methods: SHAP, LIME, Integrated Gradients, Attention Maps\n",
    "### Benchmark on: Fidelity, Sparsity, Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for XAI benchmarking\n",
    "%pip install lime captum \n",
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95886e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, fidelity_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XAI Libraries\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from captum.attr import IntegratedGradients, LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72908a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "print(\"ðŸ“Š Loading dataset...\")\n",
    "\n",
    "load_df = pd.read_excel(\"load_scenarios.xlsx\")\n",
    "cont_df = pd.read_csv(\"n1_contingency_balanced_filled_complete.csv\")\n",
    "cont_df = cont_df[cont_df['Scenario'] < 1000].reset_index(drop=True)\n",
    "\n",
    "# Extract features\n",
    "load_features = load_df[[\"P_mw\", \"Q_mvar\"]].values\n",
    "load_features = load_features.reshape(1000, 40)\n",
    "load_features_expanded = np.repeat(load_features, 41, axis=0)\n",
    "\n",
    "bus_cols = [col for col in cont_df.columns if col.startswith(\"V_bus_\")]\n",
    "line_cols = [col for col in cont_df.columns if col.startswith(\"Loading_line_\")]\n",
    "voltages = cont_df[bus_cols].values.astype(np.float32)\n",
    "line_flows = cont_df[line_cols].values.astype(np.float32)\n",
    "\n",
    "# Combine features\n",
    "X = np.concatenate([load_features_expanded, voltages, line_flows], axis=1)\n",
    "y = cont_df['Severity'].values.astype(np.int64)\n",
    "\n",
    "# Feature names\n",
    "feature_names = []\n",
    "for i in range(20):\n",
    "    feature_names.extend([f\"P_load_{i}\", f\"Q_load_{i}\"])\n",
    "feature_names.extend([f\"V_bus_{i}\" for i in range(len(bus_cols))])\n",
    "feature_names.extend([f\"Loading_line_{i}\" for i in range(len(line_cols))])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "print(f\"Class distribution - Train: {np.bincount(y_train)}, Test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch model for Integrated Gradients\n",
    "class PowerSystemClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(PowerSystemClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Train PyTorch model\n",
    "print(\"ðŸ¤– Training PyTorch model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Create model\n",
    "model = PowerSystemClassifier(X_train_scaled.shape[1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "for epoch in range(50):  # Reduced epochs for faster execution\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/50], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    pytorch_accuracy = (predicted == y_test_tensor).float().mean().item()\n",
    "    print(f\"PyTorch Model Accuracy: {pytorch_accuracy:.4f}\")\n",
    "\n",
    "# Train sklearn model for SHAP and LIME\n",
    "print(\"ðŸ¤– Training sklearn model...\")\n",
    "sklearn_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "sklearn_model.fit(X_train_scaled, y_train)\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_model.predict(X_test_scaled))\n",
    "print(f\"Sklearn Model Accuracy: {sklearn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ff8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select evaluation samples\n",
    "print(\"ðŸŽ¯ Selecting evaluation samples...\")\n",
    "\n",
    "# Select 100 samples from test set for benchmarking\n",
    "n_samples = min(100, len(X_test_scaled))\n",
    "eval_indices = np.random.choice(len(X_test_scaled), size=n_samples, replace=False)\n",
    "X_eval = X_test_scaled[eval_indices]\n",
    "y_eval = y_test[eval_indices]\n",
    "\n",
    "print(f\"Selected {n_samples} samples for evaluation\")\n",
    "print(f\"Evaluation set class distribution: {np.bincount(y_eval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: SHAP Explanations\n",
    "print(\"ðŸ” Generating SHAP explanations...\")\n",
    "\n",
    "# SHAP for sklearn model\n",
    "explainer_shap = shap.Explainer(sklearn_model, X_train_scaled[:100])  # Use subset for faster computation\n",
    "shap_values = explainer_shap(X_eval[:20])  # Evaluate on subset\n",
    "\n",
    "# Extract SHAP values for positive class\n",
    "if len(shap_values.values.shape) == 3:  # Multi-class output\n",
    "    shap_attributions = shap_values.values[:, :, 1]  # Class 1 (unstable)\n",
    "else:\n",
    "    shap_attributions = shap_values.values\n",
    "\n",
    "print(f\"SHAP attributions shape: {shap_attributions.shape}\")\n",
    "print(\"âœ… SHAP explanations generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: LIME Explanations\n",
    "print(\"ðŸ” Generating LIME explanations...\")\n",
    "\n",
    "# LIME explainer\n",
    "explainer_lime = LimeTabularExplainer(\n",
    "    X_train_scaled,\n",
    "    feature_names=feature_names,\n",
    "    class_names=['Stable', 'Unstable'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Generate LIME explanations for subset\n",
    "lime_attributions = []\n",
    "n_lime_samples = min(20, len(X_eval))\n",
    "\n",
    "for i in range(n_lime_samples):\n",
    "    explanation = explainer_lime.explain_instance(\n",
    "        X_eval[i], \n",
    "        sklearn_model.predict_proba,\n",
    "        num_features=len(feature_names)\n",
    "    )\n",
    "    \n",
    "    # Extract feature importances\n",
    "    feature_importance = dict(explanation.as_list())\n",
    "    lime_attribution = np.zeros(len(feature_names))\n",
    "    \n",
    "    for j, feature_name in enumerate(feature_names):\n",
    "        if feature_name in feature_importance:\n",
    "            lime_attribution[j] = feature_importance[feature_name]\n",
    "    \n",
    "    lime_attributions.append(lime_attribution)\n",
    "\n",
    "lime_attributions = np.array(lime_attributions)\n",
    "print(f\"LIME attributions shape: {lime_attributions.shape}\")\n",
    "print(\"âœ… LIME explanations generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Integrated Gradients\n",
    "print(\"ðŸ” Generating Integrated Gradients explanations...\")\n",
    "\n",
    "# Integrated Gradients for PyTorch model\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# Convert evaluation samples to tensors\n",
    "X_eval_tensor = torch.FloatTensor(X_eval[:20]).to(device)  # Use subset\n",
    "baseline = torch.zeros_like(X_eval_tensor[0]).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate attributions\n",
    "ig_attributions = []\n",
    "model.eval()\n",
    "\n",
    "for i in range(len(X_eval_tensor)):\n",
    "    input_tensor = X_eval_tensor[i].unsqueeze(0)\n",
    "    \n",
    "    # Get prediction for target class\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        target_class = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Compute integrated gradients\n",
    "    attribution = ig.attribute(\n",
    "        input_tensor,\n",
    "        baseline,\n",
    "        target=target_class,\n",
    "        n_steps=50\n",
    "    )\n",
    "    \n",
    "    ig_attributions.append(attribution.squeeze().cpu().numpy())\n",
    "\n",
    "ig_attributions = np.array(ig_attributions)\n",
    "print(f\"Integrated Gradients attributions shape: {ig_attributions.shape}\")\n",
    "print(\"âœ… Integrated Gradients explanations generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Attention Maps (Simplified - using gradient-based attention)\n",
    "print(\"ðŸ” Generating gradient-based attention maps...\")\n",
    "\n",
    "# Simple gradient-based attention\n",
    "attention_attributions = []\n",
    "model.eval()\n",
    "\n",
    "for i in range(len(X_eval_tensor)):\n",
    "    input_tensor = X_eval_tensor[i].unsqueeze(0)\n",
    "    input_tensor.requires_grad_()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "    target_class = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    output[0, target_class].backward()\n",
    "    \n",
    "    # Get gradients as attention\n",
    "    attention = input_tensor.grad.squeeze().cpu().numpy()\n",
    "    attention_attributions.append(attention)\n",
    "\n",
    "attention_attributions = np.array(attention_attributions)\n",
    "print(f\"Attention attributions shape: {attention_attributions.shape}\")\n",
    "print(\"âœ… Attention explanations generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking Metrics\n",
    "print(\"ðŸ“Š Computing benchmarking metrics...\")\n",
    "\n",
    "def compute_sparsity(attributions):\n",
    "    \"\"\"Compute sparsity as percentage of near-zero attributions\"\"\"\n",
    "    threshold = 1e-4\n",
    "    return np.mean(np.abs(attributions) < threshold) * 100\n",
    "\n",
    "def compute_fidelity_approximation(attributions, X_eval_subset, model_func, top_k=10):\n",
    "    \"\"\"Approximate fidelity by removing top-k features and measuring prediction change\"\"\"\n",
    "    fidelity_scores = []\n",
    "    \n",
    "    for i in range(len(attributions)):\n",
    "        # Get original prediction\n",
    "        original_pred = model_func(X_eval_subset[i:i+1])[0]\n",
    "        \n",
    "        # Find top-k important features\n",
    "        top_features = np.argsort(np.abs(attributions[i]))[-top_k:]\n",
    "        \n",
    "        # Create perturbed input (set top features to mean)\n",
    "        perturbed_input = X_eval_subset[i:i+1].copy()\n",
    "        feature_means = np.mean(X_train_scaled, axis=0)\n",
    "        perturbed_input[0, top_features] = feature_means[top_features]\n",
    "        \n",
    "        # Get perturbed prediction\n",
    "        perturbed_pred = model_func(perturbed_input)[0]\n",
    "        \n",
    "        # Compute fidelity as prediction change\n",
    "        fidelity = np.abs(original_pred - perturbed_pred)\n",
    "        fidelity_scores.append(fidelity)\n",
    "    \n",
    "    return np.mean(fidelity_scores)\n",
    "\n",
    "def compute_consistency(attributions):\n",
    "    \"\"\"Compute consistency as standard deviation across samples\"\"\"\n",
    "    feature_importance_std = np.std(attributions, axis=0)\n",
    "    return np.mean(feature_importance_std)\n",
    "\n",
    "# Ensure all attributions have the same number of samples\n",
    "min_samples = min(len(shap_attributions), len(lime_attributions), \n",
    "                 len(ig_attributions), len(attention_attributions))\n",
    "\n",
    "print(f\"Evaluating on {min_samples} samples for fair comparison\")\n",
    "\n",
    "# Compute metrics for each method\n",
    "methods = {\n",
    "    'SHAP': shap_attributions[:min_samples],\n",
    "    'LIME': lime_attributions[:min_samples], \n",
    "    'Integrated Gradients': ig_attributions[:min_samples],\n",
    "    'Gradient Attention': attention_attributions[:min_samples]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for method_name, attributions in methods.items():\n",
    "    print(f\"\\nEvaluating {method_name}...\")\n",
    "    \n",
    "    # Sparsity\n",
    "    sparsity = compute_sparsity(attributions)\n",
    "    \n",
    "    # Fidelity (approximate)\n",
    "    if method_name in ['SHAP', 'LIME']:\n",
    "        fidelity = compute_fidelity_approximation(\n",
    "            attributions, X_eval[:min_samples], \n",
    "            lambda x: sklearn_model.predict_proba(x)[:, 1]\n",
    "        )\n",
    "    else:\n",
    "        def pytorch_predict_func(x):\n",
    "            with torch.no_grad():\n",
    "                tensor_x = torch.FloatTensor(x).to(device)\n",
    "                output = model(tensor_x)\n",
    "                return F.softmax(output, dim=1)[:, 1].cpu().numpy()\n",
    "        \n",
    "        fidelity = compute_fidelity_approximation(\n",
    "            attributions, X_eval[:min_samples], pytorch_predict_func\n",
    "        )\n",
    "    \n",
    "    # Consistency\n",
    "    consistency = compute_consistency(attributions)\n",
    "    \n",
    "    # Feature concentration (how concentrated are the attributions)\n",
    "    concentration = np.mean([np.std(attr) for attr in attributions])\n",
    "    \n",
    "    results.append({\n",
    "        'Method': method_name,\n",
    "        'Sparsity (%)': sparsity,\n",
    "        'Fidelity': fidelity,\n",
    "        'Consistency': consistency,\n",
    "        'Concentration': concentration,\n",
    "        'Avg Absolute Attribution': np.mean(np.abs(attributions))\n",
    "    })\n",
    "    \n",
    "    print(f\"  Sparsity: {sparsity:.2f}%\")\n",
    "    print(f\"  Fidelity: {fidelity:.4f}\")\n",
    "    print(f\"  Consistency: {consistency:.4f}\")\n",
    "    print(f\"  Concentration: {concentration:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Benchmarking Results Summary:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Save results\n",
    "results_df.to_excel(\"xai_benchmarking_results.xlsx\", index=False)\n",
    "print(\"\\nâœ… Results saved to 'xai_benchmarking_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of benchmarking results\n",
    "print(\"ðŸ“Š Creating visualizations...\")\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('XAI Methods Benchmarking Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Sparsity (%)', 'Fidelity', 'Consistency', 'Concentration', 'Avg Absolute Attribution']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'lightsalmon', 'lightpink']\n",
    "\n",
    "# Individual metric plots\n",
    "for i, metric in enumerate(metrics):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    bars = ax.bar(results_df['Method'], results_df[metric], color=colors[i % len(colors)], alpha=0.7)\n",
    "    ax.set_title(f'{metric} by XAI Method', fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Overall comparison radar chart (in the last subplot)\n",
    "ax_radar = axes[1, 2]\n",
    "ax_radar.remove()  # Remove the last subplot\n",
    "\n",
    "# Create a combined score visualization\n",
    "ax_combined = fig.add_subplot(2, 3, 6)\n",
    "\n",
    "# Normalize metrics to 0-1 scale for fair comparison\n",
    "normalized_results = results_df.copy()\n",
    "for col in ['Sparsity (%)', 'Fidelity', 'Consistency', 'Concentration', 'Avg Absolute Attribution']:\n",
    "    max_val = normalized_results[col].max()\n",
    "    min_val = normalized_results[col].min()\n",
    "    if max_val != min_val:\n",
    "        normalized_results[col] = (normalized_results[col] - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        normalized_results[col] = 0.5\n",
    "\n",
    "# For some metrics, lower is better (invert them)\n",
    "normalized_results['Consistency'] = 1 - normalized_results['Consistency']  # Lower consistency is better\n",
    "\n",
    "# Calculate combined score\n",
    "normalized_results['Combined Score'] = (\n",
    "    normalized_results['Sparsity (%)'] * 0.2 +\n",
    "    normalized_results['Fidelity'] * 0.3 +\n",
    "    normalized_results['Consistency'] * 0.3 +\n",
    "    normalized_results['Concentration'] * 0.2\n",
    ")\n",
    "\n",
    "bars_combined = ax_combined.bar(normalized_results['Method'], normalized_results['Combined Score'], \n",
    "                               color='gold', alpha=0.7)\n",
    "ax_combined.set_title('Combined Performance Score', fontweight='bold')\n",
    "ax_combined.set_ylabel('Normalized Score (0-1)')\n",
    "ax_combined.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars_combined:\n",
    "    height = bar.get_height()\n",
    "    ax_combined.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('xai_benchmarking_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance comparison for power system specific features\n",
    "print(\"\\nðŸ“Š Creating feature group importance comparison...\")\n",
    "\n",
    "# Group features by type\n",
    "load_indices = list(range(40))  # P and Q loads\n",
    "voltage_indices = list(range(40, 40 + len(bus_cols)))\n",
    "flow_indices = list(range(40 + len(bus_cols), len(feature_names)))\n",
    "\n",
    "feature_groups = {\n",
    "    'Load Features': load_indices,\n",
    "    'Voltage Features': voltage_indices, \n",
    "    'Line Flow Features': flow_indices\n",
    "}\n",
    "\n",
    "group_importance = []\n",
    "\n",
    "for method_name, attributions in methods.items():\n",
    "    method_importance = {'Method': method_name}\n",
    "    \n",
    "    for group_name, indices in feature_groups.items():\n",
    "        if len(indices) > 0 and max(indices) < attributions.shape[1]:\n",
    "            group_attrs = attributions[:, indices]\n",
    "            avg_importance = np.mean(np.abs(group_attrs))\n",
    "            method_importance[group_name] = avg_importance\n",
    "        else:\n",
    "            method_importance[group_name] = 0\n",
    "    \n",
    "    group_importance.append(method_importance)\n",
    "\n",
    "group_df = pd.DataFrame(group_importance)\n",
    "\n",
    "# Plot feature group importance\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "x = np.arange(len(group_df['Method']))\n",
    "width = 0.25\n",
    "\n",
    "for i, group in enumerate(['Load Features', 'Voltage Features', 'Line Flow Features']):\n",
    "    ax.bar(x + i*width, group_df[group], width, label=group, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('XAI Method')\n",
    "ax.set_ylabel('Average Absolute Importance')\n",
    "ax.set_title('Feature Group Importance by XAI Method')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(group_df['Method'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('xai_feature_group_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save detailed results\n",
    "with pd.ExcelWriter(\"detailed_xai_benchmarking.xlsx\", engine='xlsxwriter') as writer:\n",
    "    results_df.to_excel(writer, sheet_name=\"Overall_Results\", index=False)\n",
    "    group_df.to_excel(writer, sheet_name=\"Feature_Group_Importance\", index=False)\n",
    "    normalized_results.to_excel(writer, sheet_name=\"Normalized_Scores\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Detailed results saved to 'detailed_xai_benchmarking.xlsx'\")\n",
    "print(\"âœ… Visualizations saved as PNG files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Recommendations\n",
    "print(\"\\nðŸŽ¯ XAI Benchmarking Summary and Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find best method for each metric\n",
    "best_methods = {}\n",
    "for metric in ['Sparsity (%)', 'Fidelity', 'Combined Score']:\n",
    "    if metric == 'Consistency':  # Lower is better\n",
    "        best_idx = results_df[metric].idxmin()\n",
    "    else:  # Higher is better\n",
    "        if metric == 'Combined Score':\n",
    "            best_idx = normalized_results[metric].idxmax()\n",
    "        else:\n",
    "            best_idx = results_df[metric].idxmax()\n",
    "    \n",
    "    if metric == 'Combined Score':\n",
    "        best_methods[metric] = normalized_results.loc[best_idx, 'Method']\n",
    "    else:\n",
    "        best_methods[metric] = results_df.loc[best_idx, 'Method']\n",
    "\n",
    "print(\"ðŸ† Best Performing Methods:\")\n",
    "for metric, method in best_methods.items():\n",
    "    print(f\"  â€¢ {metric}: {method}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Method Characteristics:\")\n",
    "print(\"  â€¢ SHAP: Model-agnostic, theoretically grounded, good for global insights\")\n",
    "print(\"  â€¢ LIME: Local explanations, intuitive, fast computation\")\n",
    "print(\"  â€¢ Integrated Gradients: Attribution method, smooth gradients, requires gradients\")\n",
    "print(\"  â€¢ Gradient Attention: Simple, fast, but may be noisy\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Recommendations for Power System Applications:\")\n",
    "if 'SHAP' in list(best_methods.values()):\n",
    "    print(\"  âœ… SHAP recommended for comprehensive analysis and regulatory compliance\")\n",
    "if 'LIME' in list(best_methods.values()):\n",
    "    print(\"  âœ… LIME recommended for real-time operational explanations\")\n",
    "if 'Integrated Gradients' in list(best_methods.values()):\n",
    "    print(\"  âœ… Integrated Gradients recommended for deep learning model analysis\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Phase 4 Complete: XAI benchmarking analysis finished!\")\n",
    "print(\"ðŸ“ Outputs:\")\n",
    "print(\"   - xai_benchmarking_results.xlsx: Summary metrics\")\n",
    "print(\"   - detailed_xai_benchmarking.xlsx: Comprehensive results\")\n",
    "print(\"   - xai_benchmarking_comparison.png: Performance comparison charts\")\n",
    "print(\"   - xai_feature_group_importance.png: Feature importance by group\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
